<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 16.0.0"/>
    <title>divisor.acestep.models.lyrics_utils.lyric_encoder API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:27px;vertical-align:bottom;width:50px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><style>
    .pdoc .mermaid-pre {
        border: none;
        background: none;
    }
</style>
<script type="module" defer>
    import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs";

    /* Re-invoke Mermaid when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => mermaid.run()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../lyrics_utils.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;divisor.acestep.models.lyrics_utils</a>

            <img src="https://raw.githubusercontent.com/darkshapes/entity-statement/refs/heads/main/png/divisor/divisor_75.png" class="logo" alt="project logo"/>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#ConvolutionModule">ConvolutionModule</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ConvolutionModule.__init__">ConvolutionModule</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConvolutionModule.pointwise_conv1">pointwise_conv1</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConvolutionModule.depthwise_conv">depthwise_conv</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConvolutionModule.pointwise_conv2">pointwise_conv2</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConvolutionModule.activation">activation</a>
                        </li>
                        <li>
                                <a class="function" href="#ConvolutionModule.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#PositionwiseFeedForward">PositionwiseFeedForward</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#PositionwiseFeedForward.__init__">PositionwiseFeedForward</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionwiseFeedForward.w_1">w_1</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionwiseFeedForward.activation">activation</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionwiseFeedForward.dropout">dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionwiseFeedForward.w_2">w_2</a>
                        </li>
                        <li>
                                <a class="function" href="#PositionwiseFeedForward.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Swish">Swish</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Swish.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MultiHeadedAttention">MultiHeadedAttention</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MultiHeadedAttention.__init__">MultiHeadedAttention</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.d_k">d_k</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.h">h</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.linear_q">linear_q</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.linear_k">linear_k</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.linear_v">linear_v</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.linear_out">linear_out</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiHeadedAttention.dropout">dropout</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiHeadedAttention.forward_qkv">forward_qkv</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiHeadedAttention.forward_attention">forward_attention</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiHeadedAttention.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#RelPositionMultiHeadedAttention">RelPositionMultiHeadedAttention</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#RelPositionMultiHeadedAttention.__init__">RelPositionMultiHeadedAttention</a>
                        </li>
                        <li>
                                <a class="variable" href="#RelPositionMultiHeadedAttention.linear_pos">linear_pos</a>
                        </li>
                        <li>
                                <a class="variable" href="#RelPositionMultiHeadedAttention.pos_bias_u">pos_bias_u</a>
                        </li>
                        <li>
                                <a class="variable" href="#RelPositionMultiHeadedAttention.pos_bias_v">pos_bias_v</a>
                        </li>
                        <li>
                                <a class="function" href="#RelPositionMultiHeadedAttention.rel_shift">rel_shift</a>
                        </li>
                        <li>
                                <a class="function" href="#RelPositionMultiHeadedAttention.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#subsequent_mask">subsequent_mask</a>
            </li>
            <li>
                    <a class="function" href="#subsequent_chunk_mask">subsequent_chunk_mask</a>
            </li>
            <li>
                    <a class="function" href="#add_optional_chunk_mask">add_optional_chunk_mask</a>
            </li>
            <li>
                    <a class="class" href="#ConformerEncoderLayer">ConformerEncoderLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ConformerEncoderLayer.__init__">ConformerEncoderLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.self_attn">self_attn</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.feed_forward">feed_forward</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.feed_forward_macaron">feed_forward_macaron</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.conv_module">conv_module</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.norm_ff">norm_ff</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.norm_mha">norm_mha</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.dropout">dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.size">size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoderLayer.normalize_before">normalize_before</a>
                        </li>
                        <li>
                                <a class="function" href="#ConformerEncoderLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#EspnetRelPositionalEncoding">EspnetRelPositionalEncoding</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#EspnetRelPositionalEncoding.__init__">EspnetRelPositionalEncoding</a>
                        </li>
                        <li>
                                <a class="variable" href="#EspnetRelPositionalEncoding.d_model">d_model</a>
                        </li>
                        <li>
                                <a class="variable" href="#EspnetRelPositionalEncoding.xscale">xscale</a>
                        </li>
                        <li>
                                <a class="variable" href="#EspnetRelPositionalEncoding.dropout">dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#EspnetRelPositionalEncoding.pe">pe</a>
                        </li>
                        <li>
                                <a class="function" href="#EspnetRelPositionalEncoding.extend_pe">extend_pe</a>
                        </li>
                        <li>
                                <a class="function" href="#EspnetRelPositionalEncoding.forward">forward</a>
                        </li>
                        <li>
                                <a class="function" href="#EspnetRelPositionalEncoding.position_encoding">position_encoding</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#LinearEmbed">LinearEmbed</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LinearEmbed.__init__">LinearEmbed</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbed.out">out</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbed.pos_enc">pos_enc</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearEmbed.position_encoding">position_encoding</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearEmbed.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="variable" href="#ATTENTION_CLASSES">ATTENTION_CLASSES</a>
            </li>
            <li>
                    <a class="variable" href="#ACTIVATION_CLASSES">ACTIVATION_CLASSES</a>
            </li>
            <li>
                    <a class="function" href="#make_pad_mask">make_pad_mask</a>
            </li>
            <li>
                    <a class="class" href="#ConformerEncoder">ConformerEncoder</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ConformerEncoder.__init__">ConformerEncoder</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.output_size">output_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.embed">embed</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.normalize_before">normalize_before</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.after_norm">after_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.gradient_checkpointing">gradient_checkpointing</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.use_dynamic_chunk">use_dynamic_chunk</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.static_chunk_size">static_chunk_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.use_dynamic_left_chunk">use_dynamic_left_chunk</a>
                        </li>
                        <li>
                                <a class="variable" href="#ConformerEncoder.encoders">encoders</a>
                        </li>
                        <li>
                                <a class="function" href="#ConformerEncoder.forward_layers">forward_layers</a>
                        </li>
                        <li>
                                <a class="function" href="#ConformerEncoder.forward_layers_checkpointed">forward_layers_checkpointed</a>
                        </li>
                        <li>
                                <a class="function" href="#ConformerEncoder.forward">forward</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22160%22%20viewBox%3D%220%200%20150%2080%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M132.316%2048.886c.276-4.679%202.342-6.698%204.409-7.982s4.27-1.165%206.751-1.055c1.586.07%203.044.156%204.222-.482%201.142-.619%202.026-1.932%202.162-3.739.268-3.576-1.929-5.368-5.006-5.551s-7.599.524-10.517%201.606c-4.455%201.652-8.588%206.606-9.552%208.992s-2.342%206.193-1.745%2010.873%202.664%209.221%205.878%2011.79%205.878%203.808%2010.103%204.312%203.444.229%206.062.229%205.006-2.202%204.914-4.909-2.296-5.001-4.501-4.863-3.077.505-5.281.229-7.715-2.064-7.899-9.451z%22%20fill%3D%22%23198754%22/%3E%3Ccircle%20cx%3D%22101.504%22%20cy%3D%2248.943%22%20r%3D%2214.208%22%20fill%3D%22none%22%20stroke%3D%22%23198754%22%20stroke-width%3D%229.354%22/%3E%3Cpath%20d%3D%22M87.81.002c-3.637.065-5.001.454-7.014%201.232s-3.443%201.363-6.3%204.282c-1.723%201.76-3.148%205.019-3.776%207.329-.413%201.521-.316%202.63-.316%202.63l-.195%2034.612c.065%205.774-6.755%208.305-9.612%208.37s-9.678-1.038-9.743-9.408%207.128-9.521%208.362-9.521c1.413-.13%202.526-.021%203.718-.016%202.071.009%204.157-.778%204.092-4.671s-4.157-4.736-4.157-4.736c-6.3-.843-11.43%202.206-11.43%202.206S40.917%2038.15%2041.372%2049.634%2051.568%2068.19%2061.311%2068.125s18.316-7.007%2018.445-17.193l.13-22.772c.046-2.291%202.683-3.644%204.476-4.203.745-.232%201.694-.274%201.694-.274l10.457-.13s4.871-.324%207.729-3.114%204.352-6.294%204.352-6.294.974-3.049.13-4.606-.195-1.233-2.792-3.309-8.573-4.477-8.573-4.477S91.447-.063%2087.81.002zM0%2047.169l.065%2028.417S0%2080.127%204.481%2079.997s5.072-3.866%205.049-4.152l-.113-28.482s1.624-7.656%209.937-7.721%2010.002%206.942%2010.002%208.499-.909%2010.51-9.093%2010.51c-.948%200-2.99-.567-4.145-.272-3.919%201-3.194%204.554-3.194%204.554s.065%205.061%207.404%204.996%2018.575-6.034%2018.575-19.074S26.953%2030.04%2019.549%2029.91%201.234%2035.296%200%2047.169z%22%20fill%3D%22%23198754%22/%3E%3Cg%20transform%3D%22matrix%28.325601%200%200%20.325256%20-10.32669%20-45.802786%29%22%3E%3Ccircle%20cx%3D%22297.554%22%20cy%3D%22172.286%22%20r%3D%2216.5%22%20fill%3D%22%23fff%22/%3E%3Cellipse%20cx%3D%22297.709%22%20cy%3D%22172.642%22%20rx%3D%2211.071%22%20ry%3D%2210.871%22%20fill%3D%22%23105a48%22/%3E%3Ccircle%20cx%3D%22304.104%22%20cy%3D%22167.667%22%20r%3D%224.5%22%20fill%3D%22%23fff%22/%3E%3C/g%3E%3Cpath%20d%3D%22M94.661%2017.032l.893-1.476s.99.714%201.916.925%201.575.114%202.955.114l14.565-.162c1.283-.032%203.085-.762%203.02-3.293s-.373-3.503-.373-3.503l1.283-.487s.52.503.877%201.573.309%201.995.292%202.66-.227%201.541-.227%201.541%201.564-.308%202.359-1.038.823-.779%201.489-1.508.812-.86.812-.86.552-.13.877.26.341.957.065%201.46-1.672%202.206-3.247%203.066-2.76%201.427-3.929%201.768-3.848.73-7.063.714l-10.944-.114s-2.143-.081-3.02-.373-2.241-.973-2.598-1.265z%22%20fill%3D%22%23d36d49%22/%3E%3Cg%20fill%3D%22%23105a48%22%3E%3Cellipse%20cx%3D%2293.052%22%20cy%3D%2243.567%22%20rx%3D%22.869%22%20ry%3D%221.014%22%20transform%3D%22rotate%28341.022%29%22/%3E%3Cellipse%20cx%3D%22104.3%22%20cy%3D%22-16.184%22%20rx%3D%22.865%22%20ry%3D%221.009%22%20transform%3D%22rotate%2814.786%29%22/%3E%3C/g%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../../../divisor.html">divisor</a><wbr>.<a href="./../../../acestep.html">acestep</a><wbr>.<a href="./../../models.html">models</a><wbr>.<a href="./../lyrics_utils.html">lyrics_utils</a><wbr>.lyric_encoder    </h1>

                
                        <input id="mod-lyric_encoder-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-lyric_encoder-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">   1</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">   2</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">   3</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">   4</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">   5</span></a>
</span><span id="L-6"><a href="#L-6"><span class="linenos">   6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">   7</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConvolutionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">   8</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;ConvolutionModule in Conformer model.&quot;&quot;&quot;</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">   9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos">  10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">  11</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">  12</span></a>        <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">  13</span></a>        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">  14</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">  15</span></a>        <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">  16</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">  17</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">  18</span></a>    <span class="p">):</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">  19</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an ConvolutionModule object.</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">  20</span></a><span class="sd">        Args:</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">  21</span></a><span class="sd">            channels (int): The number of channels of conv layers.</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">  22</span></a><span class="sd">            kernel_size (int): Kernel size of conv layers.</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">  23</span></a><span class="sd">            causal (int): Whether use causal convolution or not</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos">  24</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">  25</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">  26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos">  27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">  28</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">  29</span></a>            <span class="mi">2</span> <span class="o">*</span> <span class="n">channels</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">  30</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">  31</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">  32</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">  33</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">  34</span></a>        <span class="p">)</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">  35</span></a>        <span class="c1"># self.lorder is used to distinguish if it&#39;s a causal convolution,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">  36</span></a>        <span class="c1"># if self.lorder &gt; 0: it&#39;s a causal convolution, the input will be</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">  37</span></a>        <span class="c1">#    padded with self.lorder frames on the left in forward.</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">  38</span></a>        <span class="c1"># else: it&#39;s a symmetrical convolution</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">  39</span></a>        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">  40</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">  41</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">  42</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">  43</span></a>            <span class="c1"># kernel_size should be an odd number for none causal convolution</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">  44</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">  45</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">  46</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">  47</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">  48</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">  49</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">  50</span></a>            <span class="n">kernel_size</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos">  51</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">  52</span></a>            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">  53</span></a>            <span class="n">groups</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">  54</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">  55</span></a>        <span class="p">)</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">  56</span></a>
</span><span id="L-57"><a href="#L-57"><span class="linenos">  57</span></a>        <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span> <span class="s2">&quot;layer_norm&quot;</span><span class="p">]</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">  58</span></a>        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">:</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">  59</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">  60</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos">  61</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">  62</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">  63</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos">  64</span></a>
</span><span id="L-65"><a href="#L-65"><span class="linenos">  65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos">  66</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos">  67</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos">  68</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos">  69</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos">  70</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos">  71</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos">  72</span></a>        <span class="p">)</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos">  73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos">  74</span></a>
</span><span id="L-75"><a href="#L-75"><span class="linenos">  75</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos">  76</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos">  77</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos">  78</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos">  79</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos">  80</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos">  81</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute convolution module.</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos">  82</span></a><span class="sd">        Args:</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos">  83</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, channels).</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos">  84</span></a><span class="sd">            mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos">  85</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos">  86</span></a><span class="sd">            cache (torch.Tensor): left context cache, it is only</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos">  87</span></a><span class="sd">                used in causal convolution (#batch, channels, cache_t),</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos">  88</span></a><span class="sd">                (0, 0, 0) meas fake cache.</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos">  89</span></a><span class="sd">        Returns:</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos">  90</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, channels).</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos">  91</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos">  92</span></a>        <span class="c1"># exchange the temporal dimension and the feature dimension</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos">  93</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (#batch, channels, time)</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos">  94</span></a>
</span><span id="L-95"><a href="#L-95"><span class="linenos">  95</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos">  96</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos">  97</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos">  98</span></a>
</span><span id="L-99"><a href="#L-99"><span class="linenos">  99</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos"> 100</span></a>            <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># cache_t == 0</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos"> 101</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos"> 102</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos"> 103</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># equal batch</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos"> 104</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># equal channel</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos"> 105</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos"> 106</span></a>            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos"> 107</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="p">:]</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos"> 108</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos"> 109</span></a>            <span class="c1"># It&#39;s better we just return None if no cache is required,</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos"> 110</span></a>            <span class="c1"># However, for JIT export, here we just fake one tensor instead of</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos"> 111</span></a>            <span class="c1"># None.</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos"> 112</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos"> 113</span></a>
</span><span id="L-114"><a href="#L-114"><span class="linenos"> 114</span></a>        <span class="c1"># GLU mechanism</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos"> 115</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, 2*channel, dim)</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos"> 116</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, channel, dim)</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos"> 117</span></a>
</span><span id="L-118"><a href="#L-118"><span class="linenos"> 118</span></a>        <span class="c1"># 1D Depthwise Conv</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos"> 119</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos"> 120</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos"> 121</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos"> 122</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos"> 123</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos"> 124</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos"> 125</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos"> 126</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos"> 127</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos"> 128</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos"> 129</span></a>
</span><span id="L-130"><a href="#L-130"><span class="linenos"> 130</span></a>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">new_cache</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos"> 131</span></a>
</span><span id="L-132"><a href="#L-132"><span class="linenos"> 132</span></a>
</span><span id="L-133"><a href="#L-133"><span class="linenos"> 133</span></a><span class="k">class</span><span class="w"> </span><span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos"> 134</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Positionwise feed forward layer.</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos"> 135</span></a>
</span><span id="L-136"><a href="#L-136"><span class="linenos"> 136</span></a><span class="sd">    FeedForward are appied on each position of the sequence.</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos"> 137</span></a><span class="sd">    The output dim is same with the input dim.</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos"> 138</span></a>
</span><span id="L-139"><a href="#L-139"><span class="linenos"> 139</span></a><span class="sd">    Args:</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos"> 140</span></a><span class="sd">        idim (int): Input dimenstion.</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos"> 141</span></a><span class="sd">        hidden_units (int): The number of hidden units.</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos"> 142</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos"> 143</span></a><span class="sd">        activation (torch.nn.Module): Activation function</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos"> 144</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos"> 145</span></a>
</span><span id="L-146"><a href="#L-146"><span class="linenos"> 146</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos"> 147</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos"> 148</span></a>        <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos"> 149</span></a>        <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos"> 150</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos"> 151</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos"> 152</span></a>    <span class="p">):</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos"> 153</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct a PositionwiseFeedForward object.&quot;&quot;&quot;</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos"> 154</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos"> 155</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">)</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos"> 156</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos"> 157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos"> 158</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">idim</span><span class="p">)</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos"> 159</span></a>
</span><span id="L-160"><a href="#L-160"><span class="linenos"> 160</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos"> 161</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward function.</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos"> 162</span></a>
</span><span id="L-163"><a href="#L-163"><span class="linenos"> 163</span></a><span class="sd">        Args:</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos"> 164</span></a><span class="sd">            xs: input tensor (B, L, D)</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos"> 165</span></a><span class="sd">        Returns:</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos"> 166</span></a><span class="sd">            output tensor, (B, L, D)</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos"> 167</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos"> 168</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">xs</span><span class="p">))))</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos"> 169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos"> 170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos"> 171</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Swish</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos"> 172</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct an Swish object.&quot;&quot;&quot;</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos"> 173</span></a>
</span><span id="L-174"><a href="#L-174"><span class="linenos"> 174</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos"> 175</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Return Swish activation function.&quot;&quot;&quot;</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos"> 176</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos"> 177</span></a>
</span><span id="L-178"><a href="#L-178"><span class="linenos"> 178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos"> 179</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos"> 180</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-Head Attention layer.</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos"> 181</span></a>
</span><span id="L-182"><a href="#L-182"><span class="linenos"> 182</span></a><span class="sd">    Args:</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos"> 183</span></a><span class="sd">        n_head (int): The number of heads.</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos"> 184</span></a><span class="sd">        n_feat (int): The number of features.</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos"> 185</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos"> 186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos"> 187</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos"> 188</span></a>
</span><span id="L-189"><a href="#L-189"><span class="linenos"> 189</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos"> 190</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos"> 191</span></a>    <span class="p">):</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos"> 192</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an MultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos"> 193</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos"> 194</span></a>        <span class="k">assert</span> <span class="n">n_feat</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos"> 195</span></a>        <span class="c1"># We assume d_v always equals d_k</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos"> 196</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">n_feat</span> <span class="o">//</span> <span class="n">n_head</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos"> 197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">n_head</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos"> 198</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos"> 199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">key_bias</span><span class="p">)</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos"> 200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos"> 201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos"> 202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos"> 203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos"> 204</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_qkv</span><span class="p">(</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos"> 205</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos"> 206</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos"> 207</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform query, key and value.</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos"> 208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos"> 209</span></a><span class="sd">        Args:</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos"> 210</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos"> 211</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos"> 212</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos"> 213</span></a>
</span><span id="L-214"><a href="#L-214"><span class="linenos"> 214</span></a><span class="sd">        Returns:</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos"> 215</span></a><span class="sd">            torch.Tensor: Transformed query tensor, size</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos"> 216</span></a><span class="sd">                (#batch, n_head, time1, d_k).</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos"> 217</span></a><span class="sd">            torch.Tensor: Transformed key tensor, size</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos"> 218</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos"> 219</span></a><span class="sd">            torch.Tensor: Transformed value tensor, size</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos"> 220</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos"> 221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos"> 222</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos"> 223</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos"> 224</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos"> 225</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos"> 226</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos"> 227</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos"> 228</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos"> 229</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos"> 230</span></a>        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos"> 231</span></a>
</span><span id="L-232"><a href="#L-232"><span class="linenos"> 232</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_attention</span><span class="p">(</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos"> 233</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos"> 234</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos"> 235</span></a>        <span class="n">scores</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos"> 236</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos"> 237</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos"> 238</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute attention context vector.</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos"> 239</span></a>
</span><span id="L-240"><a href="#L-240"><span class="linenos"> 240</span></a><span class="sd">        Args:</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos"> 241</span></a><span class="sd">            value (torch.Tensor): Transformed value, size</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos"> 242</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos"> 243</span></a><span class="sd">            scores (torch.Tensor): Attention score, size</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos"> 244</span></a><span class="sd">                (#batch, n_head, time1, time2).</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos"> 245</span></a><span class="sd">            mask (torch.Tensor): Mask, size (#batch, 1, time2) or</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos"> 246</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos"> 247</span></a>
</span><span id="L-248"><a href="#L-248"><span class="linenos"> 248</span></a><span class="sd">        Returns:</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos"> 249</span></a><span class="sd">            torch.Tensor: Transformed value (#batch, time1, d_model)</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos"> 250</span></a><span class="sd">                weighted by the attention score (#batch, time1, time2).</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos"> 251</span></a>
</span><span id="L-252"><a href="#L-252"><span class="linenos"> 252</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos"> 253</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos"> 254</span></a>
</span><span id="L-255"><a href="#L-255"><span class="linenos"> 255</span></a>        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time2 &gt; 0</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos"> 256</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos"> 257</span></a>            <span class="c1"># For last chunk, time2 might be larger than scores.size(-1)</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos"> 258</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos"> 259</span></a>            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos"> 260</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos"> 261</span></a>                <span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos"> 262</span></a>            <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos"> 263</span></a>
</span><span id="L-264"><a href="#L-264"><span class="linenos"> 264</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos"> 265</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos"> 266</span></a>
</span><span id="L-267"><a href="#L-267"><span class="linenos"> 267</span></a>        <span class="n">p_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos"> 268</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos"> 269</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos"> 270</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos"> 271</span></a>        <span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos"> 272</span></a>
</span><span id="L-273"><a href="#L-273"><span class="linenos"> 273</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos"> 274</span></a>
</span><span id="L-275"><a href="#L-275"><span class="linenos"> 275</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos"> 276</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos"> 277</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos"> 278</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos"> 279</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos"> 280</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos"> 281</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos"> 282</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos"> 283</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos"> 284</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute scaled dot product attention.</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos"> 285</span></a>
</span><span id="L-286"><a href="#L-286"><span class="linenos"> 286</span></a><span class="sd">        Args:</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos"> 287</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos"> 288</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos"> 289</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos"> 290</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos"> 291</span></a><span class="sd">                (#batch, time1, time2).</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos"> 292</span></a><span class="sd">                1.When applying cross attention between decoder and encoder,</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos"> 293</span></a><span class="sd">                the batch padding mask for input is in (#batch, 1, T) shape.</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos"> 294</span></a><span class="sd">                2.When applying self attention of encoder,</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos"> 295</span></a><span class="sd">                the mask is in (#batch, T, T)  shape.</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos"> 296</span></a><span class="sd">                3.When applying self attention of decoder,</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos"> 297</span></a><span class="sd">                the mask is in (#batch, L, L)  shape.</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos"> 298</span></a><span class="sd">                4.If the different position in decoder see different block</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos"> 299</span></a><span class="sd">                of the encoder, such as Mocha, the passed in mask could be</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos"> 300</span></a><span class="sd">                in (#batch, L, T) shape. But there is no such case in current</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos"> 301</span></a><span class="sd">                CosyVoice.</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos"> 302</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos"> 303</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos"> 304</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos"> 305</span></a>
</span><span id="L-306"><a href="#L-306"><span class="linenos"> 306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos"> 307</span></a><span class="sd">        Returns:</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos"> 308</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos"> 309</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos"> 310</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos"> 311</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos"> 312</span></a>
</span><span id="L-313"><a href="#L-313"><span class="linenos"> 313</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos"> 314</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos"> 315</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos"> 316</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos"> 317</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos"> 318</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos"> 319</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos"> 320</span></a>
</span><span id="L-321"><a href="#L-321"><span class="linenos"> 321</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos"> 322</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos"> 323</span></a>
</span><span id="L-324"><a href="#L-324"><span class="linenos"> 324</span></a>
</span><span id="L-325"><a href="#L-325"><span class="linenos"> 325</span></a><span class="k">class</span><span class="w"> </span><span class="nc">RelPositionMultiHeadedAttention</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos"> 326</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-Head Attention layer with relative position encoding.</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos"> 327</span></a><span class="sd">    Paper: https://arxiv.org/abs/1901.02860</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos"> 328</span></a><span class="sd">    Args:</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos"> 329</span></a><span class="sd">        n_head (int): The number of heads.</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos"> 330</span></a><span class="sd">        n_feat (int): The number of features.</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos"> 331</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos"> 332</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos"> 333</span></a>
</span><span id="L-334"><a href="#L-334"><span class="linenos"> 334</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos"> 335</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos"> 336</span></a>    <span class="p">):</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos"> 337</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an RelPositionMultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos"> 338</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos"> 339</span></a>        <span class="c1"># linear transformation for positional encoding</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos"> 340</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos"> 341</span></a>        <span class="c1"># these two learnable bias are used in matrix c and matrix d</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos"> 342</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos"> 343</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos"> 344</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos"> 345</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos"> 346</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos"> 347</span></a>
</span><span id="L-348"><a href="#L-348"><span class="linenos"> 348</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">rel_shift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos"> 349</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute relative positional encoding.</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos"> 350</span></a>
</span><span id="L-351"><a href="#L-351"><span class="linenos"> 351</span></a><span class="sd">        Args:</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos"> 352</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos"> 353</span></a><span class="sd">            time1 means the length of query vector.</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos"> 354</span></a>
</span><span id="L-355"><a href="#L-355"><span class="linenos"> 355</span></a><span class="sd">        Returns:</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos"> 356</span></a><span class="sd">            torch.Tensor: Output tensor.</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos"> 357</span></a>
</span><span id="L-358"><a href="#L-358"><span class="linenos"> 358</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos"> 359</span></a>        <span class="n">zero_pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos"> 360</span></a>            <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos"> 361</span></a>        <span class="p">)</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos"> 362</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">zero_pad</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos"> 363</span></a>
</span><span id="L-364"><a href="#L-364"><span class="linenos"> 364</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">x_padded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos"> 365</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x_padded</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos"> 366</span></a>            <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos"> 367</span></a>        <span class="p">]</span>  <span class="c1"># only keep the positions from 0 to time2</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos"> 368</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos"> 369</span></a>
</span><span id="L-370"><a href="#L-370"><span class="linenos"> 370</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos"> 371</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos"> 372</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos"> 373</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos"> 374</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos"> 375</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos"> 376</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos"> 377</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos"> 378</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos"> 379</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute &#39;Scaled Dot Product Attention&#39; with rel. positional encoding.</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos"> 380</span></a><span class="sd">        Args:</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos"> 381</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos"> 382</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos"> 383</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos"> 384</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos"> 385</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos"> 386</span></a><span class="sd">            pos_emb (torch.Tensor): Positional embedding tensor</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos"> 387</span></a><span class="sd">                (#batch, time2, size).</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos"> 388</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos"> 389</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos"> 390</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos"> 391</span></a><span class="sd">        Returns:</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos"> 392</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos"> 393</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos"> 394</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos"> 395</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos"> 396</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos"> 397</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos"> 398</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, time1, head, d_k)</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos"> 399</span></a>
</span><span id="L-400"><a href="#L-400"><span class="linenos"> 400</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos"> 401</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos"> 402</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos"> 403</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos"> 404</span></a>        <span class="c1"># NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it&#39;s</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos"> 405</span></a>        <span class="c1">#   non-trivial to calculate `next_cache_start` here.</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos"> 406</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos"> 407</span></a>
</span><span id="L-408"><a href="#L-408"><span class="linenos"> 408</span></a>        <span class="n">n_batch_pos</span> <span class="o">=</span> <span class="n">pos_emb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos"> 409</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos"> 410</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos"> 411</span></a>
</span><span id="L-412"><a href="#L-412"><span class="linenos"> 412</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos"> 413</span></a>        <span class="n">q_with_bias_u</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos"> 414</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos"> 415</span></a>        <span class="n">q_with_bias_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos"> 416</span></a>
</span><span id="L-417"><a href="#L-417"><span class="linenos"> 417</span></a>        <span class="c1"># compute attention score</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos"> 418</span></a>        <span class="c1"># first compute matrix a and matrix c</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos"> 419</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos"> 420</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos"> 421</span></a>        <span class="n">matrix_ac</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_u</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos"> 422</span></a>
</span><span id="L-423"><a href="#L-423"><span class="linenos"> 423</span></a>        <span class="c1"># compute matrix b and matrix d</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos"> 424</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos"> 425</span></a>        <span class="n">matrix_bd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_v</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos"> 426</span></a>        <span class="c1"># NOTE(Xiang Lyu): Keep rel_shift since espnet rel_pos_emb is used</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos"> 427</span></a>        <span class="k">if</span> <span class="n">matrix_ac</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">matrix_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos"> 428</span></a>            <span class="n">matrix_bd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rel_shift</span><span class="p">(</span><span class="n">matrix_bd</span><span class="p">)</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos"> 429</span></a>
</span><span id="L-430"><a href="#L-430"><span class="linenos"> 430</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix_ac</span> <span class="o">+</span> <span class="n">matrix_bd</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos"> 431</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos"> 432</span></a>        <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos"> 433</span></a>
</span><span id="L-434"><a href="#L-434"><span class="linenos"> 434</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos"> 435</span></a>
</span><span id="L-436"><a href="#L-436"><span class="linenos"> 436</span></a>
</span><span id="L-437"><a href="#L-437"><span class="linenos"> 437</span></a><span class="k">def</span><span class="w"> </span><span class="nf">subsequent_mask</span><span class="p">(</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos"> 438</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos"> 439</span></a>    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos"> 440</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos"> 441</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create mask for subsequent steps (size, size).</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos"> 442</span></a>
</span><span id="L-443"><a href="#L-443"><span class="linenos"> 443</span></a><span class="sd">    This mask is used only in decoder which works in an auto-regressive mode.</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos"> 444</span></a><span class="sd">    This means the current step could only do attention with its left steps.</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos"> 445</span></a>
</span><span id="L-446"><a href="#L-446"><span class="linenos"> 446</span></a><span class="sd">    In encoder, fully attention is used when streaming is not necessary and</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos"> 447</span></a><span class="sd">    the sequence is not long. In this  case, no attention mask is needed.</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos"> 448</span></a>
</span><span id="L-449"><a href="#L-449"><span class="linenos"> 449</span></a><span class="sd">    When streaming is need, chunk-based attention is used in encoder. See</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos"> 450</span></a><span class="sd">    subsequent_chunk_mask for the chunk-based attention mask.</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos"> 451</span></a>
</span><span id="L-452"><a href="#L-452"><span class="linenos"> 452</span></a><span class="sd">    Args:</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos"> 453</span></a><span class="sd">        size (int): size of mask</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos"> 454</span></a><span class="sd">        str device (str): &quot;cpu&quot; or &quot;cuda&quot; or torch.Tensor.device</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos"> 455</span></a><span class="sd">        dtype (torch.device): result dtype</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos"> 456</span></a>
</span><span id="L-457"><a href="#L-457"><span class="linenos"> 457</span></a><span class="sd">    Returns:</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos"> 458</span></a><span class="sd">        torch.Tensor: mask</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos"> 459</span></a>
</span><span id="L-460"><a href="#L-460"><span class="linenos"> 460</span></a><span class="sd">    Examples:</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos"> 461</span></a><span class="sd">        &gt;&gt;&gt; subsequent_mask(3)</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos"> 462</span></a><span class="sd">        [[1, 0, 0],</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos"> 463</span></a><span class="sd">         [1, 1, 0],</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos"> 464</span></a><span class="sd">         [1, 1, 1]]</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos"> 465</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos"> 466</span></a>    <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-467"><a href="#L-467"><span class="linenos"> 467</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos"> 468</span></a>    <span class="n">arange</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos"> 469</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">&lt;=</span> <span class="n">arange</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos"> 470</span></a>    <span class="k">return</span> <span class="n">mask</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos"> 471</span></a>
</span><span id="L-472"><a href="#L-472"><span class="linenos"> 472</span></a>
</span><span id="L-473"><a href="#L-473"><span class="linenos"> 473</span></a><span class="k">def</span><span class="w"> </span><span class="nf">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos"> 474</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos"> 475</span></a>    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos"> 476</span></a>    <span class="n">num_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos"> 477</span></a>    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos"> 478</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos"> 479</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create mask for subsequent steps (size, size) with chunk size,</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos"> 480</span></a><span class="sd">       this is for streaming encoder</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos"> 481</span></a>
</span><span id="L-482"><a href="#L-482"><span class="linenos"> 482</span></a><span class="sd">    Args:</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos"> 483</span></a><span class="sd">        size (int): size of mask</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos"> 484</span></a><span class="sd">        chunk_size (int): size of chunk</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos"> 485</span></a><span class="sd">        num_left_chunks (int): number of left chunks</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos"> 486</span></a><span class="sd">            &lt;0: use full chunk</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos"> 487</span></a><span class="sd">            &gt;=0: use num_left_chunks</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos"> 488</span></a><span class="sd">        device (torch.device): &quot;cpu&quot; or &quot;cuda&quot; or torch.Tensor.device</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos"> 489</span></a>
</span><span id="L-490"><a href="#L-490"><span class="linenos"> 490</span></a><span class="sd">    Returns:</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos"> 491</span></a><span class="sd">        torch.Tensor: mask</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos"> 492</span></a>
</span><span id="L-493"><a href="#L-493"><span class="linenos"> 493</span></a><span class="sd">    Examples:</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos"> 494</span></a><span class="sd">        &gt;&gt;&gt; subsequent_chunk_mask(4, 2)</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos"> 495</span></a><span class="sd">        [[1, 1, 0, 0],</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos"> 496</span></a><span class="sd">         [1, 1, 0, 0],</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos"> 497</span></a><span class="sd">         [1, 1, 1, 1],</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos"> 498</span></a><span class="sd">         [1, 1, 1, 1]]</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos"> 499</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos"> 500</span></a>    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos"> 501</span></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos"> 502</span></a>        <span class="k">if</span> <span class="n">num_left_chunks</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos"> 503</span></a>            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos"> 504</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos"> 505</span></a>            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="n">num_left_chunks</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos"> 506</span></a>        <span class="n">ending</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="n">chunk_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos"> 507</span></a>        <span class="n">ret</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">ending</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos"> 508</span></a>    <span class="k">return</span> <span class="n">ret</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos"> 509</span></a>
</span><span id="L-510"><a href="#L-510"><span class="linenos"> 510</span></a>
</span><span id="L-511"><a href="#L-511"><span class="linenos"> 511</span></a><span class="k">def</span><span class="w"> </span><span class="nf">add_optional_chunk_mask</span><span class="p">(</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos"> 512</span></a>    <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos"> 513</span></a>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos"> 514</span></a>    <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos"> 515</span></a>    <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos"> 516</span></a>    <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos"> 517</span></a>    <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos"> 518</span></a>    <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos"> 519</span></a>    <span class="n">enable_full_context</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos"> 520</span></a><span class="p">):</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos"> 521</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply optional mask for encoder.</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos"> 522</span></a>
</span><span id="L-523"><a href="#L-523"><span class="linenos"> 523</span></a><span class="sd">    Args:</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos"> 524</span></a><span class="sd">        xs (torch.Tensor): padded input, (B, L, D), L for max length</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos"> 525</span></a><span class="sd">        mask (torch.Tensor): mask for xs, (B, 1, L)</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos"> 526</span></a><span class="sd">        use_dynamic_chunk (bool): whether to use dynamic chunk or not</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos"> 527</span></a><span class="sd">        use_dynamic_left_chunk (bool): whether to use dynamic left chunk for</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos"> 528</span></a><span class="sd">            training.</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos"> 529</span></a><span class="sd">        decoding_chunk_size (int): decoding chunk size for dynamic chunk, it&#39;s</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos"> 530</span></a><span class="sd">            0: default for training, use random dynamic chunk.</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos"> 531</span></a><span class="sd">            &lt;0: for decoding, use full chunk.</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos"> 532</span></a><span class="sd">            &gt;0: for decoding, use fixed chunk size as set.</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos"> 533</span></a><span class="sd">        static_chunk_size (int): chunk size for static chunk training/decoding</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos"> 534</span></a><span class="sd">            if it&#39;s greater than 0, if use_dynamic_chunk is true,</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos"> 535</span></a><span class="sd">            this parameter will be ignored</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos"> 536</span></a><span class="sd">        num_decoding_left_chunks: number of left chunks, this is for decoding,</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos"> 537</span></a><span class="sd">            the chunk size is decoding_chunk_size.</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos"> 538</span></a><span class="sd">            &gt;=0: use num_decoding_left_chunks</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos"> 539</span></a><span class="sd">            &lt;0: use all left chunks</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos"> 540</span></a><span class="sd">        enable_full_context (bool):</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos"> 541</span></a><span class="sd">            True: chunk size is either [1, 25] or full context(max_len)</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos"> 542</span></a><span class="sd">            False: chunk size ~ U[1, 25]</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos"> 543</span></a>
</span><span id="L-544"><a href="#L-544"><span class="linenos"> 544</span></a><span class="sd">    Returns:</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos"> 545</span></a><span class="sd">        torch.Tensor: chunk mask of the input xs.</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos"> 546</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos"> 547</span></a>    <span class="c1"># Whether to use chunk mask or not</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos"> 548</span></a>    <span class="k">if</span> <span class="n">use_dynamic_chunk</span><span class="p">:</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos"> 549</span></a>        <span class="n">max_len</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos"> 550</span></a>        <span class="k">if</span> <span class="n">decoding_chunk_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos"> 551</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">max_len</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos"> 552</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos"> 553</span></a>        <span class="k">elif</span> <span class="n">decoding_chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos"> 554</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">decoding_chunk_size</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos"> 555</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">num_decoding_left_chunks</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos"> 556</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos"> 557</span></a>            <span class="c1"># chunk size is either [1, 25] or full context(max_len).</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos"> 558</span></a>            <span class="c1"># Since we use 4 times subsampling and allow up to 1s(100 frames)</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos"> 559</span></a>            <span class="c1"># delay, the maximum frame is 100 / 4 = 25.</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos"> 560</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos"> 561</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos"> 562</span></a>            <span class="k">if</span> <span class="n">chunk_size</span> <span class="o">&gt;</span> <span class="n">max_len</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">enable_full_context</span><span class="p">:</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos"> 563</span></a>                <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">max_len</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos"> 564</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-565"><a href="#L-565"><span class="linenos"> 565</span></a>                <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">chunk_size</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos"> 566</span></a>                <span class="k">if</span> <span class="n">use_dynamic_left_chunk</span><span class="p">:</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos"> 567</span></a>                    <span class="n">max_left_chunks</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">chunk_size</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos"> 568</span></a>                    <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_left_chunks</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos"> 569</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos"> 570</span></a>            <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">num_left_chunks</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos"> 571</span></a>        <span class="p">)</span>  <span class="c1"># (L, L)</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos"> 572</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">chunk_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos"> 573</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&amp;</span> <span class="n">chunk_masks</span>  <span class="c1"># (B, L, L)</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos"> 574</span></a>    <span class="k">elif</span> <span class="n">static_chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos"> 575</span></a>        <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">num_decoding_left_chunks</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos"> 576</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos"> 577</span></a>            <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">static_chunk_size</span><span class="p">,</span> <span class="n">num_left_chunks</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos"> 578</span></a>        <span class="p">)</span>  <span class="c1"># (L, L)</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos"> 579</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">chunk_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos"> 580</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&amp;</span> <span class="n">chunk_masks</span>  <span class="c1"># (B, L, L)</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos"> 581</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos"> 582</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos"> 583</span></a>    <span class="k">return</span> <span class="n">chunk_masks</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos"> 584</span></a>
</span><span id="L-585"><a href="#L-585"><span class="linenos"> 585</span></a>
</span><span id="L-586"><a href="#L-586"><span class="linenos"> 586</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos"> 587</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Encoder layer module.</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos"> 588</span></a><span class="sd">    Args:</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos"> 589</span></a><span class="sd">        size (int): Input dimension.</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos"> 590</span></a><span class="sd">        self_attn (torch.nn.Module): Self-attention module instance.</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos"> 591</span></a><span class="sd">            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention`</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos"> 592</span></a><span class="sd">            instance can be used as the argument.</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos"> 593</span></a><span class="sd">        feed_forward (torch.nn.Module): Feed-forward module instance.</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos"> 594</span></a><span class="sd">            `PositionwiseFeedForward` instance can be used as the argument.</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos"> 595</span></a><span class="sd">        feed_forward_macaron (torch.nn.Module): Additional feed-forward module</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos"> 596</span></a><span class="sd">             instance.</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos"> 597</span></a><span class="sd">            `PositionwiseFeedForward` instance can be used as the argument.</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos"> 598</span></a><span class="sd">        conv_module (torch.nn.Module): Convolution module instance.</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos"> 599</span></a><span class="sd">            `ConvlutionModule` instance can be used as the argument.</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos"> 600</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos"> 601</span></a><span class="sd">        normalize_before (bool):</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos"> 602</span></a><span class="sd">            True: use layer_norm before each sub-block.</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos"> 603</span></a><span class="sd">            False: use layer_norm after each sub-block.</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos"> 604</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos"> 605</span></a>
</span><span id="L-606"><a href="#L-606"><span class="linenos"> 606</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos"> 607</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos"> 608</span></a>        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos"> 609</span></a>        <span class="n">self_attn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos"> 610</span></a>        <span class="n">feed_forward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos"> 611</span></a>        <span class="n">feed_forward_macaron</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos"> 612</span></a>        <span class="n">conv_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos"> 613</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos"> 614</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos"> 615</span></a>    <span class="p">):</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos"> 616</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an EncoderLayer object.&quot;&quot;&quot;</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos"> 617</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos"> 618</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
</span><span id="L-619"><a href="#L-619"><span class="linenos"> 619</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos"> 620</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="o">=</span> <span class="n">feed_forward_macaron</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos"> 621</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="o">=</span> <span class="n">conv_module</span>
</span><span id="L-622"><a href="#L-622"><span class="linenos"> 622</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the FNN module</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos"> 623</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the MHA module</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos"> 624</span></a>        <span class="k">if</span> <span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos"> 625</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos"> 626</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos"> 627</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-628"><a href="#L-628"><span class="linenos"> 628</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos"> 629</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos"> 630</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the CNN module</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos"> 631</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
</span><span id="L-632"><a href="#L-632"><span class="linenos"> 632</span></a>                <span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos"> 633</span></a>            <span class="p">)</span>  <span class="c1"># for the final output of the block</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos"> 634</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-635"><a href="#L-635"><span class="linenos"> 635</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos"> 636</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos"> 637</span></a>
</span><span id="L-638"><a href="#L-638"><span class="linenos"> 638</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos"> 639</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos"> 640</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos"> 641</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos"> 642</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-643"><a href="#L-643"><span class="linenos"> 643</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos"> 644</span></a>        <span class="n">att_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos"> 645</span></a>        <span class="n">cnn_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos"> 646</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos"> 647</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute encoded features.</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos"> 648</span></a>
</span><span id="L-649"><a href="#L-649"><span class="linenos"> 649</span></a><span class="sd">        Args:</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos"> 650</span></a><span class="sd">            x (torch.Tensor): (#batch, time, size)</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos"> 651</span></a><span class="sd">            mask (torch.Tensor): Mask tensor for the input (#batch, timetime),</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos"> 652</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos"> 653</span></a><span class="sd">            pos_emb (torch.Tensor): positional encoding, must not be None</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos"> 654</span></a><span class="sd">                for ConformerEncoderLayer.</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos"> 655</span></a><span class="sd">            mask_pad (torch.Tensor): batch padding mask used for conv module.</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos"> 656</span></a><span class="sd">                (#batch, 1time), (0, 0, 0) means fake mask.</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos"> 657</span></a><span class="sd">            att_cache (torch.Tensor): Cache tensor of the KEY &amp; VALUE</span>
</span><span id="L-658"><a href="#L-658"><span class="linenos"> 658</span></a><span class="sd">                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos"> 659</span></a><span class="sd">            cnn_cache (torch.Tensor): Convolution cache in conformer layer</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos"> 660</span></a><span class="sd">                (#batch=1, size, cache_t2)</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos"> 661</span></a><span class="sd">        Returns:</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos"> 662</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, size).</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos"> 663</span></a><span class="sd">            torch.Tensor: Mask tensor (#batch, time, time).</span>
</span><span id="L-664"><a href="#L-664"><span class="linenos"> 664</span></a><span class="sd">            torch.Tensor: att_cache tensor,</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos"> 665</span></a><span class="sd">                (#batch=1, head, cache_t1 + time, d_k * 2).</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos"> 666</span></a><span class="sd">            torch.Tensor: cnn_cahce tensor (#batch, size, cache_t2).</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos"> 667</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos"> 668</span></a>
</span><span id="L-669"><a href="#L-669"><span class="linenos"> 669</span></a>        <span class="c1"># whether to use macaron style</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos"> 670</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos"> 671</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-672"><a href="#L-672"><span class="linenos"> 672</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-673"><a href="#L-673"><span class="linenos"> 673</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos"> 674</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos"> 675</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos"> 676</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos"> 677</span></a>
</span><span id="L-678"><a href="#L-678"><span class="linenos"> 678</span></a>        <span class="c1"># multi-headed self-attention module</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos"> 679</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos"> 680</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos"> 681</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos"> 682</span></a>        <span class="n">x_att</span><span class="p">,</span> <span class="n">new_att_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">att_cache</span><span class="p">)</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos"> 683</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_att</span><span class="p">)</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos"> 684</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos"> 685</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos"> 686</span></a>
</span><span id="L-687"><a href="#L-687"><span class="linenos"> 687</span></a>        <span class="c1"># convolution module</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos"> 688</span></a>        <span class="c1"># Fake new cnn cache here, and then change it in conv_module</span>
</span><span id="L-689"><a href="#L-689"><span class="linenos"> 689</span></a>        <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos"> 690</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos"> 691</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos"> 692</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos"> 693</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos"> 694</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos"> 695</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos"> 696</span></a>
</span><span id="L-697"><a href="#L-697"><span class="linenos"> 697</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos"> 698</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos"> 699</span></a>
</span><span id="L-700"><a href="#L-700"><span class="linenos"> 700</span></a>        <span class="c1"># feed forward module</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos"> 701</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos"> 702</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos"> 703</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos"> 704</span></a>
</span><span id="L-705"><a href="#L-705"><span class="linenos"> 705</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos"> 706</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos"> 707</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos"> 708</span></a>
</span><span id="L-709"><a href="#L-709"><span class="linenos"> 709</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos"> 710</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos"> 711</span></a>
</span><span id="L-712"><a href="#L-712"><span class="linenos"> 712</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">new_att_cache</span><span class="p">,</span> <span class="n">new_cnn_cache</span>
</span><span id="L-713"><a href="#L-713"><span class="linenos"> 713</span></a>
</span><span id="L-714"><a href="#L-714"><span class="linenos"> 714</span></a>
</span><span id="L-715"><a href="#L-715"><span class="linenos"> 715</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EspnetRelPositionalEncoding</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos"> 716</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Relative positional encoding module (new implementation).</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos"> 717</span></a>
</span><span id="L-718"><a href="#L-718"><span class="linenos"> 718</span></a><span class="sd">    Details can be found in https://github.com/espnet/espnet/pull/2816.</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos"> 719</span></a>
</span><span id="L-720"><a href="#L-720"><span class="linenos"> 720</span></a><span class="sd">    See : Appendix B in https://arxiv.org/abs/1901.02860</span>
</span><span id="L-721"><a href="#L-721"><span class="linenos"> 721</span></a>
</span><span id="L-722"><a href="#L-722"><span class="linenos"> 722</span></a><span class="sd">    Args:</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos"> 723</span></a><span class="sd">        d_model (int): Embedding dimension.</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos"> 724</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos"> 725</span></a><span class="sd">        max_len (int): Maximum input length.</span>
</span><span id="L-726"><a href="#L-726"><span class="linenos"> 726</span></a>
</span><span id="L-727"><a href="#L-727"><span class="linenos"> 727</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos"> 728</span></a>
</span><span id="L-729"><a href="#L-729"><span class="linenos"> 729</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos"> 730</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an PositionalEncoding object.&quot;&quot;&quot;</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos"> 731</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">EspnetRelPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos"> 732</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos"> 733</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos"> 734</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos"> 735</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-736"><a href="#L-736"><span class="linenos"> 736</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">))</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos"> 737</span></a>
</span><span id="L-738"><a href="#L-738"><span class="linenos"> 738</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">extend_pe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos"> 739</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the positional encodings.&quot;&quot;&quot;</span>
</span><span id="L-740"><a href="#L-740"><span class="linenos"> 740</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos"> 741</span></a>            <span class="c1"># self.pe contains both positive and negative parts</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos"> 742</span></a>            <span class="c1"># the length of self.pe is 2 * input_len - 1</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos"> 743</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-744"><a href="#L-744"><span class="linenos"> 744</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos"> 745</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos"> 746</span></a>                <span class="k">return</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos"> 747</span></a>        <span class="c1"># Suppose `i` means to the position of query vecotr and `j` means the</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos"> 748</span></a>        <span class="c1"># position of key vector. We use position relative positions when keys</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos"> 749</span></a>        <span class="c1"># are to the left (i&gt;j) and negative relative positions otherwise (i&lt;j).</span>
</span><span id="L-750"><a href="#L-750"><span class="linenos"> 750</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos"> 751</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos"> 752</span></a>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos"> 753</span></a>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos"> 754</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="L-755"><a href="#L-755"><span class="linenos"> 755</span></a>            <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos"> 756</span></a>        <span class="p">)</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos"> 757</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos"> 758</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos"> 759</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="L-760"><a href="#L-760"><span class="linenos"> 760</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos"> 761</span></a>
</span><span id="L-762"><a href="#L-762"><span class="linenos"> 762</span></a>        <span class="c1"># Reserve the order of positive indices and concat both positive and</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos"> 763</span></a>        <span class="c1"># negative indices. This is used to support the shifting trick</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos"> 764</span></a>        <span class="c1"># as in https://arxiv.org/abs/1901.02860</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos"> 765</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">pe_positive</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos"> 766</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">pe_negative</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos"> 767</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pe_positive</span><span class="p">,</span> <span class="n">pe_negative</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos"> 768</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos"> 769</span></a>
</span><span id="L-770"><a href="#L-770"><span class="linenos"> 770</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos"> 771</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos"> 772</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos"> 773</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Add positional encoding.</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos"> 774</span></a>
</span><span id="L-775"><a href="#L-775"><span class="linenos"> 775</span></a><span class="sd">        Args:</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos"> 776</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, time, `*`).</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos"> 777</span></a>
</span><span id="L-778"><a href="#L-778"><span class="linenos"> 778</span></a><span class="sd">        Returns:</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos"> 779</span></a><span class="sd">            torch.Tensor: Encoded tensor (batch, time, `*`).</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos"> 780</span></a>
</span><span id="L-781"><a href="#L-781"><span class="linenos"> 781</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos"> 782</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos"> 783</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos"> 784</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">)</span>
</span><span id="L-785"><a href="#L-785"><span class="linenos"> 785</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span>
</span><span id="L-786"><a href="#L-786"><span class="linenos"> 786</span></a>
</span><span id="L-787"><a href="#L-787"><span class="linenos"> 787</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="L-788"><a href="#L-788"><span class="linenos"> 788</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos"> 789</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-790"><a href="#L-790"><span class="linenos"> 790</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;For getting encoding in a streaming fashion</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos"> 791</span></a>
</span><span id="L-792"><a href="#L-792"><span class="linenos"> 792</span></a><span class="sd">        Attention!!!!!</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos"> 793</span></a><span class="sd">        we apply dropout only once at the whole utterance level in a none</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos"> 794</span></a><span class="sd">        streaming way, but will call this function several times with</span>
</span><span id="L-795"><a href="#L-795"><span class="linenos"> 795</span></a><span class="sd">        increasing input size in a streaming scenario, so the dropout will</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos"> 796</span></a><span class="sd">        be applied several times.</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos"> 797</span></a>
</span><span id="L-798"><a href="#L-798"><span class="linenos"> 798</span></a><span class="sd">        Args:</span>
</span><span id="L-799"><a href="#L-799"><span class="linenos"> 799</span></a><span class="sd">            offset (int or torch.tensor): start offset</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos"> 800</span></a><span class="sd">            size (int): required size of position encoding</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos"> 801</span></a>
</span><span id="L-802"><a href="#L-802"><span class="linenos"> 802</span></a><span class="sd">        Returns:</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos"> 803</span></a><span class="sd">            torch.Tensor: Corresponding encoding</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos"> 804</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-805"><a href="#L-805"><span class="linenos"> 805</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos"> 806</span></a>            <span class="p">:,</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos"> 807</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">size</span><span class="p">,</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos"> 808</span></a>        <span class="p">]</span>
</span><span id="L-809"><a href="#L-809"><span class="linenos"> 809</span></a>        <span class="k">return</span> <span class="n">pos_emb</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos"> 810</span></a>
</span><span id="L-811"><a href="#L-811"><span class="linenos"> 811</span></a>
</span><span id="L-812"><a href="#L-812"><span class="linenos"> 812</span></a><span class="k">class</span><span class="w"> </span><span class="nc">LinearEmbed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-813"><a href="#L-813"><span class="linenos"> 813</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear transform the input without subsampling</span>
</span><span id="L-814"><a href="#L-814"><span class="linenos"> 814</span></a>
</span><span id="L-815"><a href="#L-815"><span class="linenos"> 815</span></a><span class="sd">    Args:</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos"> 816</span></a><span class="sd">        idim (int): Input dimension.</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos"> 817</span></a><span class="sd">        odim (int): Output dimension.</span>
</span><span id="L-818"><a href="#L-818"><span class="linenos"> 818</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos"> 819</span></a>
</span><span id="L-820"><a href="#L-820"><span class="linenos"> 820</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos"> 821</span></a>
</span><span id="L-822"><a href="#L-822"><span class="linenos"> 822</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos"> 823</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">odim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">pos_enc_class</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
</span><span id="L-824"><a href="#L-824"><span class="linenos"> 824</span></a>    <span class="p">):</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos"> 825</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an linear object.&quot;&quot;&quot;</span>
</span><span id="L-826"><a href="#L-826"><span class="linenos"> 826</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos"> 827</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-828"><a href="#L-828"><span class="linenos"> 828</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">),</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos"> 829</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos"> 830</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos"> 831</span></a>        <span class="p">)</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos"> 832</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">pos_enc_class</span>  <span class="c1"># rel_pos_espnet</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos"> 833</span></a>
</span><span id="L-834"><a href="#L-834"><span class="linenos"> 834</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="L-835"><a href="#L-835"><span class="linenos"> 835</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos"> 836</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos"> 837</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos"> 838</span></a>
</span><span id="L-839"><a href="#L-839"><span class="linenos"> 839</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos"> 840</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos"> 841</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-842"><a href="#L-842"><span class="linenos"> 842</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Input x.</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos"> 843</span></a>
</span><span id="L-844"><a href="#L-844"><span class="linenos"> 844</span></a><span class="sd">        Args:</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos"> 845</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, idim).</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos"> 846</span></a><span class="sd">            x_mask (torch.Tensor): Input mask (#batch, 1, time).</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos"> 847</span></a>
</span><span id="L-848"><a href="#L-848"><span class="linenos"> 848</span></a><span class="sd">        Returns:</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos"> 849</span></a><span class="sd">            torch.Tensor: linear input tensor (#batch, time&#39;, odim),</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos"> 850</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="L-851"><a href="#L-851"><span class="linenos"> 851</span></a><span class="sd">            torch.Tensor: linear input mask (#batch, 1, time&#39;),</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos"> 852</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="L-853"><a href="#L-853"><span class="linenos"> 853</span></a>
</span><span id="L-854"><a href="#L-854"><span class="linenos"> 854</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos"> 855</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos"> 856</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos"> 857</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos"> 858</span></a>
</span><span id="L-859"><a href="#L-859"><span class="linenos"> 859</span></a>
</span><span id="L-860"><a href="#L-860"><span class="linenos"> 860</span></a><span class="n">ATTENTION_CLASSES</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-861"><a href="#L-861"><span class="linenos"> 861</span></a>    <span class="s2">&quot;selfattn&quot;</span><span class="p">:</span> <span class="n">MultiHeadedAttention</span><span class="p">,</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos"> 862</span></a>    <span class="s2">&quot;rel_selfattn&quot;</span><span class="p">:</span> <span class="n">RelPositionMultiHeadedAttention</span><span class="p">,</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos"> 863</span></a><span class="p">}</span>
</span><span id="L-864"><a href="#L-864"><span class="linenos"> 864</span></a>
</span><span id="L-865"><a href="#L-865"><span class="linenos"> 865</span></a><span class="n">ACTIVATION_CLASSES</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos"> 866</span></a>    <span class="s2">&quot;hardtanh&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">,</span>
</span><span id="L-867"><a href="#L-867"><span class="linenos"> 867</span></a>    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos"> 868</span></a>    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos"> 869</span></a>    <span class="s2">&quot;selu&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">,</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos"> 870</span></a>    <span class="s2">&quot;swish&quot;</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="p">,</span> <span class="s2">&quot;SiLU&quot;</span><span class="p">,</span> <span class="n">Swish</span><span class="p">),</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos"> 871</span></a>    <span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos"> 872</span></a><span class="p">}</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos"> 873</span></a>
</span><span id="L-874"><a href="#L-874"><span class="linenos"> 874</span></a>
</span><span id="L-875"><a href="#L-875"><span class="linenos"> 875</span></a><span class="k">def</span><span class="w"> </span><span class="nf">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos"> 876</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Make mask tensor containing indices of padded part.</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos"> 877</span></a>
</span><span id="L-878"><a href="#L-878"><span class="linenos"> 878</span></a><span class="sd">    See description of make_non_pad_mask.</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos"> 879</span></a>
</span><span id="L-880"><a href="#L-880"><span class="linenos"> 880</span></a><span class="sd">    Args:</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos"> 881</span></a><span class="sd">        lengths (torch.Tensor): Batch of lengths (B,).</span>
</span><span id="L-882"><a href="#L-882"><span class="linenos"> 882</span></a><span class="sd">    Returns:</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos"> 883</span></a><span class="sd">        torch.Tensor: Mask tensor containing indices of padded part.</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos"> 884</span></a>
</span><span id="L-885"><a href="#L-885"><span class="linenos"> 885</span></a><span class="sd">    Examples:</span>
</span><span id="L-886"><a href="#L-886"><span class="linenos"> 886</span></a><span class="sd">        &gt;&gt;&gt; lengths = [5, 3, 2]</span>
</span><span id="L-887"><a href="#L-887"><span class="linenos"> 887</span></a><span class="sd">        &gt;&gt;&gt; make_pad_mask(lengths)</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos"> 888</span></a><span class="sd">        masks = [[0, 0, 0, 0 ,0],</span>
</span><span id="L-889"><a href="#L-889"><span class="linenos"> 889</span></a><span class="sd">                 [0, 0, 0, 1, 1],</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos"> 890</span></a><span class="sd">                 [0, 0, 1, 1, 1]]</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos"> 891</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-892"><a href="#L-892"><span class="linenos"> 892</span></a>    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos"> 893</span></a>    <span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span> <span class="k">if</span> <span class="n">max_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="L-894"><a href="#L-894"><span class="linenos"> 894</span></a>    <span class="n">seq_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-895"><a href="#L-895"><span class="linenos"> 895</span></a>    <span class="n">seq_range_expand</span> <span class="o">=</span> <span class="n">seq_range</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos"> 896</span></a>    <span class="n">seq_length_expand</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-897"><a href="#L-897"><span class="linenos"> 897</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">seq_range_expand</span> <span class="o">&gt;=</span> <span class="n">seq_length_expand</span>
</span><span id="L-898"><a href="#L-898"><span class="linenos"> 898</span></a>    <span class="k">return</span> <span class="n">mask</span>
</span><span id="L-899"><a href="#L-899"><span class="linenos"> 899</span></a>
</span><span id="L-900"><a href="#L-900"><span class="linenos"> 900</span></a>
</span><span id="L-901"><a href="#L-901"><span class="linenos"> 901</span></a><span class="c1"># https://github.com/FunAudioLLM/CosyVoice/blob/main/examples/magicdata-read/cosyvoice/conf/cosyvoice.yaml</span>
</span><span id="L-902"><a href="#L-902"><span class="linenos"> 902</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConformerEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-903"><a href="#L-903"><span class="linenos"> 903</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Conformer encoder module.&quot;&quot;&quot;</span>
</span><span id="L-904"><a href="#L-904"><span class="linenos"> 904</span></a>
</span><span id="L-905"><a href="#L-905"><span class="linenos"> 905</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-906"><a href="#L-906"><span class="linenos"> 906</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos"> 907</span></a>        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-908"><a href="#L-908"><span class="linenos"> 908</span></a>        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
</span><span id="L-909"><a href="#L-909"><span class="linenos"> 909</span></a>        <span class="n">attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="L-910"><a href="#L-910"><span class="linenos"> 910</span></a>        <span class="n">linear_units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos"> 911</span></a>        <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
</span><span id="L-912"><a href="#L-912"><span class="linenos"> 912</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-913"><a href="#L-913"><span class="linenos"> 913</span></a>        <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-914"><a href="#L-914"><span class="linenos"> 914</span></a>        <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="L-915"><a href="#L-915"><span class="linenos"> 915</span></a>        <span class="n">input_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos"> 916</span></a>        <span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_pos_espnet&quot;</span><span class="p">,</span>
</span><span id="L-917"><a href="#L-917"><span class="linenos"> 917</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos"> 918</span></a>        <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># 1: causal_mask; 0: full_mask</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos"> 919</span></a>        <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos"> 920</span></a>        <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos"> 921</span></a>        <span class="n">positionwise_conv_kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="L-922"><a href="#L-922"><span class="linenos"> 922</span></a>        <span class="n">macaron_style</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-923"><a href="#L-923"><span class="linenos"> 923</span></a>        <span class="n">selfattention_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_selfattn&quot;</span><span class="p">,</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos"> 924</span></a>        <span class="n">activation_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;swish&quot;</span><span class="p">,</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos"> 925</span></a>        <span class="n">use_cnn_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos"> 926</span></a>        <span class="n">cnn_module_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos"> 927</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-928"><a href="#L-928"><span class="linenos"> 928</span></a>        <span class="n">cnn_module_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="L-929"><a href="#L-929"><span class="linenos"> 929</span></a>        <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-930"><a href="#L-930"><span class="linenos"> 930</span></a>        <span class="n">gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos"> 931</span></a>    <span class="p">):</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos"> 932</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct ConformerEncoder</span>
</span><span id="L-933"><a href="#L-933"><span class="linenos"> 933</span></a>
</span><span id="L-934"><a href="#L-934"><span class="linenos"> 934</span></a><span class="sd">        Args:</span>
</span><span id="L-935"><a href="#L-935"><span class="linenos"> 935</span></a><span class="sd">            input_size to use_dynamic_chunk, see in BaseEncoder</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos"> 936</span></a><span class="sd">            positionwise_conv_kernel_size (int): Kernel size of positionwise</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos"> 937</span></a><span class="sd">                conv1d layer.</span>
</span><span id="L-938"><a href="#L-938"><span class="linenos"> 938</span></a><span class="sd">            macaron_style (bool): Whether to use macaron style for</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos"> 939</span></a><span class="sd">                positionwise layer.</span>
</span><span id="L-940"><a href="#L-940"><span class="linenos"> 940</span></a><span class="sd">            selfattention_layer_type (str): Encoder attention layer type,</span>
</span><span id="L-941"><a href="#L-941"><span class="linenos"> 941</span></a><span class="sd">                the parameter has no effect now, it&#39;s just for configure</span>
</span><span id="L-942"><a href="#L-942"><span class="linenos"> 942</span></a><span class="sd">                compatibility. #&#39;rel_selfattn&#39;</span>
</span><span id="L-943"><a href="#L-943"><span class="linenos"> 943</span></a><span class="sd">            activation_type (str): Encoder activation function type.</span>
</span><span id="L-944"><a href="#L-944"><span class="linenos"> 944</span></a><span class="sd">            use_cnn_module (bool): Whether to use convolution module.</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos"> 945</span></a><span class="sd">            cnn_module_kernel (int): Kernel size of convolution module.</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos"> 946</span></a><span class="sd">            causal (bool): whether to use causal convolution or not.</span>
</span><span id="L-947"><a href="#L-947"><span class="linenos"> 947</span></a><span class="sd">            key_bias: whether use bias in attention.linear_k, False for whisper models.</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos"> 948</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos"> 949</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-950"><a href="#L-950"><span class="linenos"> 950</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos"> 951</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">LinearEmbed</span><span class="p">(</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos"> 952</span></a>            <span class="n">input_size</span><span class="p">,</span>
</span><span id="L-953"><a href="#L-953"><span class="linenos"> 953</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="L-954"><a href="#L-954"><span class="linenos"> 954</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="L-955"><a href="#L-955"><span class="linenos"> 955</span></a>            <span class="n">EspnetRelPositionalEncoding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">positional_dropout_rate</span><span class="p">),</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos"> 956</span></a>        <span class="p">)</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos"> 957</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos"> 958</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="L-959"><a href="#L-959"><span class="linenos"> 959</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">gradient_checkpointing</span>
</span><span id="L-960"><a href="#L-960"><span class="linenos"> 960</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos"> 961</span></a>
</span><span id="L-962"><a href="#L-962"><span class="linenos"> 962</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span> <span class="o">=</span> <span class="n">static_chunk_size</span>
</span><span id="L-963"><a href="#L-963"><span class="linenos"> 963</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos"> 964</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_left_chunk</span>
</span><span id="L-965"><a href="#L-965"><span class="linenos"> 965</span></a>        <span class="n">activation</span> <span class="o">=</span> <span class="n">ACTIVATION_CLASSES</span><span class="p">[</span><span class="n">activation_type</span><span class="p">]()</span>
</span><span id="L-966"><a href="#L-966"><span class="linenos"> 966</span></a>
</span><span id="L-967"><a href="#L-967"><span class="linenos"> 967</span></a>        <span class="c1"># self-attention module definition</span>
</span><span id="L-968"><a href="#L-968"><span class="linenos"> 968</span></a>        <span class="n">encoder_selfattn_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-969"><a href="#L-969"><span class="linenos"> 969</span></a>            <span class="n">attention_heads</span><span class="p">,</span>
</span><span id="L-970"><a href="#L-970"><span class="linenos"> 970</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="L-971"><a href="#L-971"><span class="linenos"> 971</span></a>            <span class="n">attention_dropout_rate</span><span class="p">,</span>
</span><span id="L-972"><a href="#L-972"><span class="linenos"> 972</span></a>            <span class="n">key_bias</span><span class="p">,</span>
</span><span id="L-973"><a href="#L-973"><span class="linenos"> 973</span></a>        <span class="p">)</span>
</span><span id="L-974"><a href="#L-974"><span class="linenos"> 974</span></a>        <span class="c1"># feed-forward module definition</span>
</span><span id="L-975"><a href="#L-975"><span class="linenos"> 975</span></a>        <span class="n">positionwise_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-976"><a href="#L-976"><span class="linenos"> 976</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="L-977"><a href="#L-977"><span class="linenos"> 977</span></a>            <span class="n">linear_units</span><span class="p">,</span>
</span><span id="L-978"><a href="#L-978"><span class="linenos"> 978</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="L-979"><a href="#L-979"><span class="linenos"> 979</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="L-980"><a href="#L-980"><span class="linenos"> 980</span></a>        <span class="p">)</span>
</span><span id="L-981"><a href="#L-981"><span class="linenos"> 981</span></a>        <span class="c1"># convolution module definition</span>
</span><span id="L-982"><a href="#L-982"><span class="linenos"> 982</span></a>        <span class="n">convolution_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-983"><a href="#L-983"><span class="linenos"> 983</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="L-984"><a href="#L-984"><span class="linenos"> 984</span></a>            <span class="n">cnn_module_kernel</span><span class="p">,</span>
</span><span id="L-985"><a href="#L-985"><span class="linenos"> 985</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="L-986"><a href="#L-986"><span class="linenos"> 986</span></a>            <span class="n">cnn_module_norm</span><span class="p">,</span>
</span><span id="L-987"><a href="#L-987"><span class="linenos"> 987</span></a>            <span class="n">causal</span><span class="p">,</span>
</span><span id="L-988"><a href="#L-988"><span class="linenos"> 988</span></a>        <span class="p">)</span>
</span><span id="L-989"><a href="#L-989"><span class="linenos"> 989</span></a>
</span><span id="L-990"><a href="#L-990"><span class="linenos"> 990</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-991"><a href="#L-991"><span class="linenos"> 991</span></a>            <span class="p">[</span>
</span><span id="L-992"><a href="#L-992"><span class="linenos"> 992</span></a>                <span class="n">ConformerEncoderLayer</span><span class="p">(</span>
</span><span id="L-993"><a href="#L-993"><span class="linenos"> 993</span></a>                    <span class="n">output_size</span><span class="p">,</span>
</span><span id="L-994"><a href="#L-994"><span class="linenos"> 994</span></a>                    <span class="n">RelPositionMultiHeadedAttention</span><span class="p">(</span><span class="o">*</span><span class="n">encoder_selfattn_layer_args</span><span class="p">),</span>
</span><span id="L-995"><a href="#L-995"><span class="linenos"> 995</span></a>                    <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">),</span>
</span><span id="L-996"><a href="#L-996"><span class="linenos"> 996</span></a>                    <span class="p">(</span>
</span><span id="L-997"><a href="#L-997"><span class="linenos"> 997</span></a>                        <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">)</span>
</span><span id="L-998"><a href="#L-998"><span class="linenos"> 998</span></a>                        <span class="k">if</span> <span class="n">macaron_style</span>
</span><span id="L-999"><a href="#L-999"><span class="linenos"> 999</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1000"><a href="#L-1000"><span class="linenos">1000</span></a>                    <span class="p">),</span>
</span><span id="L-1001"><a href="#L-1001"><span class="linenos">1001</span></a>                    <span class="p">(</span>
</span><span id="L-1002"><a href="#L-1002"><span class="linenos">1002</span></a>                        <span class="n">ConvolutionModule</span><span class="p">(</span><span class="o">*</span><span class="n">convolution_layer_args</span><span class="p">)</span>
</span><span id="L-1003"><a href="#L-1003"><span class="linenos">1003</span></a>                        <span class="k">if</span> <span class="n">use_cnn_module</span>
</span><span id="L-1004"><a href="#L-1004"><span class="linenos">1004</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1005"><a href="#L-1005"><span class="linenos">1005</span></a>                    <span class="p">),</span>
</span><span id="L-1006"><a href="#L-1006"><span class="linenos">1006</span></a>                    <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="L-1007"><a href="#L-1007"><span class="linenos">1007</span></a>                    <span class="n">normalize_before</span><span class="p">,</span>
</span><span id="L-1008"><a href="#L-1008"><span class="linenos">1008</span></a>                <span class="p">)</span>
</span><span id="L-1009"><a href="#L-1009"><span class="linenos">1009</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
</span><span id="L-1010"><a href="#L-1010"><span class="linenos">1010</span></a>            <span class="p">]</span>
</span><span id="L-1011"><a href="#L-1011"><span class="linenos">1011</span></a>        <span class="p">)</span>
</span><span id="L-1012"><a href="#L-1012"><span class="linenos">1012</span></a>
</span><span id="L-1013"><a href="#L-1013"><span class="linenos">1013</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers</span><span class="p">(</span>
</span><span id="L-1014"><a href="#L-1014"><span class="linenos">1014</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1015"><a href="#L-1015"><span class="linenos">1015</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1016"><a href="#L-1016"><span class="linenos">1016</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1017"><a href="#L-1017"><span class="linenos">1017</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1018"><a href="#L-1018"><span class="linenos">1018</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1019"><a href="#L-1019"><span class="linenos">1019</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1020"><a href="#L-1020"><span class="linenos">1020</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="L-1021"><a href="#L-1021"><span class="linenos">1021</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="L-1022"><a href="#L-1022"><span class="linenos">1022</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span><span id="L-1023"><a href="#L-1023"><span class="linenos">1023</span></a>
</span><span id="L-1024"><a href="#L-1024"><span class="linenos">1024</span></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
</span><span id="L-1025"><a href="#L-1025"><span class="linenos">1025</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers_checkpointed</span><span class="p">(</span>
</span><span id="L-1026"><a href="#L-1026"><span class="linenos">1026</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1027"><a href="#L-1027"><span class="linenos">1027</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1028"><a href="#L-1028"><span class="linenos">1028</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1029"><a href="#L-1029"><span class="linenos">1029</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1030"><a href="#L-1030"><span class="linenos">1030</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1031"><a href="#L-1031"><span class="linenos">1031</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1032"><a href="#L-1032"><span class="linenos">1032</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="L-1033"><a href="#L-1033"><span class="linenos">1033</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
</span><span id="L-1034"><a href="#L-1034"><span class="linenos">1034</span></a>                <span class="n">layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">use_reentrant</span><span class="o">=</span><span class="kc">False</span>
</span><span id="L-1035"><a href="#L-1035"><span class="linenos">1035</span></a>            <span class="p">)</span>
</span><span id="L-1036"><a href="#L-1036"><span class="linenos">1036</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span><span id="L-1037"><a href="#L-1037"><span class="linenos">1037</span></a>
</span><span id="L-1038"><a href="#L-1038"><span class="linenos">1038</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-1039"><a href="#L-1039"><span class="linenos">1039</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1040"><a href="#L-1040"><span class="linenos">1040</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1041"><a href="#L-1041"><span class="linenos">1041</span></a>        <span class="n">pad_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1042"><a href="#L-1042"><span class="linenos">1042</span></a>        <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="L-1043"><a href="#L-1043"><span class="linenos">1043</span></a>        <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-1044"><a href="#L-1044"><span class="linenos">1044</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-1045"><a href="#L-1045"><span class="linenos">1045</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed positions in tensor.</span>
</span><span id="L-1046"><a href="#L-1046"><span class="linenos">1046</span></a>
</span><span id="L-1047"><a href="#L-1047"><span class="linenos">1047</span></a><span class="sd">        Args:</span>
</span><span id="L-1048"><a href="#L-1048"><span class="linenos">1048</span></a><span class="sd">            xs: padded input tensor (B, T, D)</span>
</span><span id="L-1049"><a href="#L-1049"><span class="linenos">1049</span></a><span class="sd">            xs_lens: input length (B)</span>
</span><span id="L-1050"><a href="#L-1050"><span class="linenos">1050</span></a><span class="sd">            decoding_chunk_size: decoding chunk size for dynamic chunk</span>
</span><span id="L-1051"><a href="#L-1051"><span class="linenos">1051</span></a><span class="sd">                0: default for training, use random dynamic chunk.</span>
</span><span id="L-1052"><a href="#L-1052"><span class="linenos">1052</span></a><span class="sd">                &lt;0: for decoding, use full chunk.</span>
</span><span id="L-1053"><a href="#L-1053"><span class="linenos">1053</span></a><span class="sd">                &gt;0: for decoding, use fixed chunk size as set.</span>
</span><span id="L-1054"><a href="#L-1054"><span class="linenos">1054</span></a><span class="sd">            num_decoding_left_chunks: number of left chunks, this is for decoding,</span>
</span><span id="L-1055"><a href="#L-1055"><span class="linenos">1055</span></a><span class="sd">            the chunk size is decoding_chunk_size.</span>
</span><span id="L-1056"><a href="#L-1056"><span class="linenos">1056</span></a><span class="sd">                &gt;=0: use num_decoding_left_chunks</span>
</span><span id="L-1057"><a href="#L-1057"><span class="linenos">1057</span></a><span class="sd">                &lt;0: use all left chunks</span>
</span><span id="L-1058"><a href="#L-1058"><span class="linenos">1058</span></a><span class="sd">        Returns:</span>
</span><span id="L-1059"><a href="#L-1059"><span class="linenos">1059</span></a><span class="sd">            encoder output tensor xs, and subsampled masks</span>
</span><span id="L-1060"><a href="#L-1060"><span class="linenos">1060</span></a><span class="sd">            xs: padded output tensor (B, T&#39; ~= T/subsample_rate, D)</span>
</span><span id="L-1061"><a href="#L-1061"><span class="linenos">1061</span></a><span class="sd">            masks: torch.Tensor batch padding mask after subsample</span>
</span><span id="L-1062"><a href="#L-1062"><span class="linenos">1062</span></a><span class="sd">                (B, 1, T&#39; ~= T/subsample_rate)</span>
</span><span id="L-1063"><a href="#L-1063"><span class="linenos">1063</span></a><span class="sd">        NOTE(xcsong):</span>
</span><span id="L-1064"><a href="#L-1064"><span class="linenos">1064</span></a><span class="sd">            We pass the `__call__` method of the modules instead of `forward` to the</span>
</span><span id="L-1065"><a href="#L-1065"><span class="linenos">1065</span></a><span class="sd">            checkpointing API because `__call__` attaches all the hooks of the module.</span>
</span><span id="L-1066"><a href="#L-1066"><span class="linenos">1066</span></a><span class="sd">            https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</span>
</span><span id="L-1067"><a href="#L-1067"><span class="linenos">1067</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1068"><a href="#L-1068"><span class="linenos">1068</span></a>        <span class="n">T</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1069"><a href="#L-1069"><span class="linenos">1069</span></a>        <span class="n">masks</span> <span class="o">=</span> <span class="n">pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, T)</span>
</span><span id="L-1070"><a href="#L-1070"><span class="linenos">1070</span></a>        <span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="L-1071"><a href="#L-1071"><span class="linenos">1071</span></a>        <span class="n">mask_pad</span> <span class="o">=</span> <span class="n">masks</span>  <span class="c1"># (B, 1, T/subsample_rate)</span>
</span><span id="L-1072"><a href="#L-1072"><span class="linenos">1072</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">add_optional_chunk_mask</span><span class="p">(</span>
</span><span id="L-1073"><a href="#L-1073"><span class="linenos">1073</span></a>            <span class="n">xs</span><span class="p">,</span>
</span><span id="L-1074"><a href="#L-1074"><span class="linenos">1074</span></a>            <span class="n">masks</span><span class="p">,</span>
</span><span id="L-1075"><a href="#L-1075"><span class="linenos">1075</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span><span class="p">,</span>
</span><span id="L-1076"><a href="#L-1076"><span class="linenos">1076</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span><span class="p">,</span>
</span><span id="L-1077"><a href="#L-1077"><span class="linenos">1077</span></a>            <span class="n">decoding_chunk_size</span><span class="p">,</span>
</span><span id="L-1078"><a href="#L-1078"><span class="linenos">1078</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span><span class="p">,</span>
</span><span id="L-1079"><a href="#L-1079"><span class="linenos">1079</span></a>            <span class="n">num_decoding_left_chunks</span><span class="p">,</span>
</span><span id="L-1080"><a href="#L-1080"><span class="linenos">1080</span></a>        <span class="p">)</span>
</span><span id="L-1081"><a href="#L-1081"><span class="linenos">1081</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span><span id="L-1082"><a href="#L-1082"><span class="linenos">1082</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers_checkpointed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="L-1083"><a href="#L-1083"><span class="linenos">1083</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1084"><a href="#L-1084"><span class="linenos">1084</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="L-1085"><a href="#L-1085"><span class="linenos">1085</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="L-1086"><a href="#L-1086"><span class="linenos">1086</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="L-1087"><a href="#L-1087"><span class="linenos">1087</span></a>        <span class="c1"># Here we assume the mask is not changed in encoder layers, so just</span>
</span><span id="L-1088"><a href="#L-1088"><span class="linenos">1088</span></a>        <span class="c1"># return the masks before encoder layers, and the masks will be used</span>
</span><span id="L-1089"><a href="#L-1089"><span class="linenos">1089</span></a>        <span class="c1"># for cross attention with decoder later</span>
</span><span id="L-1090"><a href="#L-1090"><span class="linenos">1090</span></a>        <span class="k">return</span> <span class="n">xs</span><span class="p">,</span> <span class="n">masks</span>
</span></pre></div>


            </section>
                <section id="ConvolutionModule">
                            <input id="ConvolutionModule-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ConvolutionModule</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="ConvolutionModule-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConvolutionModule"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvolutionModule-8"><a href="#ConvolutionModule-8"><span class="linenos">  8</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConvolutionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="ConvolutionModule-9"><a href="#ConvolutionModule-9"><span class="linenos">  9</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;ConvolutionModule in Conformer model.&quot;&quot;&quot;</span>
</span><span id="ConvolutionModule-10"><a href="#ConvolutionModule-10"><span class="linenos"> 10</span></a>
</span><span id="ConvolutionModule-11"><a href="#ConvolutionModule-11"><span class="linenos"> 11</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConvolutionModule-12"><a href="#ConvolutionModule-12"><span class="linenos"> 12</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConvolutionModule-13"><a href="#ConvolutionModule-13"><span class="linenos"> 13</span></a>        <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConvolutionModule-14"><a href="#ConvolutionModule-14"><span class="linenos"> 14</span></a>        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="ConvolutionModule-15"><a href="#ConvolutionModule-15"><span class="linenos"> 15</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="ConvolutionModule-16"><a href="#ConvolutionModule-16"><span class="linenos"> 16</span></a>        <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="ConvolutionModule-17"><a href="#ConvolutionModule-17"><span class="linenos"> 17</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvolutionModule-18"><a href="#ConvolutionModule-18"><span class="linenos"> 18</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConvolutionModule-19"><a href="#ConvolutionModule-19"><span class="linenos"> 19</span></a>    <span class="p">):</span>
</span><span id="ConvolutionModule-20"><a href="#ConvolutionModule-20"><span class="linenos"> 20</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an ConvolutionModule object.</span>
</span><span id="ConvolutionModule-21"><a href="#ConvolutionModule-21"><span class="linenos"> 21</span></a><span class="sd">        Args:</span>
</span><span id="ConvolutionModule-22"><a href="#ConvolutionModule-22"><span class="linenos"> 22</span></a><span class="sd">            channels (int): The number of channels of conv layers.</span>
</span><span id="ConvolutionModule-23"><a href="#ConvolutionModule-23"><span class="linenos"> 23</span></a><span class="sd">            kernel_size (int): Kernel size of conv layers.</span>
</span><span id="ConvolutionModule-24"><a href="#ConvolutionModule-24"><span class="linenos"> 24</span></a><span class="sd">            causal (int): Whether use causal convolution or not</span>
</span><span id="ConvolutionModule-25"><a href="#ConvolutionModule-25"><span class="linenos"> 25</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvolutionModule-26"><a href="#ConvolutionModule-26"><span class="linenos"> 26</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConvolutionModule-27"><a href="#ConvolutionModule-27"><span class="linenos"> 27</span></a>
</span><span id="ConvolutionModule-28"><a href="#ConvolutionModule-28"><span class="linenos"> 28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule-29"><a href="#ConvolutionModule-29"><span class="linenos"> 29</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-30"><a href="#ConvolutionModule-30"><span class="linenos"> 30</span></a>            <span class="mi">2</span> <span class="o">*</span> <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-31"><a href="#ConvolutionModule-31"><span class="linenos"> 31</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule-32"><a href="#ConvolutionModule-32"><span class="linenos"> 32</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule-33"><a href="#ConvolutionModule-33"><span class="linenos"> 33</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ConvolutionModule-34"><a href="#ConvolutionModule-34"><span class="linenos"> 34</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule-35"><a href="#ConvolutionModule-35"><span class="linenos"> 35</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule-36"><a href="#ConvolutionModule-36"><span class="linenos"> 36</span></a>        <span class="c1"># self.lorder is used to distinguish if it&#39;s a causal convolution,</span>
</span><span id="ConvolutionModule-37"><a href="#ConvolutionModule-37"><span class="linenos"> 37</span></a>        <span class="c1"># if self.lorder &gt; 0: it&#39;s a causal convolution, the input will be</span>
</span><span id="ConvolutionModule-38"><a href="#ConvolutionModule-38"><span class="linenos"> 38</span></a>        <span class="c1">#    padded with self.lorder frames on the left in forward.</span>
</span><span id="ConvolutionModule-39"><a href="#ConvolutionModule-39"><span class="linenos"> 39</span></a>        <span class="c1"># else: it&#39;s a symmetrical convolution</span>
</span><span id="ConvolutionModule-40"><a href="#ConvolutionModule-40"><span class="linenos"> 40</span></a>        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
</span><span id="ConvolutionModule-41"><a href="#ConvolutionModule-41"><span class="linenos"> 41</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="ConvolutionModule-42"><a href="#ConvolutionModule-42"><span class="linenos"> 42</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="ConvolutionModule-43"><a href="#ConvolutionModule-43"><span class="linenos"> 43</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule-44"><a href="#ConvolutionModule-44"><span class="linenos"> 44</span></a>            <span class="c1"># kernel_size should be an odd number for none causal convolution</span>
</span><span id="ConvolutionModule-45"><a href="#ConvolutionModule-45"><span class="linenos"> 45</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="ConvolutionModule-46"><a href="#ConvolutionModule-46"><span class="linenos"> 46</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="ConvolutionModule-47"><a href="#ConvolutionModule-47"><span class="linenos"> 47</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="ConvolutionModule-48"><a href="#ConvolutionModule-48"><span class="linenos"> 48</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule-49"><a href="#ConvolutionModule-49"><span class="linenos"> 49</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-50"><a href="#ConvolutionModule-50"><span class="linenos"> 50</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-51"><a href="#ConvolutionModule-51"><span class="linenos"> 51</span></a>            <span class="n">kernel_size</span><span class="p">,</span>
</span><span id="ConvolutionModule-52"><a href="#ConvolutionModule-52"><span class="linenos"> 52</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule-53"><a href="#ConvolutionModule-53"><span class="linenos"> 53</span></a>            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="ConvolutionModule-54"><a href="#ConvolutionModule-54"><span class="linenos"> 54</span></a>            <span class="n">groups</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-55"><a href="#ConvolutionModule-55"><span class="linenos"> 55</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule-56"><a href="#ConvolutionModule-56"><span class="linenos"> 56</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule-57"><a href="#ConvolutionModule-57"><span class="linenos"> 57</span></a>
</span><span id="ConvolutionModule-58"><a href="#ConvolutionModule-58"><span class="linenos"> 58</span></a>        <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span> <span class="s2">&quot;layer_norm&quot;</span><span class="p">]</span>
</span><span id="ConvolutionModule-59"><a href="#ConvolutionModule-59"><span class="linenos"> 59</span></a>        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">:</span>
</span><span id="ConvolutionModule-60"><a href="#ConvolutionModule-60"><span class="linenos"> 60</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="ConvolutionModule-61"><a href="#ConvolutionModule-61"><span class="linenos"> 61</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="ConvolutionModule-62"><a href="#ConvolutionModule-62"><span class="linenos"> 62</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule-63"><a href="#ConvolutionModule-63"><span class="linenos"> 63</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="ConvolutionModule-64"><a href="#ConvolutionModule-64"><span class="linenos"> 64</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="ConvolutionModule-65"><a href="#ConvolutionModule-65"><span class="linenos"> 65</span></a>
</span><span id="ConvolutionModule-66"><a href="#ConvolutionModule-66"><span class="linenos"> 66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule-67"><a href="#ConvolutionModule-67"><span class="linenos"> 67</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-68"><a href="#ConvolutionModule-68"><span class="linenos"> 68</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule-69"><a href="#ConvolutionModule-69"><span class="linenos"> 69</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule-70"><a href="#ConvolutionModule-70"><span class="linenos"> 70</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule-71"><a href="#ConvolutionModule-71"><span class="linenos"> 71</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ConvolutionModule-72"><a href="#ConvolutionModule-72"><span class="linenos"> 72</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule-73"><a href="#ConvolutionModule-73"><span class="linenos"> 73</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule-74"><a href="#ConvolutionModule-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="ConvolutionModule-75"><a href="#ConvolutionModule-75"><span class="linenos"> 75</span></a>
</span><span id="ConvolutionModule-76"><a href="#ConvolutionModule-76"><span class="linenos"> 76</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConvolutionModule-77"><a href="#ConvolutionModule-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConvolutionModule-78"><a href="#ConvolutionModule-78"><span class="linenos"> 78</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConvolutionModule-79"><a href="#ConvolutionModule-79"><span class="linenos"> 79</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="ConvolutionModule-80"><a href="#ConvolutionModule-80"><span class="linenos"> 80</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConvolutionModule-81"><a href="#ConvolutionModule-81"><span class="linenos"> 81</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConvolutionModule-82"><a href="#ConvolutionModule-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute convolution module.</span>
</span><span id="ConvolutionModule-83"><a href="#ConvolutionModule-83"><span class="linenos"> 83</span></a><span class="sd">        Args:</span>
</span><span id="ConvolutionModule-84"><a href="#ConvolutionModule-84"><span class="linenos"> 84</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, channels).</span>
</span><span id="ConvolutionModule-85"><a href="#ConvolutionModule-85"><span class="linenos"> 85</span></a><span class="sd">            mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),</span>
</span><span id="ConvolutionModule-86"><a href="#ConvolutionModule-86"><span class="linenos"> 86</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="ConvolutionModule-87"><a href="#ConvolutionModule-87"><span class="linenos"> 87</span></a><span class="sd">            cache (torch.Tensor): left context cache, it is only</span>
</span><span id="ConvolutionModule-88"><a href="#ConvolutionModule-88"><span class="linenos"> 88</span></a><span class="sd">                used in causal convolution (#batch, channels, cache_t),</span>
</span><span id="ConvolutionModule-89"><a href="#ConvolutionModule-89"><span class="linenos"> 89</span></a><span class="sd">                (0, 0, 0) meas fake cache.</span>
</span><span id="ConvolutionModule-90"><a href="#ConvolutionModule-90"><span class="linenos"> 90</span></a><span class="sd">        Returns:</span>
</span><span id="ConvolutionModule-91"><a href="#ConvolutionModule-91"><span class="linenos"> 91</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, channels).</span>
</span><span id="ConvolutionModule-92"><a href="#ConvolutionModule-92"><span class="linenos"> 92</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvolutionModule-93"><a href="#ConvolutionModule-93"><span class="linenos"> 93</span></a>        <span class="c1"># exchange the temporal dimension and the feature dimension</span>
</span><span id="ConvolutionModule-94"><a href="#ConvolutionModule-94"><span class="linenos"> 94</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (#batch, channels, time)</span>
</span><span id="ConvolutionModule-95"><a href="#ConvolutionModule-95"><span class="linenos"> 95</span></a>
</span><span id="ConvolutionModule-96"><a href="#ConvolutionModule-96"><span class="linenos"> 96</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="ConvolutionModule-97"><a href="#ConvolutionModule-97"><span class="linenos"> 97</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="ConvolutionModule-98"><a href="#ConvolutionModule-98"><span class="linenos"> 98</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule-99"><a href="#ConvolutionModule-99"><span class="linenos"> 99</span></a>
</span><span id="ConvolutionModule-100"><a href="#ConvolutionModule-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="ConvolutionModule-101"><a href="#ConvolutionModule-101"><span class="linenos">101</span></a>            <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># cache_t == 0</span>
</span><span id="ConvolutionModule-102"><a href="#ConvolutionModule-102"><span class="linenos">102</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule-103"><a href="#ConvolutionModule-103"><span class="linenos">103</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule-104"><a href="#ConvolutionModule-104"><span class="linenos">104</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># equal batch</span>
</span><span id="ConvolutionModule-105"><a href="#ConvolutionModule-105"><span class="linenos">105</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># equal channel</span>
</span><span id="ConvolutionModule-106"><a href="#ConvolutionModule-106"><span class="linenos">106</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule-107"><a href="#ConvolutionModule-107"><span class="linenos">107</span></a>            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span>
</span><span id="ConvolutionModule-108"><a href="#ConvolutionModule-108"><span class="linenos">108</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="p">:]</span>
</span><span id="ConvolutionModule-109"><a href="#ConvolutionModule-109"><span class="linenos">109</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule-110"><a href="#ConvolutionModule-110"><span class="linenos">110</span></a>            <span class="c1"># It&#39;s better we just return None if no cache is required,</span>
</span><span id="ConvolutionModule-111"><a href="#ConvolutionModule-111"><span class="linenos">111</span></a>            <span class="c1"># However, for JIT export, here we just fake one tensor instead of</span>
</span><span id="ConvolutionModule-112"><a href="#ConvolutionModule-112"><span class="linenos">112</span></a>            <span class="c1"># None.</span>
</span><span id="ConvolutionModule-113"><a href="#ConvolutionModule-113"><span class="linenos">113</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="ConvolutionModule-114"><a href="#ConvolutionModule-114"><span class="linenos">114</span></a>
</span><span id="ConvolutionModule-115"><a href="#ConvolutionModule-115"><span class="linenos">115</span></a>        <span class="c1"># GLU mechanism</span>
</span><span id="ConvolutionModule-116"><a href="#ConvolutionModule-116"><span class="linenos">116</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, 2*channel, dim)</span>
</span><span id="ConvolutionModule-117"><a href="#ConvolutionModule-117"><span class="linenos">117</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, channel, dim)</span>
</span><span id="ConvolutionModule-118"><a href="#ConvolutionModule-118"><span class="linenos">118</span></a>
</span><span id="ConvolutionModule-119"><a href="#ConvolutionModule-119"><span class="linenos">119</span></a>        <span class="c1"># 1D Depthwise Conv</span>
</span><span id="ConvolutionModule-120"><a href="#ConvolutionModule-120"><span class="linenos">120</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvolutionModule-121"><a href="#ConvolutionModule-121"><span class="linenos">121</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="ConvolutionModule-122"><a href="#ConvolutionModule-122"><span class="linenos">122</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule-123"><a href="#ConvolutionModule-123"><span class="linenos">123</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConvolutionModule-124"><a href="#ConvolutionModule-124"><span class="linenos">124</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="ConvolutionModule-125"><a href="#ConvolutionModule-125"><span class="linenos">125</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule-126"><a href="#ConvolutionModule-126"><span class="linenos">126</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvolutionModule-127"><a href="#ConvolutionModule-127"><span class="linenos">127</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="ConvolutionModule-128"><a href="#ConvolutionModule-128"><span class="linenos">128</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="ConvolutionModule-129"><a href="#ConvolutionModule-129"><span class="linenos">129</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule-130"><a href="#ConvolutionModule-130"><span class="linenos">130</span></a>
</span><span id="ConvolutionModule-131"><a href="#ConvolutionModule-131"><span class="linenos">131</span></a>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>ConvolutionModule in Conformer model.</p>
</div>


                            <div id="ConvolutionModule.__init__" class="classattr">
                                        <input id="ConvolutionModule.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ConvolutionModule</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>,</span><span class="param">	<span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;batch_norm&#39;</span>,</span><span class="param">	<span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="ConvolutionModule.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConvolutionModule.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvolutionModule.__init__-11"><a href="#ConvolutionModule.__init__-11"><span class="linenos">11</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConvolutionModule.__init__-12"><a href="#ConvolutionModule.__init__-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-13"><a href="#ConvolutionModule.__init__-13"><span class="linenos">13</span></a>        <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-14"><a href="#ConvolutionModule.__init__-14"><span class="linenos">14</span></a>        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-15"><a href="#ConvolutionModule.__init__-15"><span class="linenos">15</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="ConvolutionModule.__init__-16"><a href="#ConvolutionModule.__init__-16"><span class="linenos">16</span></a>        <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-17"><a href="#ConvolutionModule.__init__-17"><span class="linenos">17</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-18"><a href="#ConvolutionModule.__init__-18"><span class="linenos">18</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-19"><a href="#ConvolutionModule.__init__-19"><span class="linenos">19</span></a>    <span class="p">):</span>
</span><span id="ConvolutionModule.__init__-20"><a href="#ConvolutionModule.__init__-20"><span class="linenos">20</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an ConvolutionModule object.</span>
</span><span id="ConvolutionModule.__init__-21"><a href="#ConvolutionModule.__init__-21"><span class="linenos">21</span></a><span class="sd">        Args:</span>
</span><span id="ConvolutionModule.__init__-22"><a href="#ConvolutionModule.__init__-22"><span class="linenos">22</span></a><span class="sd">            channels (int): The number of channels of conv layers.</span>
</span><span id="ConvolutionModule.__init__-23"><a href="#ConvolutionModule.__init__-23"><span class="linenos">23</span></a><span class="sd">            kernel_size (int): Kernel size of conv layers.</span>
</span><span id="ConvolutionModule.__init__-24"><a href="#ConvolutionModule.__init__-24"><span class="linenos">24</span></a><span class="sd">            causal (int): Whether use causal convolution or not</span>
</span><span id="ConvolutionModule.__init__-25"><a href="#ConvolutionModule.__init__-25"><span class="linenos">25</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvolutionModule.__init__-26"><a href="#ConvolutionModule.__init__-26"><span class="linenos">26</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConvolutionModule.__init__-27"><a href="#ConvolutionModule.__init__-27"><span class="linenos">27</span></a>
</span><span id="ConvolutionModule.__init__-28"><a href="#ConvolutionModule.__init__-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule.__init__-29"><a href="#ConvolutionModule.__init__-29"><span class="linenos">29</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-30"><a href="#ConvolutionModule.__init__-30"><span class="linenos">30</span></a>            <span class="mi">2</span> <span class="o">*</span> <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-31"><a href="#ConvolutionModule.__init__-31"><span class="linenos">31</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-32"><a href="#ConvolutionModule.__init__-32"><span class="linenos">32</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-33"><a href="#ConvolutionModule.__init__-33"><span class="linenos">33</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-34"><a href="#ConvolutionModule.__init__-34"><span class="linenos">34</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-35"><a href="#ConvolutionModule.__init__-35"><span class="linenos">35</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule.__init__-36"><a href="#ConvolutionModule.__init__-36"><span class="linenos">36</span></a>        <span class="c1"># self.lorder is used to distinguish if it&#39;s a causal convolution,</span>
</span><span id="ConvolutionModule.__init__-37"><a href="#ConvolutionModule.__init__-37"><span class="linenos">37</span></a>        <span class="c1"># if self.lorder &gt; 0: it&#39;s a causal convolution, the input will be</span>
</span><span id="ConvolutionModule.__init__-38"><a href="#ConvolutionModule.__init__-38"><span class="linenos">38</span></a>        <span class="c1">#    padded with self.lorder frames on the left in forward.</span>
</span><span id="ConvolutionModule.__init__-39"><a href="#ConvolutionModule.__init__-39"><span class="linenos">39</span></a>        <span class="c1"># else: it&#39;s a symmetrical convolution</span>
</span><span id="ConvolutionModule.__init__-40"><a href="#ConvolutionModule.__init__-40"><span class="linenos">40</span></a>        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
</span><span id="ConvolutionModule.__init__-41"><a href="#ConvolutionModule.__init__-41"><span class="linenos">41</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="ConvolutionModule.__init__-42"><a href="#ConvolutionModule.__init__-42"><span class="linenos">42</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="ConvolutionModule.__init__-43"><a href="#ConvolutionModule.__init__-43"><span class="linenos">43</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule.__init__-44"><a href="#ConvolutionModule.__init__-44"><span class="linenos">44</span></a>            <span class="c1"># kernel_size should be an odd number for none causal convolution</span>
</span><span id="ConvolutionModule.__init__-45"><a href="#ConvolutionModule.__init__-45"><span class="linenos">45</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="ConvolutionModule.__init__-46"><a href="#ConvolutionModule.__init__-46"><span class="linenos">46</span></a>            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="ConvolutionModule.__init__-47"><a href="#ConvolutionModule.__init__-47"><span class="linenos">47</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="ConvolutionModule.__init__-48"><a href="#ConvolutionModule.__init__-48"><span class="linenos">48</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule.__init__-49"><a href="#ConvolutionModule.__init__-49"><span class="linenos">49</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-50"><a href="#ConvolutionModule.__init__-50"><span class="linenos">50</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-51"><a href="#ConvolutionModule.__init__-51"><span class="linenos">51</span></a>            <span class="n">kernel_size</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-52"><a href="#ConvolutionModule.__init__-52"><span class="linenos">52</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-53"><a href="#ConvolutionModule.__init__-53"><span class="linenos">53</span></a>            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-54"><a href="#ConvolutionModule.__init__-54"><span class="linenos">54</span></a>            <span class="n">groups</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-55"><a href="#ConvolutionModule.__init__-55"><span class="linenos">55</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-56"><a href="#ConvolutionModule.__init__-56"><span class="linenos">56</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule.__init__-57"><a href="#ConvolutionModule.__init__-57"><span class="linenos">57</span></a>
</span><span id="ConvolutionModule.__init__-58"><a href="#ConvolutionModule.__init__-58"><span class="linenos">58</span></a>        <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span> <span class="s2">&quot;layer_norm&quot;</span><span class="p">]</span>
</span><span id="ConvolutionModule.__init__-59"><a href="#ConvolutionModule.__init__-59"><span class="linenos">59</span></a>        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">:</span>
</span><span id="ConvolutionModule.__init__-60"><a href="#ConvolutionModule.__init__-60"><span class="linenos">60</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="ConvolutionModule.__init__-61"><a href="#ConvolutionModule.__init__-61"><span class="linenos">61</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="ConvolutionModule.__init__-62"><a href="#ConvolutionModule.__init__-62"><span class="linenos">62</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule.__init__-63"><a href="#ConvolutionModule.__init__-63"><span class="linenos">63</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="ConvolutionModule.__init__-64"><a href="#ConvolutionModule.__init__-64"><span class="linenos">64</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
</span><span id="ConvolutionModule.__init__-65"><a href="#ConvolutionModule.__init__-65"><span class="linenos">65</span></a>
</span><span id="ConvolutionModule.__init__-66"><a href="#ConvolutionModule.__init__-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
</span><span id="ConvolutionModule.__init__-67"><a href="#ConvolutionModule.__init__-67"><span class="linenos">67</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-68"><a href="#ConvolutionModule.__init__-68"><span class="linenos">68</span></a>            <span class="n">channels</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-69"><a href="#ConvolutionModule.__init__-69"><span class="linenos">69</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-70"><a href="#ConvolutionModule.__init__-70"><span class="linenos">70</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-71"><a href="#ConvolutionModule.__init__-71"><span class="linenos">71</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-72"><a href="#ConvolutionModule.__init__-72"><span class="linenos">72</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ConvolutionModule.__init__-73"><a href="#ConvolutionModule.__init__-73"><span class="linenos">73</span></a>        <span class="p">)</span>
</span><span id="ConvolutionModule.__init__-74"><a href="#ConvolutionModule.__init__-74"><span class="linenos">74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span></pre></div>


            <div class="docstring"><p>Construct an ConvolutionModule object.
Args:
    channels (int): The number of channels of conv layers.
    kernel_size (int): Kernel size of conv layers.
    causal (int): Whether use causal convolution or not</p>
</div>


                            </div>
                            <div id="ConvolutionModule.pointwise_conv1" class="classattr">
                                <div class="attr variable">
            <span class="name">pointwise_conv1</span>

        
    </div>
    <a class="headerlink" href="#ConvolutionModule.pointwise_conv1"></a>
    
    

                            </div>
                            <div id="ConvolutionModule.depthwise_conv" class="classattr">
                                <div class="attr variable">
            <span class="name">depthwise_conv</span>

        
    </div>
    <a class="headerlink" href="#ConvolutionModule.depthwise_conv"></a>
    
    

                            </div>
                            <div id="ConvolutionModule.pointwise_conv2" class="classattr">
                                <div class="attr variable">
            <span class="name">pointwise_conv2</span>

        
    </div>
    <a class="headerlink" href="#ConvolutionModule.pointwise_conv2"></a>
    
    

                            </div>
                            <div id="ConvolutionModule.activation" class="classattr">
                                <div class="attr variable">
            <span class="name">activation</span>

        
    </div>
    <a class="headerlink" href="#ConvolutionModule.activation"></a>
    
    

                            </div>
                            <div id="ConvolutionModule.forward" class="classattr">
                                        <input id="ConvolutionModule.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>,</span><span class="param">	<span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="ConvolutionModule.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConvolutionModule.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvolutionModule.forward-76"><a href="#ConvolutionModule.forward-76"><span class="linenos"> 76</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConvolutionModule.forward-77"><a href="#ConvolutionModule.forward-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConvolutionModule.forward-78"><a href="#ConvolutionModule.forward-78"><span class="linenos"> 78</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConvolutionModule.forward-79"><a href="#ConvolutionModule.forward-79"><span class="linenos"> 79</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="ConvolutionModule.forward-80"><a href="#ConvolutionModule.forward-80"><span class="linenos"> 80</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConvolutionModule.forward-81"><a href="#ConvolutionModule.forward-81"><span class="linenos"> 81</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConvolutionModule.forward-82"><a href="#ConvolutionModule.forward-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute convolution module.</span>
</span><span id="ConvolutionModule.forward-83"><a href="#ConvolutionModule.forward-83"><span class="linenos"> 83</span></a><span class="sd">        Args:</span>
</span><span id="ConvolutionModule.forward-84"><a href="#ConvolutionModule.forward-84"><span class="linenos"> 84</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, channels).</span>
</span><span id="ConvolutionModule.forward-85"><a href="#ConvolutionModule.forward-85"><span class="linenos"> 85</span></a><span class="sd">            mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),</span>
</span><span id="ConvolutionModule.forward-86"><a href="#ConvolutionModule.forward-86"><span class="linenos"> 86</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="ConvolutionModule.forward-87"><a href="#ConvolutionModule.forward-87"><span class="linenos"> 87</span></a><span class="sd">            cache (torch.Tensor): left context cache, it is only</span>
</span><span id="ConvolutionModule.forward-88"><a href="#ConvolutionModule.forward-88"><span class="linenos"> 88</span></a><span class="sd">                used in causal convolution (#batch, channels, cache_t),</span>
</span><span id="ConvolutionModule.forward-89"><a href="#ConvolutionModule.forward-89"><span class="linenos"> 89</span></a><span class="sd">                (0, 0, 0) meas fake cache.</span>
</span><span id="ConvolutionModule.forward-90"><a href="#ConvolutionModule.forward-90"><span class="linenos"> 90</span></a><span class="sd">        Returns:</span>
</span><span id="ConvolutionModule.forward-91"><a href="#ConvolutionModule.forward-91"><span class="linenos"> 91</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, channels).</span>
</span><span id="ConvolutionModule.forward-92"><a href="#ConvolutionModule.forward-92"><span class="linenos"> 92</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvolutionModule.forward-93"><a href="#ConvolutionModule.forward-93"><span class="linenos"> 93</span></a>        <span class="c1"># exchange the temporal dimension and the feature dimension</span>
</span><span id="ConvolutionModule.forward-94"><a href="#ConvolutionModule.forward-94"><span class="linenos"> 94</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (#batch, channels, time)</span>
</span><span id="ConvolutionModule.forward-95"><a href="#ConvolutionModule.forward-95"><span class="linenos"> 95</span></a>
</span><span id="ConvolutionModule.forward-96"><a href="#ConvolutionModule.forward-96"><span class="linenos"> 96</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="ConvolutionModule.forward-97"><a href="#ConvolutionModule.forward-97"><span class="linenos"> 97</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="ConvolutionModule.forward-98"><a href="#ConvolutionModule.forward-98"><span class="linenos"> 98</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-99"><a href="#ConvolutionModule.forward-99"><span class="linenos"> 99</span></a>
</span><span id="ConvolutionModule.forward-100"><a href="#ConvolutionModule.forward-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="ConvolutionModule.forward-101"><a href="#ConvolutionModule.forward-101"><span class="linenos">101</span></a>            <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># cache_t == 0</span>
</span><span id="ConvolutionModule.forward-102"><a href="#ConvolutionModule.forward-102"><span class="linenos">102</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-103"><a href="#ConvolutionModule.forward-103"><span class="linenos">103</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule.forward-104"><a href="#ConvolutionModule.forward-104"><span class="linenos">104</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># equal batch</span>
</span><span id="ConvolutionModule.forward-105"><a href="#ConvolutionModule.forward-105"><span class="linenos">105</span></a>                <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># equal channel</span>
</span><span id="ConvolutionModule.forward-106"><a href="#ConvolutionModule.forward-106"><span class="linenos">106</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-107"><a href="#ConvolutionModule.forward-107"><span class="linenos">107</span></a>            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span>
</span><span id="ConvolutionModule.forward-108"><a href="#ConvolutionModule.forward-108"><span class="linenos">108</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="p">:]</span>
</span><span id="ConvolutionModule.forward-109"><a href="#ConvolutionModule.forward-109"><span class="linenos">109</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvolutionModule.forward-110"><a href="#ConvolutionModule.forward-110"><span class="linenos">110</span></a>            <span class="c1"># It&#39;s better we just return None if no cache is required,</span>
</span><span id="ConvolutionModule.forward-111"><a href="#ConvolutionModule.forward-111"><span class="linenos">111</span></a>            <span class="c1"># However, for JIT export, here we just fake one tensor instead of</span>
</span><span id="ConvolutionModule.forward-112"><a href="#ConvolutionModule.forward-112"><span class="linenos">112</span></a>            <span class="c1"># None.</span>
</span><span id="ConvolutionModule.forward-113"><a href="#ConvolutionModule.forward-113"><span class="linenos">113</span></a>            <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-114"><a href="#ConvolutionModule.forward-114"><span class="linenos">114</span></a>
</span><span id="ConvolutionModule.forward-115"><a href="#ConvolutionModule.forward-115"><span class="linenos">115</span></a>        <span class="c1"># GLU mechanism</span>
</span><span id="ConvolutionModule.forward-116"><a href="#ConvolutionModule.forward-116"><span class="linenos">116</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, 2*channel, dim)</span>
</span><span id="ConvolutionModule.forward-117"><a href="#ConvolutionModule.forward-117"><span class="linenos">117</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, channel, dim)</span>
</span><span id="ConvolutionModule.forward-118"><a href="#ConvolutionModule.forward-118"><span class="linenos">118</span></a>
</span><span id="ConvolutionModule.forward-119"><a href="#ConvolutionModule.forward-119"><span class="linenos">119</span></a>        <span class="c1"># 1D Depthwise Conv</span>
</span><span id="ConvolutionModule.forward-120"><a href="#ConvolutionModule.forward-120"><span class="linenos">120</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-121"><a href="#ConvolutionModule.forward-121"><span class="linenos">121</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="ConvolutionModule.forward-122"><a href="#ConvolutionModule.forward-122"><span class="linenos">122</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-123"><a href="#ConvolutionModule.forward-123"><span class="linenos">123</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConvolutionModule.forward-124"><a href="#ConvolutionModule.forward-124"><span class="linenos">124</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
</span><span id="ConvolutionModule.forward-125"><a href="#ConvolutionModule.forward-125"><span class="linenos">125</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-126"><a href="#ConvolutionModule.forward-126"><span class="linenos">126</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-127"><a href="#ConvolutionModule.forward-127"><span class="linenos">127</span></a>        <span class="c1"># mask batch padding</span>
</span><span id="ConvolutionModule.forward-128"><a href="#ConvolutionModule.forward-128"><span class="linenos">128</span></a>        <span class="k">if</span> <span class="n">mask_pad</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time &gt; 0</span>
</span><span id="ConvolutionModule.forward-129"><a href="#ConvolutionModule.forward-129"><span class="linenos">129</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="ConvolutionModule.forward-130"><a href="#ConvolutionModule.forward-130"><span class="linenos">130</span></a>
</span><span id="ConvolutionModule.forward-131"><a href="#ConvolutionModule.forward-131"><span class="linenos">131</span></a>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>Compute convolution module.
Args:
    x (torch.Tensor): Input tensor (#batch, time, channels).
    mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),
        (0, 0, 0) means fake mask.
    cache (torch.Tensor): left context cache, it is only
        used in causal convolution (#batch, channels, cache_t),
        (0, 0, 0) meas fake cache.
Returns:
    torch.Tensor: Output tensor (#batch, time, channels).</p>
</div>


                            </div>
                </section>
                <section id="PositionwiseFeedForward">
                            <input id="PositionwiseFeedForward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">PositionwiseFeedForward</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="PositionwiseFeedForward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionwiseFeedForward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionwiseFeedForward-134"><a href="#PositionwiseFeedForward-134"><span class="linenos">134</span></a><span class="k">class</span><span class="w"> </span><span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="PositionwiseFeedForward-135"><a href="#PositionwiseFeedForward-135"><span class="linenos">135</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Positionwise feed forward layer.</span>
</span><span id="PositionwiseFeedForward-136"><a href="#PositionwiseFeedForward-136"><span class="linenos">136</span></a>
</span><span id="PositionwiseFeedForward-137"><a href="#PositionwiseFeedForward-137"><span class="linenos">137</span></a><span class="sd">    FeedForward are appied on each position of the sequence.</span>
</span><span id="PositionwiseFeedForward-138"><a href="#PositionwiseFeedForward-138"><span class="linenos">138</span></a><span class="sd">    The output dim is same with the input dim.</span>
</span><span id="PositionwiseFeedForward-139"><a href="#PositionwiseFeedForward-139"><span class="linenos">139</span></a>
</span><span id="PositionwiseFeedForward-140"><a href="#PositionwiseFeedForward-140"><span class="linenos">140</span></a><span class="sd">    Args:</span>
</span><span id="PositionwiseFeedForward-141"><a href="#PositionwiseFeedForward-141"><span class="linenos">141</span></a><span class="sd">        idim (int): Input dimenstion.</span>
</span><span id="PositionwiseFeedForward-142"><a href="#PositionwiseFeedForward-142"><span class="linenos">142</span></a><span class="sd">        hidden_units (int): The number of hidden units.</span>
</span><span id="PositionwiseFeedForward-143"><a href="#PositionwiseFeedForward-143"><span class="linenos">143</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="PositionwiseFeedForward-144"><a href="#PositionwiseFeedForward-144"><span class="linenos">144</span></a><span class="sd">        activation (torch.nn.Module): Activation function</span>
</span><span id="PositionwiseFeedForward-145"><a href="#PositionwiseFeedForward-145"><span class="linenos">145</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="PositionwiseFeedForward-146"><a href="#PositionwiseFeedForward-146"><span class="linenos">146</span></a>
</span><span id="PositionwiseFeedForward-147"><a href="#PositionwiseFeedForward-147"><span class="linenos">147</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="PositionwiseFeedForward-148"><a href="#PositionwiseFeedForward-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward-149"><a href="#PositionwiseFeedForward-149"><span class="linenos">149</span></a>        <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward-150"><a href="#PositionwiseFeedForward-150"><span class="linenos">150</span></a>        <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward-151"><a href="#PositionwiseFeedForward-151"><span class="linenos">151</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward-152"><a href="#PositionwiseFeedForward-152"><span class="linenos">152</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="PositionwiseFeedForward-153"><a href="#PositionwiseFeedForward-153"><span class="linenos">153</span></a>    <span class="p">):</span>
</span><span id="PositionwiseFeedForward-154"><a href="#PositionwiseFeedForward-154"><span class="linenos">154</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct a PositionwiseFeedForward object.&quot;&quot;&quot;</span>
</span><span id="PositionwiseFeedForward-155"><a href="#PositionwiseFeedForward-155"><span class="linenos">155</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="PositionwiseFeedForward-156"><a href="#PositionwiseFeedForward-156"><span class="linenos">156</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">)</span>
</span><span id="PositionwiseFeedForward-157"><a href="#PositionwiseFeedForward-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="PositionwiseFeedForward-158"><a href="#PositionwiseFeedForward-158"><span class="linenos">158</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="PositionwiseFeedForward-159"><a href="#PositionwiseFeedForward-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">idim</span><span class="p">)</span>
</span><span id="PositionwiseFeedForward-160"><a href="#PositionwiseFeedForward-160"><span class="linenos">160</span></a>
</span><span id="PositionwiseFeedForward-161"><a href="#PositionwiseFeedForward-161"><span class="linenos">161</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="PositionwiseFeedForward-162"><a href="#PositionwiseFeedForward-162"><span class="linenos">162</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward function.</span>
</span><span id="PositionwiseFeedForward-163"><a href="#PositionwiseFeedForward-163"><span class="linenos">163</span></a>
</span><span id="PositionwiseFeedForward-164"><a href="#PositionwiseFeedForward-164"><span class="linenos">164</span></a><span class="sd">        Args:</span>
</span><span id="PositionwiseFeedForward-165"><a href="#PositionwiseFeedForward-165"><span class="linenos">165</span></a><span class="sd">            xs: input tensor (B, L, D)</span>
</span><span id="PositionwiseFeedForward-166"><a href="#PositionwiseFeedForward-166"><span class="linenos">166</span></a><span class="sd">        Returns:</span>
</span><span id="PositionwiseFeedForward-167"><a href="#PositionwiseFeedForward-167"><span class="linenos">167</span></a><span class="sd">            output tensor, (B, L, D)</span>
</span><span id="PositionwiseFeedForward-168"><a href="#PositionwiseFeedForward-168"><span class="linenos">168</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PositionwiseFeedForward-169"><a href="#PositionwiseFeedForward-169"><span class="linenos">169</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">xs</span><span class="p">))))</span>
</span></pre></div>


            <div class="docstring"><p>Positionwise feed forward layer.</p>

<p>FeedForward are appied on each position of the sequence.
The output dim is same with the input dim.</p>

<p>Args:
    idim (int): Input dimenstion.
    hidden_units (int): The number of hidden units.
    dropout_rate (float): Dropout rate.
    activation (torch.nn.Module): Activation function</p>
</div>


                            <div id="PositionwiseFeedForward.__init__" class="classattr">
                                        <input id="PositionwiseFeedForward.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">PositionwiseFeedForward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">idim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span></span>)</span>

                <label class="view-source-button" for="PositionwiseFeedForward.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionwiseFeedForward.__init__-147"><a href="#PositionwiseFeedForward.__init__-147"><span class="linenos">147</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="PositionwiseFeedForward.__init__-148"><a href="#PositionwiseFeedForward.__init__-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward.__init__-149"><a href="#PositionwiseFeedForward.__init__-149"><span class="linenos">149</span></a>        <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward.__init__-150"><a href="#PositionwiseFeedForward.__init__-150"><span class="linenos">150</span></a>        <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward.__init__-151"><a href="#PositionwiseFeedForward.__init__-151"><span class="linenos">151</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="PositionwiseFeedForward.__init__-152"><a href="#PositionwiseFeedForward.__init__-152"><span class="linenos">152</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="PositionwiseFeedForward.__init__-153"><a href="#PositionwiseFeedForward.__init__-153"><span class="linenos">153</span></a>    <span class="p">):</span>
</span><span id="PositionwiseFeedForward.__init__-154"><a href="#PositionwiseFeedForward.__init__-154"><span class="linenos">154</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct a PositionwiseFeedForward object.&quot;&quot;&quot;</span>
</span><span id="PositionwiseFeedForward.__init__-155"><a href="#PositionwiseFeedForward.__init__-155"><span class="linenos">155</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="PositionwiseFeedForward.__init__-156"><a href="#PositionwiseFeedForward.__init__-156"><span class="linenos">156</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">)</span>
</span><span id="PositionwiseFeedForward.__init__-157"><a href="#PositionwiseFeedForward.__init__-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="PositionwiseFeedForward.__init__-158"><a href="#PositionwiseFeedForward.__init__-158"><span class="linenos">158</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="PositionwiseFeedForward.__init__-159"><a href="#PositionwiseFeedForward.__init__-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">idim</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct a PositionwiseFeedForward object.</p>
</div>


                            </div>
                            <div id="PositionwiseFeedForward.w_1" class="classattr">
                                <div class="attr variable">
            <span class="name">w_1</span>

        
    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.w_1"></a>
    
    

                            </div>
                            <div id="PositionwiseFeedForward.activation" class="classattr">
                                <div class="attr variable">
            <span class="name">activation</span>

        
    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.activation"></a>
    
    

                            </div>
                            <div id="PositionwiseFeedForward.dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">dropout</span>

        
    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.dropout"></a>
    
    

                            </div>
                            <div id="PositionwiseFeedForward.w_2" class="classattr">
                                <div class="attr variable">
            <span class="name">w_2</span>

        
    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.w_2"></a>
    
    

                            </div>
                            <div id="PositionwiseFeedForward.forward" class="classattr">
                                        <input id="PositionwiseFeedForward.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="PositionwiseFeedForward.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionwiseFeedForward.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionwiseFeedForward.forward-161"><a href="#PositionwiseFeedForward.forward-161"><span class="linenos">161</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="PositionwiseFeedForward.forward-162"><a href="#PositionwiseFeedForward.forward-162"><span class="linenos">162</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward function.</span>
</span><span id="PositionwiseFeedForward.forward-163"><a href="#PositionwiseFeedForward.forward-163"><span class="linenos">163</span></a>
</span><span id="PositionwiseFeedForward.forward-164"><a href="#PositionwiseFeedForward.forward-164"><span class="linenos">164</span></a><span class="sd">        Args:</span>
</span><span id="PositionwiseFeedForward.forward-165"><a href="#PositionwiseFeedForward.forward-165"><span class="linenos">165</span></a><span class="sd">            xs: input tensor (B, L, D)</span>
</span><span id="PositionwiseFeedForward.forward-166"><a href="#PositionwiseFeedForward.forward-166"><span class="linenos">166</span></a><span class="sd">        Returns:</span>
</span><span id="PositionwiseFeedForward.forward-167"><a href="#PositionwiseFeedForward.forward-167"><span class="linenos">167</span></a><span class="sd">            output tensor, (B, L, D)</span>
</span><span id="PositionwiseFeedForward.forward-168"><a href="#PositionwiseFeedForward.forward-168"><span class="linenos">168</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PositionwiseFeedForward.forward-169"><a href="#PositionwiseFeedForward.forward-169"><span class="linenos">169</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">xs</span><span class="p">))))</span>
</span></pre></div>


            <div class="docstring"><p>Forward function.</p>

<p>Args:
    xs: input tensor (B, L, D)
Returns:
    output tensor, (B, L, D)</p>
</div>


                            </div>
                </section>
                <section id="Swish">
                            <input id="Swish-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Swish</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="Swish-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Swish"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Swish-172"><a href="#Swish-172"><span class="linenos">172</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Swish</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="Swish-173"><a href="#Swish-173"><span class="linenos">173</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct an Swish object.&quot;&quot;&quot;</span>
</span><span id="Swish-174"><a href="#Swish-174"><span class="linenos">174</span></a>
</span><span id="Swish-175"><a href="#Swish-175"><span class="linenos">175</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="Swish-176"><a href="#Swish-176"><span class="linenos">176</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Return Swish activation function.&quot;&quot;&quot;</span>
</span><span id="Swish-177"><a href="#Swish-177"><span class="linenos">177</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct an Swish object.</p>
</div>


                            <div id="Swish.forward" class="classattr">
                                        <input id="Swish.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="Swish.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Swish.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Swish.forward-175"><a href="#Swish.forward-175"><span class="linenos">175</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="Swish.forward-176"><a href="#Swish.forward-176"><span class="linenos">176</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Return Swish activation function.&quot;&quot;&quot;</span>
</span><span id="Swish.forward-177"><a href="#Swish.forward-177"><span class="linenos">177</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Return Swish activation function.</p>
</div>


                            </div>
                </section>
                <section id="MultiHeadedAttention">
                            <input id="MultiHeadedAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MultiHeadedAttention</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MultiHeadedAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiHeadedAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiHeadedAttention-180"><a href="#MultiHeadedAttention-180"><span class="linenos">180</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MultiHeadedAttention-181"><a href="#MultiHeadedAttention-181"><span class="linenos">181</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-Head Attention layer.</span>
</span><span id="MultiHeadedAttention-182"><a href="#MultiHeadedAttention-182"><span class="linenos">182</span></a>
</span><span id="MultiHeadedAttention-183"><a href="#MultiHeadedAttention-183"><span class="linenos">183</span></a><span class="sd">    Args:</span>
</span><span id="MultiHeadedAttention-184"><a href="#MultiHeadedAttention-184"><span class="linenos">184</span></a><span class="sd">        n_head (int): The number of heads.</span>
</span><span id="MultiHeadedAttention-185"><a href="#MultiHeadedAttention-185"><span class="linenos">185</span></a><span class="sd">        n_feat (int): The number of features.</span>
</span><span id="MultiHeadedAttention-186"><a href="#MultiHeadedAttention-186"><span class="linenos">186</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="MultiHeadedAttention-187"><a href="#MultiHeadedAttention-187"><span class="linenos">187</span></a>
</span><span id="MultiHeadedAttention-188"><a href="#MultiHeadedAttention-188"><span class="linenos">188</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention-189"><a href="#MultiHeadedAttention-189"><span class="linenos">189</span></a>
</span><span id="MultiHeadedAttention-190"><a href="#MultiHeadedAttention-190"><span class="linenos">190</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="MultiHeadedAttention-191"><a href="#MultiHeadedAttention-191"><span class="linenos">191</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MultiHeadedAttention-192"><a href="#MultiHeadedAttention-192"><span class="linenos">192</span></a>    <span class="p">):</span>
</span><span id="MultiHeadedAttention-193"><a href="#MultiHeadedAttention-193"><span class="linenos">193</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an MultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention-194"><a href="#MultiHeadedAttention-194"><span class="linenos">194</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MultiHeadedAttention-195"><a href="#MultiHeadedAttention-195"><span class="linenos">195</span></a>        <span class="k">assert</span> <span class="n">n_feat</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="MultiHeadedAttention-196"><a href="#MultiHeadedAttention-196"><span class="linenos">196</span></a>        <span class="c1"># We assume d_v always equals d_k</span>
</span><span id="MultiHeadedAttention-197"><a href="#MultiHeadedAttention-197"><span class="linenos">197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">n_feat</span> <span class="o">//</span> <span class="n">n_head</span>
</span><span id="MultiHeadedAttention-198"><a href="#MultiHeadedAttention-198"><span class="linenos">198</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">n_head</span>
</span><span id="MultiHeadedAttention-199"><a href="#MultiHeadedAttention-199"><span class="linenos">199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-200"><a href="#MultiHeadedAttention-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">key_bias</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-201"><a href="#MultiHeadedAttention-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-202"><a href="#MultiHeadedAttention-202"><span class="linenos">202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-203"><a href="#MultiHeadedAttention-203"><span class="linenos">203</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-204"><a href="#MultiHeadedAttention-204"><span class="linenos">204</span></a>
</span><span id="MultiHeadedAttention-205"><a href="#MultiHeadedAttention-205"><span class="linenos">205</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_qkv</span><span class="p">(</span>
</span><span id="MultiHeadedAttention-206"><a href="#MultiHeadedAttention-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</span><span id="MultiHeadedAttention-207"><a href="#MultiHeadedAttention-207"><span class="linenos">207</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="MultiHeadedAttention-208"><a href="#MultiHeadedAttention-208"><span class="linenos">208</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform query, key and value.</span>
</span><span id="MultiHeadedAttention-209"><a href="#MultiHeadedAttention-209"><span class="linenos">209</span></a>
</span><span id="MultiHeadedAttention-210"><a href="#MultiHeadedAttention-210"><span class="linenos">210</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention-211"><a href="#MultiHeadedAttention-211"><span class="linenos">211</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="MultiHeadedAttention-212"><a href="#MultiHeadedAttention-212"><span class="linenos">212</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention-213"><a href="#MultiHeadedAttention-213"><span class="linenos">213</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention-214"><a href="#MultiHeadedAttention-214"><span class="linenos">214</span></a>
</span><span id="MultiHeadedAttention-215"><a href="#MultiHeadedAttention-215"><span class="linenos">215</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention-216"><a href="#MultiHeadedAttention-216"><span class="linenos">216</span></a><span class="sd">            torch.Tensor: Transformed query tensor, size</span>
</span><span id="MultiHeadedAttention-217"><a href="#MultiHeadedAttention-217"><span class="linenos">217</span></a><span class="sd">                (#batch, n_head, time1, d_k).</span>
</span><span id="MultiHeadedAttention-218"><a href="#MultiHeadedAttention-218"><span class="linenos">218</span></a><span class="sd">            torch.Tensor: Transformed key tensor, size</span>
</span><span id="MultiHeadedAttention-219"><a href="#MultiHeadedAttention-219"><span class="linenos">219</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention-220"><a href="#MultiHeadedAttention-220"><span class="linenos">220</span></a><span class="sd">            torch.Tensor: Transformed value tensor, size</span>
</span><span id="MultiHeadedAttention-221"><a href="#MultiHeadedAttention-221"><span class="linenos">221</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention-222"><a href="#MultiHeadedAttention-222"><span class="linenos">222</span></a>
</span><span id="MultiHeadedAttention-223"><a href="#MultiHeadedAttention-223"><span class="linenos">223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention-224"><a href="#MultiHeadedAttention-224"><span class="linenos">224</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-225"><a href="#MultiHeadedAttention-225"><span class="linenos">225</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-226"><a href="#MultiHeadedAttention-226"><span class="linenos">226</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-227"><a href="#MultiHeadedAttention-227"><span class="linenos">227</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-228"><a href="#MultiHeadedAttention-228"><span class="linenos">228</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="MultiHeadedAttention-229"><a href="#MultiHeadedAttention-229"><span class="linenos">229</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="MultiHeadedAttention-230"><a href="#MultiHeadedAttention-230"><span class="linenos">230</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="MultiHeadedAttention-231"><a href="#MultiHeadedAttention-231"><span class="linenos">231</span></a>        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>
</span><span id="MultiHeadedAttention-232"><a href="#MultiHeadedAttention-232"><span class="linenos">232</span></a>
</span><span id="MultiHeadedAttention-233"><a href="#MultiHeadedAttention-233"><span class="linenos">233</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_attention</span><span class="p">(</span>
</span><span id="MultiHeadedAttention-234"><a href="#MultiHeadedAttention-234"><span class="linenos">234</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-235"><a href="#MultiHeadedAttention-235"><span class="linenos">235</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-236"><a href="#MultiHeadedAttention-236"><span class="linenos">236</span></a>        <span class="n">scores</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-237"><a href="#MultiHeadedAttention-237"><span class="linenos">237</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="MultiHeadedAttention-238"><a href="#MultiHeadedAttention-238"><span class="linenos">238</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MultiHeadedAttention-239"><a href="#MultiHeadedAttention-239"><span class="linenos">239</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute attention context vector.</span>
</span><span id="MultiHeadedAttention-240"><a href="#MultiHeadedAttention-240"><span class="linenos">240</span></a>
</span><span id="MultiHeadedAttention-241"><a href="#MultiHeadedAttention-241"><span class="linenos">241</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention-242"><a href="#MultiHeadedAttention-242"><span class="linenos">242</span></a><span class="sd">            value (torch.Tensor): Transformed value, size</span>
</span><span id="MultiHeadedAttention-243"><a href="#MultiHeadedAttention-243"><span class="linenos">243</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention-244"><a href="#MultiHeadedAttention-244"><span class="linenos">244</span></a><span class="sd">            scores (torch.Tensor): Attention score, size</span>
</span><span id="MultiHeadedAttention-245"><a href="#MultiHeadedAttention-245"><span class="linenos">245</span></a><span class="sd">                (#batch, n_head, time1, time2).</span>
</span><span id="MultiHeadedAttention-246"><a href="#MultiHeadedAttention-246"><span class="linenos">246</span></a><span class="sd">            mask (torch.Tensor): Mask, size (#batch, 1, time2) or</span>
</span><span id="MultiHeadedAttention-247"><a href="#MultiHeadedAttention-247"><span class="linenos">247</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="MultiHeadedAttention-248"><a href="#MultiHeadedAttention-248"><span class="linenos">248</span></a>
</span><span id="MultiHeadedAttention-249"><a href="#MultiHeadedAttention-249"><span class="linenos">249</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention-250"><a href="#MultiHeadedAttention-250"><span class="linenos">250</span></a><span class="sd">            torch.Tensor: Transformed value (#batch, time1, d_model)</span>
</span><span id="MultiHeadedAttention-251"><a href="#MultiHeadedAttention-251"><span class="linenos">251</span></a><span class="sd">                weighted by the attention score (#batch, time1, time2).</span>
</span><span id="MultiHeadedAttention-252"><a href="#MultiHeadedAttention-252"><span class="linenos">252</span></a>
</span><span id="MultiHeadedAttention-253"><a href="#MultiHeadedAttention-253"><span class="linenos">253</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention-254"><a href="#MultiHeadedAttention-254"><span class="linenos">254</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-255"><a href="#MultiHeadedAttention-255"><span class="linenos">255</span></a>
</span><span id="MultiHeadedAttention-256"><a href="#MultiHeadedAttention-256"><span class="linenos">256</span></a>        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time2 &gt; 0</span>
</span><span id="MultiHeadedAttention-257"><a href="#MultiHeadedAttention-257"><span class="linenos">257</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="MultiHeadedAttention-258"><a href="#MultiHeadedAttention-258"><span class="linenos">258</span></a>            <span class="c1"># For last chunk, time2 might be larger than scores.size(-1)</span>
</span><span id="MultiHeadedAttention-259"><a href="#MultiHeadedAttention-259"><span class="linenos">259</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="MultiHeadedAttention-260"><a href="#MultiHeadedAttention-260"><span class="linenos">260</span></a>            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</span><span id="MultiHeadedAttention-261"><a href="#MultiHeadedAttention-261"><span class="linenos">261</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span><span id="MultiHeadedAttention-262"><a href="#MultiHeadedAttention-262"><span class="linenos">262</span></a>                <span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span>
</span><span id="MultiHeadedAttention-263"><a href="#MultiHeadedAttention-263"><span class="linenos">263</span></a>            <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="MultiHeadedAttention-264"><a href="#MultiHeadedAttention-264"><span class="linenos">264</span></a>
</span><span id="MultiHeadedAttention-265"><a href="#MultiHeadedAttention-265"><span class="linenos">265</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiHeadedAttention-266"><a href="#MultiHeadedAttention-266"><span class="linenos">266</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="MultiHeadedAttention-267"><a href="#MultiHeadedAttention-267"><span class="linenos">267</span></a>
</span><span id="MultiHeadedAttention-268"><a href="#MultiHeadedAttention-268"><span class="linenos">268</span></a>        <span class="n">p_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-269"><a href="#MultiHeadedAttention-269"><span class="linenos">269</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="MultiHeadedAttention-270"><a href="#MultiHeadedAttention-270"><span class="linenos">270</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiHeadedAttention-271"><a href="#MultiHeadedAttention-271"><span class="linenos">271</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-272"><a href="#MultiHeadedAttention-272"><span class="linenos">272</span></a>        <span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span><span id="MultiHeadedAttention-273"><a href="#MultiHeadedAttention-273"><span class="linenos">273</span></a>
</span><span id="MultiHeadedAttention-274"><a href="#MultiHeadedAttention-274"><span class="linenos">274</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span><span id="MultiHeadedAttention-275"><a href="#MultiHeadedAttention-275"><span class="linenos">275</span></a>
</span><span id="MultiHeadedAttention-276"><a href="#MultiHeadedAttention-276"><span class="linenos">276</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="MultiHeadedAttention-277"><a href="#MultiHeadedAttention-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-278"><a href="#MultiHeadedAttention-278"><span class="linenos">278</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-279"><a href="#MultiHeadedAttention-279"><span class="linenos">279</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-280"><a href="#MultiHeadedAttention-280"><span class="linenos">280</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention-281"><a href="#MultiHeadedAttention-281"><span class="linenos">281</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="MultiHeadedAttention-282"><a href="#MultiHeadedAttention-282"><span class="linenos">282</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="MultiHeadedAttention-283"><a href="#MultiHeadedAttention-283"><span class="linenos">283</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="MultiHeadedAttention-284"><a href="#MultiHeadedAttention-284"><span class="linenos">284</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="MultiHeadedAttention-285"><a href="#MultiHeadedAttention-285"><span class="linenos">285</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute scaled dot product attention.</span>
</span><span id="MultiHeadedAttention-286"><a href="#MultiHeadedAttention-286"><span class="linenos">286</span></a>
</span><span id="MultiHeadedAttention-287"><a href="#MultiHeadedAttention-287"><span class="linenos">287</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention-288"><a href="#MultiHeadedAttention-288"><span class="linenos">288</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="MultiHeadedAttention-289"><a href="#MultiHeadedAttention-289"><span class="linenos">289</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention-290"><a href="#MultiHeadedAttention-290"><span class="linenos">290</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention-291"><a href="#MultiHeadedAttention-291"><span class="linenos">291</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="MultiHeadedAttention-292"><a href="#MultiHeadedAttention-292"><span class="linenos">292</span></a><span class="sd">                (#batch, time1, time2).</span>
</span><span id="MultiHeadedAttention-293"><a href="#MultiHeadedAttention-293"><span class="linenos">293</span></a><span class="sd">                1.When applying cross attention between decoder and encoder,</span>
</span><span id="MultiHeadedAttention-294"><a href="#MultiHeadedAttention-294"><span class="linenos">294</span></a><span class="sd">                the batch padding mask for input is in (#batch, 1, T) shape.</span>
</span><span id="MultiHeadedAttention-295"><a href="#MultiHeadedAttention-295"><span class="linenos">295</span></a><span class="sd">                2.When applying self attention of encoder,</span>
</span><span id="MultiHeadedAttention-296"><a href="#MultiHeadedAttention-296"><span class="linenos">296</span></a><span class="sd">                the mask is in (#batch, T, T)  shape.</span>
</span><span id="MultiHeadedAttention-297"><a href="#MultiHeadedAttention-297"><span class="linenos">297</span></a><span class="sd">                3.When applying self attention of decoder,</span>
</span><span id="MultiHeadedAttention-298"><a href="#MultiHeadedAttention-298"><span class="linenos">298</span></a><span class="sd">                the mask is in (#batch, L, L)  shape.</span>
</span><span id="MultiHeadedAttention-299"><a href="#MultiHeadedAttention-299"><span class="linenos">299</span></a><span class="sd">                4.If the different position in decoder see different block</span>
</span><span id="MultiHeadedAttention-300"><a href="#MultiHeadedAttention-300"><span class="linenos">300</span></a><span class="sd">                of the encoder, such as Mocha, the passed in mask could be</span>
</span><span id="MultiHeadedAttention-301"><a href="#MultiHeadedAttention-301"><span class="linenos">301</span></a><span class="sd">                in (#batch, L, T) shape. But there is no such case in current</span>
</span><span id="MultiHeadedAttention-302"><a href="#MultiHeadedAttention-302"><span class="linenos">302</span></a><span class="sd">                CosyVoice.</span>
</span><span id="MultiHeadedAttention-303"><a href="#MultiHeadedAttention-303"><span class="linenos">303</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="MultiHeadedAttention-304"><a href="#MultiHeadedAttention-304"><span class="linenos">304</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="MultiHeadedAttention-305"><a href="#MultiHeadedAttention-305"><span class="linenos">305</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="MultiHeadedAttention-306"><a href="#MultiHeadedAttention-306"><span class="linenos">306</span></a>
</span><span id="MultiHeadedAttention-307"><a href="#MultiHeadedAttention-307"><span class="linenos">307</span></a>
</span><span id="MultiHeadedAttention-308"><a href="#MultiHeadedAttention-308"><span class="linenos">308</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention-309"><a href="#MultiHeadedAttention-309"><span class="linenos">309</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="MultiHeadedAttention-310"><a href="#MultiHeadedAttention-310"><span class="linenos">310</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="MultiHeadedAttention-311"><a href="#MultiHeadedAttention-311"><span class="linenos">311</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="MultiHeadedAttention-312"><a href="#MultiHeadedAttention-312"><span class="linenos">312</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="MultiHeadedAttention-313"><a href="#MultiHeadedAttention-313"><span class="linenos">313</span></a>
</span><span id="MultiHeadedAttention-314"><a href="#MultiHeadedAttention-314"><span class="linenos">314</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention-315"><a href="#MultiHeadedAttention-315"><span class="linenos">315</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-316"><a href="#MultiHeadedAttention-316"><span class="linenos">316</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MultiHeadedAttention-317"><a href="#MultiHeadedAttention-317"><span class="linenos">317</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-318"><a href="#MultiHeadedAttention-318"><span class="linenos">318</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-319"><a href="#MultiHeadedAttention-319"><span class="linenos">319</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-320"><a href="#MultiHeadedAttention-320"><span class="linenos">320</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-321"><a href="#MultiHeadedAttention-321"><span class="linenos">321</span></a>
</span><span id="MultiHeadedAttention-322"><a href="#MultiHeadedAttention-322"><span class="linenos">322</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention-323"><a href="#MultiHeadedAttention-323"><span class="linenos">323</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>Multi-Head Attention layer.</p>

<p>Args:
    n_head (int): The number of heads.
    n_feat (int): The number of features.
    dropout_rate (float): Dropout rate.</p>
</div>


                            <div id="MultiHeadedAttention.__init__" class="classattr">
                                        <input id="MultiHeadedAttention.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MultiHeadedAttention</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>, </span><span class="param"><span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="MultiHeadedAttention.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiHeadedAttention.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiHeadedAttention.__init__-190"><a href="#MultiHeadedAttention.__init__-190"><span class="linenos">190</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="MultiHeadedAttention.__init__-191"><a href="#MultiHeadedAttention.__init__-191"><span class="linenos">191</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MultiHeadedAttention.__init__-192"><a href="#MultiHeadedAttention.__init__-192"><span class="linenos">192</span></a>    <span class="p">):</span>
</span><span id="MultiHeadedAttention.__init__-193"><a href="#MultiHeadedAttention.__init__-193"><span class="linenos">193</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an MultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention.__init__-194"><a href="#MultiHeadedAttention.__init__-194"><span class="linenos">194</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MultiHeadedAttention.__init__-195"><a href="#MultiHeadedAttention.__init__-195"><span class="linenos">195</span></a>        <span class="k">assert</span> <span class="n">n_feat</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="MultiHeadedAttention.__init__-196"><a href="#MultiHeadedAttention.__init__-196"><span class="linenos">196</span></a>        <span class="c1"># We assume d_v always equals d_k</span>
</span><span id="MultiHeadedAttention.__init__-197"><a href="#MultiHeadedAttention.__init__-197"><span class="linenos">197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">n_feat</span> <span class="o">//</span> <span class="n">n_head</span>
</span><span id="MultiHeadedAttention.__init__-198"><a href="#MultiHeadedAttention.__init__-198"><span class="linenos">198</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">n_head</span>
</span><span id="MultiHeadedAttention.__init__-199"><a href="#MultiHeadedAttention.__init__-199"><span class="linenos">199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.__init__-200"><a href="#MultiHeadedAttention.__init__-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">key_bias</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.__init__-201"><a href="#MultiHeadedAttention.__init__-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.__init__-202"><a href="#MultiHeadedAttention.__init__-202"><span class="linenos">202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.__init__-203"><a href="#MultiHeadedAttention.__init__-203"><span class="linenos">203</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct an MultiHeadedAttention object.</p>
</div>


                            </div>
                            <div id="MultiHeadedAttention.d_k" class="classattr">
                                <div class="attr variable">
            <span class="name">d_k</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.d_k"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.h" class="classattr">
                                <div class="attr variable">
            <span class="name">h</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.h"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.linear_q" class="classattr">
                                <div class="attr variable">
            <span class="name">linear_q</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.linear_q"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.linear_k" class="classattr">
                                <div class="attr variable">
            <span class="name">linear_k</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.linear_k"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.linear_v" class="classattr">
                                <div class="attr variable">
            <span class="name">linear_v</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.linear_v"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.linear_out" class="classattr">
                                <div class="attr variable">
            <span class="name">linear_out</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.linear_out"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">dropout</span>

        
    </div>
    <a class="headerlink" href="#MultiHeadedAttention.dropout"></a>
    
    

                            </div>
                            <div id="MultiHeadedAttention.forward_qkv" class="classattr">
                                        <input id="MultiHeadedAttention.forward_qkv-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward_qkv</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="MultiHeadedAttention.forward_qkv-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiHeadedAttention.forward_qkv"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiHeadedAttention.forward_qkv-205"><a href="#MultiHeadedAttention.forward_qkv-205"><span class="linenos">205</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_qkv</span><span class="p">(</span>
</span><span id="MultiHeadedAttention.forward_qkv-206"><a href="#MultiHeadedAttention.forward_qkv-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</span><span id="MultiHeadedAttention.forward_qkv-207"><a href="#MultiHeadedAttention.forward_qkv-207"><span class="linenos">207</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="MultiHeadedAttention.forward_qkv-208"><a href="#MultiHeadedAttention.forward_qkv-208"><span class="linenos">208</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform query, key and value.</span>
</span><span id="MultiHeadedAttention.forward_qkv-209"><a href="#MultiHeadedAttention.forward_qkv-209"><span class="linenos">209</span></a>
</span><span id="MultiHeadedAttention.forward_qkv-210"><a href="#MultiHeadedAttention.forward_qkv-210"><span class="linenos">210</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention.forward_qkv-211"><a href="#MultiHeadedAttention.forward_qkv-211"><span class="linenos">211</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="MultiHeadedAttention.forward_qkv-212"><a href="#MultiHeadedAttention.forward_qkv-212"><span class="linenos">212</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention.forward_qkv-213"><a href="#MultiHeadedAttention.forward_qkv-213"><span class="linenos">213</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention.forward_qkv-214"><a href="#MultiHeadedAttention.forward_qkv-214"><span class="linenos">214</span></a>
</span><span id="MultiHeadedAttention.forward_qkv-215"><a href="#MultiHeadedAttention.forward_qkv-215"><span class="linenos">215</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention.forward_qkv-216"><a href="#MultiHeadedAttention.forward_qkv-216"><span class="linenos">216</span></a><span class="sd">            torch.Tensor: Transformed query tensor, size</span>
</span><span id="MultiHeadedAttention.forward_qkv-217"><a href="#MultiHeadedAttention.forward_qkv-217"><span class="linenos">217</span></a><span class="sd">                (#batch, n_head, time1, d_k).</span>
</span><span id="MultiHeadedAttention.forward_qkv-218"><a href="#MultiHeadedAttention.forward_qkv-218"><span class="linenos">218</span></a><span class="sd">            torch.Tensor: Transformed key tensor, size</span>
</span><span id="MultiHeadedAttention.forward_qkv-219"><a href="#MultiHeadedAttention.forward_qkv-219"><span class="linenos">219</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention.forward_qkv-220"><a href="#MultiHeadedAttention.forward_qkv-220"><span class="linenos">220</span></a><span class="sd">            torch.Tensor: Transformed value tensor, size</span>
</span><span id="MultiHeadedAttention.forward_qkv-221"><a href="#MultiHeadedAttention.forward_qkv-221"><span class="linenos">221</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention.forward_qkv-222"><a href="#MultiHeadedAttention.forward_qkv-222"><span class="linenos">222</span></a>
</span><span id="MultiHeadedAttention.forward_qkv-223"><a href="#MultiHeadedAttention.forward_qkv-223"><span class="linenos">223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention.forward_qkv-224"><a href="#MultiHeadedAttention.forward_qkv-224"><span class="linenos">224</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_qkv-225"><a href="#MultiHeadedAttention.forward_qkv-225"><span class="linenos">225</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_qkv-226"><a href="#MultiHeadedAttention.forward_qkv-226"><span class="linenos">226</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_qkv-227"><a href="#MultiHeadedAttention.forward_qkv-227"><span class="linenos">227</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_qkv-228"><a href="#MultiHeadedAttention.forward_qkv-228"><span class="linenos">228</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="MultiHeadedAttention.forward_qkv-229"><a href="#MultiHeadedAttention.forward_qkv-229"><span class="linenos">229</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="MultiHeadedAttention.forward_qkv-230"><a href="#MultiHeadedAttention.forward_qkv-230"><span class="linenos">230</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time2, d_k)</span>
</span><span id="MultiHeadedAttention.forward_qkv-231"><a href="#MultiHeadedAttention.forward_qkv-231"><span class="linenos">231</span></a>        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>
</span></pre></div>


            <div class="docstring"><p>Transform query, key and value.</p>

<p>Args:
    query (torch.Tensor): Query tensor (#batch, time1, size).
    key (torch.Tensor): Key tensor (#batch, time2, size).
    value (torch.Tensor): Value tensor (#batch, time2, size).</p>

<p>Returns:
    torch.Tensor: Transformed query tensor, size
        (#batch, n_head, time1, d_k).
    torch.Tensor: Transformed key tensor, size
        (#batch, n_head, time2, d_k).
    torch.Tensor: Transformed value tensor, size
        (#batch, n_head, time2, d_k).</p>
</div>


                            </div>
                            <div id="MultiHeadedAttention.forward_attention" class="classattr">
                                        <input id="MultiHeadedAttention.forward_attention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward_attention</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">scores</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MultiHeadedAttention.forward_attention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiHeadedAttention.forward_attention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiHeadedAttention.forward_attention-233"><a href="#MultiHeadedAttention.forward_attention-233"><span class="linenos">233</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_attention</span><span class="p">(</span>
</span><span id="MultiHeadedAttention.forward_attention-234"><a href="#MultiHeadedAttention.forward_attention-234"><span class="linenos">234</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward_attention-235"><a href="#MultiHeadedAttention.forward_attention-235"><span class="linenos">235</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward_attention-236"><a href="#MultiHeadedAttention.forward_attention-236"><span class="linenos">236</span></a>        <span class="n">scores</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward_attention-237"><a href="#MultiHeadedAttention.forward_attention-237"><span class="linenos">237</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="MultiHeadedAttention.forward_attention-238"><a href="#MultiHeadedAttention.forward_attention-238"><span class="linenos">238</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MultiHeadedAttention.forward_attention-239"><a href="#MultiHeadedAttention.forward_attention-239"><span class="linenos">239</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute attention context vector.</span>
</span><span id="MultiHeadedAttention.forward_attention-240"><a href="#MultiHeadedAttention.forward_attention-240"><span class="linenos">240</span></a>
</span><span id="MultiHeadedAttention.forward_attention-241"><a href="#MultiHeadedAttention.forward_attention-241"><span class="linenos">241</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention.forward_attention-242"><a href="#MultiHeadedAttention.forward_attention-242"><span class="linenos">242</span></a><span class="sd">            value (torch.Tensor): Transformed value, size</span>
</span><span id="MultiHeadedAttention.forward_attention-243"><a href="#MultiHeadedAttention.forward_attention-243"><span class="linenos">243</span></a><span class="sd">                (#batch, n_head, time2, d_k).</span>
</span><span id="MultiHeadedAttention.forward_attention-244"><a href="#MultiHeadedAttention.forward_attention-244"><span class="linenos">244</span></a><span class="sd">            scores (torch.Tensor): Attention score, size</span>
</span><span id="MultiHeadedAttention.forward_attention-245"><a href="#MultiHeadedAttention.forward_attention-245"><span class="linenos">245</span></a><span class="sd">                (#batch, n_head, time1, time2).</span>
</span><span id="MultiHeadedAttention.forward_attention-246"><a href="#MultiHeadedAttention.forward_attention-246"><span class="linenos">246</span></a><span class="sd">            mask (torch.Tensor): Mask, size (#batch, 1, time2) or</span>
</span><span id="MultiHeadedAttention.forward_attention-247"><a href="#MultiHeadedAttention.forward_attention-247"><span class="linenos">247</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="MultiHeadedAttention.forward_attention-248"><a href="#MultiHeadedAttention.forward_attention-248"><span class="linenos">248</span></a>
</span><span id="MultiHeadedAttention.forward_attention-249"><a href="#MultiHeadedAttention.forward_attention-249"><span class="linenos">249</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention.forward_attention-250"><a href="#MultiHeadedAttention.forward_attention-250"><span class="linenos">250</span></a><span class="sd">            torch.Tensor: Transformed value (#batch, time1, d_model)</span>
</span><span id="MultiHeadedAttention.forward_attention-251"><a href="#MultiHeadedAttention.forward_attention-251"><span class="linenos">251</span></a><span class="sd">                weighted by the attention score (#batch, time1, time2).</span>
</span><span id="MultiHeadedAttention.forward_attention-252"><a href="#MultiHeadedAttention.forward_attention-252"><span class="linenos">252</span></a>
</span><span id="MultiHeadedAttention.forward_attention-253"><a href="#MultiHeadedAttention.forward_attention-253"><span class="linenos">253</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention.forward_attention-254"><a href="#MultiHeadedAttention.forward_attention-254"><span class="linenos">254</span></a>        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_attention-255"><a href="#MultiHeadedAttention.forward_attention-255"><span class="linenos">255</span></a>
</span><span id="MultiHeadedAttention.forward_attention-256"><a href="#MultiHeadedAttention.forward_attention-256"><span class="linenos">256</span></a>        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># time2 &gt; 0</span>
</span><span id="MultiHeadedAttention.forward_attention-257"><a href="#MultiHeadedAttention.forward_attention-257"><span class="linenos">257</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="MultiHeadedAttention.forward_attention-258"><a href="#MultiHeadedAttention.forward_attention-258"><span class="linenos">258</span></a>            <span class="c1"># For last chunk, time2 might be larger than scores.size(-1)</span>
</span><span id="MultiHeadedAttention.forward_attention-259"><a href="#MultiHeadedAttention.forward_attention-259"><span class="linenos">259</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>  <span class="c1"># (batch, 1, *, time2)</span>
</span><span id="MultiHeadedAttention.forward_attention-260"><a href="#MultiHeadedAttention.forward_attention-260"><span class="linenos">260</span></a>            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</span><span id="MultiHeadedAttention.forward_attention-261"><a href="#MultiHeadedAttention.forward_attention-261"><span class="linenos">261</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span><span id="MultiHeadedAttention.forward_attention-262"><a href="#MultiHeadedAttention.forward_attention-262"><span class="linenos">262</span></a>                <span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span>
</span><span id="MultiHeadedAttention.forward_attention-263"><a href="#MultiHeadedAttention.forward_attention-263"><span class="linenos">263</span></a>            <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="MultiHeadedAttention.forward_attention-264"><a href="#MultiHeadedAttention.forward_attention-264"><span class="linenos">264</span></a>
</span><span id="MultiHeadedAttention.forward_attention-265"><a href="#MultiHeadedAttention.forward_attention-265"><span class="linenos">265</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiHeadedAttention.forward_attention-266"><a href="#MultiHeadedAttention.forward_attention-266"><span class="linenos">266</span></a>            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="MultiHeadedAttention.forward_attention-267"><a href="#MultiHeadedAttention.forward_attention-267"><span class="linenos">267</span></a>
</span><span id="MultiHeadedAttention.forward_attention-268"><a href="#MultiHeadedAttention.forward_attention-268"><span class="linenos">268</span></a>        <span class="n">p_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_attention-269"><a href="#MultiHeadedAttention.forward_attention-269"><span class="linenos">269</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="MultiHeadedAttention.forward_attention-270"><a href="#MultiHeadedAttention.forward_attention-270"><span class="linenos">270</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiHeadedAttention.forward_attention-271"><a href="#MultiHeadedAttention.forward_attention-271"><span class="linenos">271</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward_attention-272"><a href="#MultiHeadedAttention.forward_attention-272"><span class="linenos">272</span></a>        <span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span><span id="MultiHeadedAttention.forward_attention-273"><a href="#MultiHeadedAttention.forward_attention-273"><span class="linenos">273</span></a>
</span><span id="MultiHeadedAttention.forward_attention-274"><a href="#MultiHeadedAttention.forward_attention-274"><span class="linenos">274</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, time1, d_model)</span>
</span></pre></div>


            <div class="docstring"><p>Compute attention context vector.</p>

<p>Args:
    value (torch.Tensor): Transformed value, size
        (#batch, n_head, time2, d_k).
    scores (torch.Tensor): Attention score, size
        (#batch, n_head, time1, time2).
    mask (torch.Tensor): Mask, size (#batch, 1, time2) or
        (#batch, time1, time2), (0, 0, 0) means fake mask.</p>

<p>Returns:
    torch.Tensor: Transformed value (#batch, time1, d_model)
        weighted by the attention score (#batch, time1, time2).</p>
</div>


                            </div>
                            <div id="MultiHeadedAttention.forward" class="classattr">
                                        <input id="MultiHeadedAttention.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>,</span><span class="param">	<span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([])</span>,</span><span class="param">	<span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="MultiHeadedAttention.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiHeadedAttention.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiHeadedAttention.forward-276"><a href="#MultiHeadedAttention.forward-276"><span class="linenos">276</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="MultiHeadedAttention.forward-277"><a href="#MultiHeadedAttention.forward-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward-278"><a href="#MultiHeadedAttention.forward-278"><span class="linenos">278</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward-279"><a href="#MultiHeadedAttention.forward-279"><span class="linenos">279</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward-280"><a href="#MultiHeadedAttention.forward-280"><span class="linenos">280</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MultiHeadedAttention.forward-281"><a href="#MultiHeadedAttention.forward-281"><span class="linenos">281</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="MultiHeadedAttention.forward-282"><a href="#MultiHeadedAttention.forward-282"><span class="linenos">282</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="MultiHeadedAttention.forward-283"><a href="#MultiHeadedAttention.forward-283"><span class="linenos">283</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="MultiHeadedAttention.forward-284"><a href="#MultiHeadedAttention.forward-284"><span class="linenos">284</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="MultiHeadedAttention.forward-285"><a href="#MultiHeadedAttention.forward-285"><span class="linenos">285</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute scaled dot product attention.</span>
</span><span id="MultiHeadedAttention.forward-286"><a href="#MultiHeadedAttention.forward-286"><span class="linenos">286</span></a>
</span><span id="MultiHeadedAttention.forward-287"><a href="#MultiHeadedAttention.forward-287"><span class="linenos">287</span></a><span class="sd">        Args:</span>
</span><span id="MultiHeadedAttention.forward-288"><a href="#MultiHeadedAttention.forward-288"><span class="linenos">288</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="MultiHeadedAttention.forward-289"><a href="#MultiHeadedAttention.forward-289"><span class="linenos">289</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention.forward-290"><a href="#MultiHeadedAttention.forward-290"><span class="linenos">290</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="MultiHeadedAttention.forward-291"><a href="#MultiHeadedAttention.forward-291"><span class="linenos">291</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="MultiHeadedAttention.forward-292"><a href="#MultiHeadedAttention.forward-292"><span class="linenos">292</span></a><span class="sd">                (#batch, time1, time2).</span>
</span><span id="MultiHeadedAttention.forward-293"><a href="#MultiHeadedAttention.forward-293"><span class="linenos">293</span></a><span class="sd">                1.When applying cross attention between decoder and encoder,</span>
</span><span id="MultiHeadedAttention.forward-294"><a href="#MultiHeadedAttention.forward-294"><span class="linenos">294</span></a><span class="sd">                the batch padding mask for input is in (#batch, 1, T) shape.</span>
</span><span id="MultiHeadedAttention.forward-295"><a href="#MultiHeadedAttention.forward-295"><span class="linenos">295</span></a><span class="sd">                2.When applying self attention of encoder,</span>
</span><span id="MultiHeadedAttention.forward-296"><a href="#MultiHeadedAttention.forward-296"><span class="linenos">296</span></a><span class="sd">                the mask is in (#batch, T, T)  shape.</span>
</span><span id="MultiHeadedAttention.forward-297"><a href="#MultiHeadedAttention.forward-297"><span class="linenos">297</span></a><span class="sd">                3.When applying self attention of decoder,</span>
</span><span id="MultiHeadedAttention.forward-298"><a href="#MultiHeadedAttention.forward-298"><span class="linenos">298</span></a><span class="sd">                the mask is in (#batch, L, L)  shape.</span>
</span><span id="MultiHeadedAttention.forward-299"><a href="#MultiHeadedAttention.forward-299"><span class="linenos">299</span></a><span class="sd">                4.If the different position in decoder see different block</span>
</span><span id="MultiHeadedAttention.forward-300"><a href="#MultiHeadedAttention.forward-300"><span class="linenos">300</span></a><span class="sd">                of the encoder, such as Mocha, the passed in mask could be</span>
</span><span id="MultiHeadedAttention.forward-301"><a href="#MultiHeadedAttention.forward-301"><span class="linenos">301</span></a><span class="sd">                in (#batch, L, T) shape. But there is no such case in current</span>
</span><span id="MultiHeadedAttention.forward-302"><a href="#MultiHeadedAttention.forward-302"><span class="linenos">302</span></a><span class="sd">                CosyVoice.</span>
</span><span id="MultiHeadedAttention.forward-303"><a href="#MultiHeadedAttention.forward-303"><span class="linenos">303</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="MultiHeadedAttention.forward-304"><a href="#MultiHeadedAttention.forward-304"><span class="linenos">304</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="MultiHeadedAttention.forward-305"><a href="#MultiHeadedAttention.forward-305"><span class="linenos">305</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="MultiHeadedAttention.forward-306"><a href="#MultiHeadedAttention.forward-306"><span class="linenos">306</span></a>
</span><span id="MultiHeadedAttention.forward-307"><a href="#MultiHeadedAttention.forward-307"><span class="linenos">307</span></a>
</span><span id="MultiHeadedAttention.forward-308"><a href="#MultiHeadedAttention.forward-308"><span class="linenos">308</span></a><span class="sd">        Returns:</span>
</span><span id="MultiHeadedAttention.forward-309"><a href="#MultiHeadedAttention.forward-309"><span class="linenos">309</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="MultiHeadedAttention.forward-310"><a href="#MultiHeadedAttention.forward-310"><span class="linenos">310</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="MultiHeadedAttention.forward-311"><a href="#MultiHeadedAttention.forward-311"><span class="linenos">311</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="MultiHeadedAttention.forward-312"><a href="#MultiHeadedAttention.forward-312"><span class="linenos">312</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="MultiHeadedAttention.forward-313"><a href="#MultiHeadedAttention.forward-313"><span class="linenos">313</span></a>
</span><span id="MultiHeadedAttention.forward-314"><a href="#MultiHeadedAttention.forward-314"><span class="linenos">314</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiHeadedAttention.forward-315"><a href="#MultiHeadedAttention.forward-315"><span class="linenos">315</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-316"><a href="#MultiHeadedAttention.forward-316"><span class="linenos">316</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MultiHeadedAttention.forward-317"><a href="#MultiHeadedAttention.forward-317"><span class="linenos">317</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-318"><a href="#MultiHeadedAttention.forward-318"><span class="linenos">318</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-319"><a href="#MultiHeadedAttention.forward-319"><span class="linenos">319</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-320"><a href="#MultiHeadedAttention.forward-320"><span class="linenos">320</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-321"><a href="#MultiHeadedAttention.forward-321"><span class="linenos">321</span></a>
</span><span id="MultiHeadedAttention.forward-322"><a href="#MultiHeadedAttention.forward-322"><span class="linenos">322</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="MultiHeadedAttention.forward-323"><a href="#MultiHeadedAttention.forward-323"><span class="linenos">323</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>Compute scaled dot product attention.</p>

<p>Args:
    query (torch.Tensor): Query tensor (#batch, time1, size).
    key (torch.Tensor): Key tensor (#batch, time2, size).
    value (torch.Tensor): Value tensor (#batch, time2, size).
    mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
        (#batch, time1, time2).
        1.When applying cross attention between decoder and encoder,
        the batch padding mask for input is in (#batch, 1, T) shape.
        2.When applying self attention of encoder,
        the mask is in (#batch, T, T)  shape.
        3.When applying self attention of decoder,
        the mask is in (#batch, L, L)  shape.
        4.If the different position in decoder see different block
        of the encoder, such as Mocha, the passed in mask could be
        in (#batch, L, T) shape. But there is no such case in current
        CosyVoice.
    cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),
        where <code>cache_t == chunk_size * num_decoding_left_chunks</code>
        and <code>head * d_k == size</code></p>

<p>Returns:
    torch.Tensor: Output tensor (#batch, time1, d_model).
    torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)
        where <code>cache_t == chunk_size * num_decoding_left_chunks</code>
        and <code>head * d_k == size</code></p>
</div>


                            </div>
                </section>
                <section id="RelPositionMultiHeadedAttention">
                            <input id="RelPositionMultiHeadedAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">RelPositionMultiHeadedAttention</span><wbr>(<span class="base"><a href="#MultiHeadedAttention">MultiHeadedAttention</a></span>):

                <label class="view-source-button" for="RelPositionMultiHeadedAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RelPositionMultiHeadedAttention-326"><a href="#RelPositionMultiHeadedAttention-326"><span class="linenos">326</span></a><span class="k">class</span><span class="w"> </span><span class="nc">RelPositionMultiHeadedAttention</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
</span><span id="RelPositionMultiHeadedAttention-327"><a href="#RelPositionMultiHeadedAttention-327"><span class="linenos">327</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-Head Attention layer with relative position encoding.</span>
</span><span id="RelPositionMultiHeadedAttention-328"><a href="#RelPositionMultiHeadedAttention-328"><span class="linenos">328</span></a><span class="sd">    Paper: https://arxiv.org/abs/1901.02860</span>
</span><span id="RelPositionMultiHeadedAttention-329"><a href="#RelPositionMultiHeadedAttention-329"><span class="linenos">329</span></a><span class="sd">    Args:</span>
</span><span id="RelPositionMultiHeadedAttention-330"><a href="#RelPositionMultiHeadedAttention-330"><span class="linenos">330</span></a><span class="sd">        n_head (int): The number of heads.</span>
</span><span id="RelPositionMultiHeadedAttention-331"><a href="#RelPositionMultiHeadedAttention-331"><span class="linenos">331</span></a><span class="sd">        n_feat (int): The number of features.</span>
</span><span id="RelPositionMultiHeadedAttention-332"><a href="#RelPositionMultiHeadedAttention-332"><span class="linenos">332</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="RelPositionMultiHeadedAttention-333"><a href="#RelPositionMultiHeadedAttention-333"><span class="linenos">333</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention-334"><a href="#RelPositionMultiHeadedAttention-334"><span class="linenos">334</span></a>
</span><span id="RelPositionMultiHeadedAttention-335"><a href="#RelPositionMultiHeadedAttention-335"><span class="linenos">335</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention-336"><a href="#RelPositionMultiHeadedAttention-336"><span class="linenos">336</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="RelPositionMultiHeadedAttention-337"><a href="#RelPositionMultiHeadedAttention-337"><span class="linenos">337</span></a>    <span class="p">):</span>
</span><span id="RelPositionMultiHeadedAttention-338"><a href="#RelPositionMultiHeadedAttention-338"><span class="linenos">338</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an RelPositionMultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention-339"><a href="#RelPositionMultiHeadedAttention-339"><span class="linenos">339</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-340"><a href="#RelPositionMultiHeadedAttention-340"><span class="linenos">340</span></a>        <span class="c1"># linear transformation for positional encoding</span>
</span><span id="RelPositionMultiHeadedAttention-341"><a href="#RelPositionMultiHeadedAttention-341"><span class="linenos">341</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-342"><a href="#RelPositionMultiHeadedAttention-342"><span class="linenos">342</span></a>        <span class="c1"># these two learnable bias are used in matrix c and matrix d</span>
</span><span id="RelPositionMultiHeadedAttention-343"><a href="#RelPositionMultiHeadedAttention-343"><span class="linenos">343</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="RelPositionMultiHeadedAttention-344"><a href="#RelPositionMultiHeadedAttention-344"><span class="linenos">344</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention-345"><a href="#RelPositionMultiHeadedAttention-345"><span class="linenos">345</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention-346"><a href="#RelPositionMultiHeadedAttention-346"><span class="linenos">346</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-347"><a href="#RelPositionMultiHeadedAttention-347"><span class="linenos">347</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-348"><a href="#RelPositionMultiHeadedAttention-348"><span class="linenos">348</span></a>
</span><span id="RelPositionMultiHeadedAttention-349"><a href="#RelPositionMultiHeadedAttention-349"><span class="linenos">349</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">rel_shift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention-350"><a href="#RelPositionMultiHeadedAttention-350"><span class="linenos">350</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute relative positional encoding.</span>
</span><span id="RelPositionMultiHeadedAttention-351"><a href="#RelPositionMultiHeadedAttention-351"><span class="linenos">351</span></a>
</span><span id="RelPositionMultiHeadedAttention-352"><a href="#RelPositionMultiHeadedAttention-352"><span class="linenos">352</span></a><span class="sd">        Args:</span>
</span><span id="RelPositionMultiHeadedAttention-353"><a href="#RelPositionMultiHeadedAttention-353"><span class="linenos">353</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).</span>
</span><span id="RelPositionMultiHeadedAttention-354"><a href="#RelPositionMultiHeadedAttention-354"><span class="linenos">354</span></a><span class="sd">            time1 means the length of query vector.</span>
</span><span id="RelPositionMultiHeadedAttention-355"><a href="#RelPositionMultiHeadedAttention-355"><span class="linenos">355</span></a>
</span><span id="RelPositionMultiHeadedAttention-356"><a href="#RelPositionMultiHeadedAttention-356"><span class="linenos">356</span></a><span class="sd">        Returns:</span>
</span><span id="RelPositionMultiHeadedAttention-357"><a href="#RelPositionMultiHeadedAttention-357"><span class="linenos">357</span></a><span class="sd">            torch.Tensor: Output tensor.</span>
</span><span id="RelPositionMultiHeadedAttention-358"><a href="#RelPositionMultiHeadedAttention-358"><span class="linenos">358</span></a>
</span><span id="RelPositionMultiHeadedAttention-359"><a href="#RelPositionMultiHeadedAttention-359"><span class="linenos">359</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention-360"><a href="#RelPositionMultiHeadedAttention-360"><span class="linenos">360</span></a>        <span class="n">zero_pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention-361"><a href="#RelPositionMultiHeadedAttention-361"><span class="linenos">361</span></a>            <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="RelPositionMultiHeadedAttention-362"><a href="#RelPositionMultiHeadedAttention-362"><span class="linenos">362</span></a>        <span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-363"><a href="#RelPositionMultiHeadedAttention-363"><span class="linenos">363</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">zero_pad</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-364"><a href="#RelPositionMultiHeadedAttention-364"><span class="linenos">364</span></a>
</span><span id="RelPositionMultiHeadedAttention-365"><a href="#RelPositionMultiHeadedAttention-365"><span class="linenos">365</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">x_padded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention-366"><a href="#RelPositionMultiHeadedAttention-366"><span class="linenos">366</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x_padded</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span>
</span><span id="RelPositionMultiHeadedAttention-367"><a href="#RelPositionMultiHeadedAttention-367"><span class="linenos">367</span></a>            <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="RelPositionMultiHeadedAttention-368"><a href="#RelPositionMultiHeadedAttention-368"><span class="linenos">368</span></a>        <span class="p">]</span>  <span class="c1"># only keep the positions from 0 to time2</span>
</span><span id="RelPositionMultiHeadedAttention-369"><a href="#RelPositionMultiHeadedAttention-369"><span class="linenos">369</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="RelPositionMultiHeadedAttention-370"><a href="#RelPositionMultiHeadedAttention-370"><span class="linenos">370</span></a>
</span><span id="RelPositionMultiHeadedAttention-371"><a href="#RelPositionMultiHeadedAttention-371"><span class="linenos">371</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention-372"><a href="#RelPositionMultiHeadedAttention-372"><span class="linenos">372</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention-373"><a href="#RelPositionMultiHeadedAttention-373"><span class="linenos">373</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention-374"><a href="#RelPositionMultiHeadedAttention-374"><span class="linenos">374</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention-375"><a href="#RelPositionMultiHeadedAttention-375"><span class="linenos">375</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention-376"><a href="#RelPositionMultiHeadedAttention-376"><span class="linenos">376</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="RelPositionMultiHeadedAttention-377"><a href="#RelPositionMultiHeadedAttention-377"><span class="linenos">377</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="RelPositionMultiHeadedAttention-378"><a href="#RelPositionMultiHeadedAttention-378"><span class="linenos">378</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="RelPositionMultiHeadedAttention-379"><a href="#RelPositionMultiHeadedAttention-379"><span class="linenos">379</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="RelPositionMultiHeadedAttention-380"><a href="#RelPositionMultiHeadedAttention-380"><span class="linenos">380</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute &#39;Scaled Dot Product Attention&#39; with rel. positional encoding.</span>
</span><span id="RelPositionMultiHeadedAttention-381"><a href="#RelPositionMultiHeadedAttention-381"><span class="linenos">381</span></a><span class="sd">        Args:</span>
</span><span id="RelPositionMultiHeadedAttention-382"><a href="#RelPositionMultiHeadedAttention-382"><span class="linenos">382</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="RelPositionMultiHeadedAttention-383"><a href="#RelPositionMultiHeadedAttention-383"><span class="linenos">383</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention-384"><a href="#RelPositionMultiHeadedAttention-384"><span class="linenos">384</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention-385"><a href="#RelPositionMultiHeadedAttention-385"><span class="linenos">385</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="RelPositionMultiHeadedAttention-386"><a href="#RelPositionMultiHeadedAttention-386"><span class="linenos">386</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="RelPositionMultiHeadedAttention-387"><a href="#RelPositionMultiHeadedAttention-387"><span class="linenos">387</span></a><span class="sd">            pos_emb (torch.Tensor): Positional embedding tensor</span>
</span><span id="RelPositionMultiHeadedAttention-388"><a href="#RelPositionMultiHeadedAttention-388"><span class="linenos">388</span></a><span class="sd">                (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention-389"><a href="#RelPositionMultiHeadedAttention-389"><span class="linenos">389</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="RelPositionMultiHeadedAttention-390"><a href="#RelPositionMultiHeadedAttention-390"><span class="linenos">390</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="RelPositionMultiHeadedAttention-391"><a href="#RelPositionMultiHeadedAttention-391"><span class="linenos">391</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="RelPositionMultiHeadedAttention-392"><a href="#RelPositionMultiHeadedAttention-392"><span class="linenos">392</span></a><span class="sd">        Returns:</span>
</span><span id="RelPositionMultiHeadedAttention-393"><a href="#RelPositionMultiHeadedAttention-393"><span class="linenos">393</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="RelPositionMultiHeadedAttention-394"><a href="#RelPositionMultiHeadedAttention-394"><span class="linenos">394</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="RelPositionMultiHeadedAttention-395"><a href="#RelPositionMultiHeadedAttention-395"><span class="linenos">395</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="RelPositionMultiHeadedAttention-396"><a href="#RelPositionMultiHeadedAttention-396"><span class="linenos">396</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="RelPositionMultiHeadedAttention-397"><a href="#RelPositionMultiHeadedAttention-397"><span class="linenos">397</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention-398"><a href="#RelPositionMultiHeadedAttention-398"><span class="linenos">398</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-399"><a href="#RelPositionMultiHeadedAttention-399"><span class="linenos">399</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, time1, head, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention-400"><a href="#RelPositionMultiHeadedAttention-400"><span class="linenos">400</span></a>
</span><span id="RelPositionMultiHeadedAttention-401"><a href="#RelPositionMultiHeadedAttention-401"><span class="linenos">401</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention-402"><a href="#RelPositionMultiHeadedAttention-402"><span class="linenos">402</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-403"><a href="#RelPositionMultiHeadedAttention-403"><span class="linenos">403</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-404"><a href="#RelPositionMultiHeadedAttention-404"><span class="linenos">404</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-405"><a href="#RelPositionMultiHeadedAttention-405"><span class="linenos">405</span></a>        <span class="c1"># NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it&#39;s</span>
</span><span id="RelPositionMultiHeadedAttention-406"><a href="#RelPositionMultiHeadedAttention-406"><span class="linenos">406</span></a>        <span class="c1">#   non-trivial to calculate `next_cache_start` here.</span>
</span><span id="RelPositionMultiHeadedAttention-407"><a href="#RelPositionMultiHeadedAttention-407"><span class="linenos">407</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-408"><a href="#RelPositionMultiHeadedAttention-408"><span class="linenos">408</span></a>
</span><span id="RelPositionMultiHeadedAttention-409"><a href="#RelPositionMultiHeadedAttention-409"><span class="linenos">409</span></a>        <span class="n">n_batch_pos</span> <span class="o">=</span> <span class="n">pos_emb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-410"><a href="#RelPositionMultiHeadedAttention-410"><span class="linenos">410</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-411"><a href="#RelPositionMultiHeadedAttention-411"><span class="linenos">411</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention-412"><a href="#RelPositionMultiHeadedAttention-412"><span class="linenos">412</span></a>
</span><span id="RelPositionMultiHeadedAttention-413"><a href="#RelPositionMultiHeadedAttention-413"><span class="linenos">413</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention-414"><a href="#RelPositionMultiHeadedAttention-414"><span class="linenos">414</span></a>        <span class="n">q_with_bias_u</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-415"><a href="#RelPositionMultiHeadedAttention-415"><span class="linenos">415</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention-416"><a href="#RelPositionMultiHeadedAttention-416"><span class="linenos">416</span></a>        <span class="n">q_with_bias_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-417"><a href="#RelPositionMultiHeadedAttention-417"><span class="linenos">417</span></a>
</span><span id="RelPositionMultiHeadedAttention-418"><a href="#RelPositionMultiHeadedAttention-418"><span class="linenos">418</span></a>        <span class="c1"># compute attention score</span>
</span><span id="RelPositionMultiHeadedAttention-419"><a href="#RelPositionMultiHeadedAttention-419"><span class="linenos">419</span></a>        <span class="c1"># first compute matrix a and matrix c</span>
</span><span id="RelPositionMultiHeadedAttention-420"><a href="#RelPositionMultiHeadedAttention-420"><span class="linenos">420</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="RelPositionMultiHeadedAttention-421"><a href="#RelPositionMultiHeadedAttention-421"><span class="linenos">421</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention-422"><a href="#RelPositionMultiHeadedAttention-422"><span class="linenos">422</span></a>        <span class="n">matrix_ac</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_u</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention-423"><a href="#RelPositionMultiHeadedAttention-423"><span class="linenos">423</span></a>
</span><span id="RelPositionMultiHeadedAttention-424"><a href="#RelPositionMultiHeadedAttention-424"><span class="linenos">424</span></a>        <span class="c1"># compute matrix b and matrix d</span>
</span><span id="RelPositionMultiHeadedAttention-425"><a href="#RelPositionMultiHeadedAttention-425"><span class="linenos">425</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention-426"><a href="#RelPositionMultiHeadedAttention-426"><span class="linenos">426</span></a>        <span class="n">matrix_bd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_v</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention-427"><a href="#RelPositionMultiHeadedAttention-427"><span class="linenos">427</span></a>        <span class="c1"># NOTE(Xiang Lyu): Keep rel_shift since espnet rel_pos_emb is used</span>
</span><span id="RelPositionMultiHeadedAttention-428"><a href="#RelPositionMultiHeadedAttention-428"><span class="linenos">428</span></a>        <span class="k">if</span> <span class="n">matrix_ac</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">matrix_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention-429"><a href="#RelPositionMultiHeadedAttention-429"><span class="linenos">429</span></a>            <span class="n">matrix_bd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rel_shift</span><span class="p">(</span><span class="n">matrix_bd</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention-430"><a href="#RelPositionMultiHeadedAttention-430"><span class="linenos">430</span></a>
</span><span id="RelPositionMultiHeadedAttention-431"><a href="#RelPositionMultiHeadedAttention-431"><span class="linenos">431</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix_ac</span> <span class="o">+</span> <span class="n">matrix_bd</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention-432"><a href="#RelPositionMultiHeadedAttention-432"><span class="linenos">432</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
</span><span id="RelPositionMultiHeadedAttention-433"><a href="#RelPositionMultiHeadedAttention-433"><span class="linenos">433</span></a>        <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention-434"><a href="#RelPositionMultiHeadedAttention-434"><span class="linenos">434</span></a>
</span><span id="RelPositionMultiHeadedAttention-435"><a href="#RelPositionMultiHeadedAttention-435"><span class="linenos">435</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>Multi-Head Attention layer with relative position encoding.
Paper: <a href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a>
Args:
    n_head (int): The number of heads.
    n_feat (int): The number of features.
    dropout_rate (float): Dropout rate.</p>
</div>


                            <div id="RelPositionMultiHeadedAttention.__init__" class="classattr">
                                        <input id="RelPositionMultiHeadedAttention.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">RelPositionMultiHeadedAttention</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>, </span><span class="param"><span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="RelPositionMultiHeadedAttention.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RelPositionMultiHeadedAttention.__init__-335"><a href="#RelPositionMultiHeadedAttention.__init__-335"><span class="linenos">335</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-336"><a href="#RelPositionMultiHeadedAttention.__init__-336"><span class="linenos">336</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-337"><a href="#RelPositionMultiHeadedAttention.__init__-337"><span class="linenos">337</span></a>    <span class="p">):</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-338"><a href="#RelPositionMultiHeadedAttention.__init__-338"><span class="linenos">338</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an RelPositionMultiHeadedAttention object.&quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-339"><a href="#RelPositionMultiHeadedAttention.__init__-339"><span class="linenos">339</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-340"><a href="#RelPositionMultiHeadedAttention.__init__-340"><span class="linenos">340</span></a>        <span class="c1"># linear transformation for positional encoding</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-341"><a href="#RelPositionMultiHeadedAttention.__init__-341"><span class="linenos">341</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feat</span><span class="p">,</span> <span class="n">n_feat</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-342"><a href="#RelPositionMultiHeadedAttention.__init__-342"><span class="linenos">342</span></a>        <span class="c1"># these two learnable bias are used in matrix c and matrix d</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-343"><a href="#RelPositionMultiHeadedAttention.__init__-343"><span class="linenos">343</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-344"><a href="#RelPositionMultiHeadedAttention.__init__-344"><span class="linenos">344</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-345"><a href="#RelPositionMultiHeadedAttention.__init__-345"><span class="linenos">345</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-346"><a href="#RelPositionMultiHeadedAttention.__init__-346"><span class="linenos">346</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.__init__-347"><a href="#RelPositionMultiHeadedAttention.__init__-347"><span class="linenos">347</span></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct an RelPositionMultiHeadedAttention object.</p>
</div>


                            </div>
                            <div id="RelPositionMultiHeadedAttention.linear_pos" class="classattr">
                                <div class="attr variable">
            <span class="name">linear_pos</span>

        
    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.linear_pos"></a>
    
    

                            </div>
                            <div id="RelPositionMultiHeadedAttention.pos_bias_u" class="classattr">
                                <div class="attr variable">
            <span class="name">pos_bias_u</span>

        
    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.pos_bias_u"></a>
    
    

                            </div>
                            <div id="RelPositionMultiHeadedAttention.pos_bias_v" class="classattr">
                                <div class="attr variable">
            <span class="name">pos_bias_v</span>

        
    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.pos_bias_v"></a>
    
    

                            </div>
                            <div id="RelPositionMultiHeadedAttention.rel_shift" class="classattr">
                                        <input id="RelPositionMultiHeadedAttention.rel_shift-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">rel_shift</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="RelPositionMultiHeadedAttention.rel_shift-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.rel_shift"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RelPositionMultiHeadedAttention.rel_shift-349"><a href="#RelPositionMultiHeadedAttention.rel_shift-349"><span class="linenos">349</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">rel_shift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-350"><a href="#RelPositionMultiHeadedAttention.rel_shift-350"><span class="linenos">350</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute relative positional encoding.</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-351"><a href="#RelPositionMultiHeadedAttention.rel_shift-351"><span class="linenos">351</span></a>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-352"><a href="#RelPositionMultiHeadedAttention.rel_shift-352"><span class="linenos">352</span></a><span class="sd">        Args:</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-353"><a href="#RelPositionMultiHeadedAttention.rel_shift-353"><span class="linenos">353</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-354"><a href="#RelPositionMultiHeadedAttention.rel_shift-354"><span class="linenos">354</span></a><span class="sd">            time1 means the length of query vector.</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-355"><a href="#RelPositionMultiHeadedAttention.rel_shift-355"><span class="linenos">355</span></a>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-356"><a href="#RelPositionMultiHeadedAttention.rel_shift-356"><span class="linenos">356</span></a><span class="sd">        Returns:</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-357"><a href="#RelPositionMultiHeadedAttention.rel_shift-357"><span class="linenos">357</span></a><span class="sd">            torch.Tensor: Output tensor.</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-358"><a href="#RelPositionMultiHeadedAttention.rel_shift-358"><span class="linenos">358</span></a>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-359"><a href="#RelPositionMultiHeadedAttention.rel_shift-359"><span class="linenos">359</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-360"><a href="#RelPositionMultiHeadedAttention.rel_shift-360"><span class="linenos">360</span></a>        <span class="n">zero_pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-361"><a href="#RelPositionMultiHeadedAttention.rel_shift-361"><span class="linenos">361</span></a>            <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-362"><a href="#RelPositionMultiHeadedAttention.rel_shift-362"><span class="linenos">362</span></a>        <span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-363"><a href="#RelPositionMultiHeadedAttention.rel_shift-363"><span class="linenos">363</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">zero_pad</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-364"><a href="#RelPositionMultiHeadedAttention.rel_shift-364"><span class="linenos">364</span></a>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-365"><a href="#RelPositionMultiHeadedAttention.rel_shift-365"><span class="linenos">365</span></a>        <span class="n">x_padded</span> <span class="o">=</span> <span class="n">x_padded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-366"><a href="#RelPositionMultiHeadedAttention.rel_shift-366"><span class="linenos">366</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x_padded</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-367"><a href="#RelPositionMultiHeadedAttention.rel_shift-367"><span class="linenos">367</span></a>            <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-368"><a href="#RelPositionMultiHeadedAttention.rel_shift-368"><span class="linenos">368</span></a>        <span class="p">]</span>  <span class="c1"># only keep the positions from 0 to time2</span>
</span><span id="RelPositionMultiHeadedAttention.rel_shift-369"><a href="#RelPositionMultiHeadedAttention.rel_shift-369"><span class="linenos">369</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Compute relative positional encoding.</p>

<p>Args:
    x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).
    time1 means the length of query vector.</p>

<p>Returns:
    torch.Tensor: Output tensor.</p>
</div>


                            </div>
                            <div id="RelPositionMultiHeadedAttention.forward" class="classattr">
                                        <input id="RelPositionMultiHeadedAttention.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>,</span><span class="param">	<span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([])</span>,</span><span class="param">	<span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="RelPositionMultiHeadedAttention.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RelPositionMultiHeadedAttention.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RelPositionMultiHeadedAttention.forward-371"><a href="#RelPositionMultiHeadedAttention.forward-371"><span class="linenos">371</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention.forward-372"><a href="#RelPositionMultiHeadedAttention.forward-372"><span class="linenos">372</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention.forward-373"><a href="#RelPositionMultiHeadedAttention.forward-373"><span class="linenos">373</span></a>        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention.forward-374"><a href="#RelPositionMultiHeadedAttention.forward-374"><span class="linenos">374</span></a>        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention.forward-375"><a href="#RelPositionMultiHeadedAttention.forward-375"><span class="linenos">375</span></a>        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="RelPositionMultiHeadedAttention.forward-376"><a href="#RelPositionMultiHeadedAttention.forward-376"><span class="linenos">376</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="RelPositionMultiHeadedAttention.forward-377"><a href="#RelPositionMultiHeadedAttention.forward-377"><span class="linenos">377</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="RelPositionMultiHeadedAttention.forward-378"><a href="#RelPositionMultiHeadedAttention.forward-378"><span class="linenos">378</span></a>        <span class="n">cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="RelPositionMultiHeadedAttention.forward-379"><a href="#RelPositionMultiHeadedAttention.forward-379"><span class="linenos">379</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="RelPositionMultiHeadedAttention.forward-380"><a href="#RelPositionMultiHeadedAttention.forward-380"><span class="linenos">380</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute &#39;Scaled Dot Product Attention&#39; with rel. positional encoding.</span>
</span><span id="RelPositionMultiHeadedAttention.forward-381"><a href="#RelPositionMultiHeadedAttention.forward-381"><span class="linenos">381</span></a><span class="sd">        Args:</span>
</span><span id="RelPositionMultiHeadedAttention.forward-382"><a href="#RelPositionMultiHeadedAttention.forward-382"><span class="linenos">382</span></a><span class="sd">            query (torch.Tensor): Query tensor (#batch, time1, size).</span>
</span><span id="RelPositionMultiHeadedAttention.forward-383"><a href="#RelPositionMultiHeadedAttention.forward-383"><span class="linenos">383</span></a><span class="sd">            key (torch.Tensor): Key tensor (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention.forward-384"><a href="#RelPositionMultiHeadedAttention.forward-384"><span class="linenos">384</span></a><span class="sd">            value (torch.Tensor): Value tensor (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention.forward-385"><a href="#RelPositionMultiHeadedAttention.forward-385"><span class="linenos">385</span></a><span class="sd">            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or</span>
</span><span id="RelPositionMultiHeadedAttention.forward-386"><a href="#RelPositionMultiHeadedAttention.forward-386"><span class="linenos">386</span></a><span class="sd">                (#batch, time1, time2), (0, 0, 0) means fake mask.</span>
</span><span id="RelPositionMultiHeadedAttention.forward-387"><a href="#RelPositionMultiHeadedAttention.forward-387"><span class="linenos">387</span></a><span class="sd">            pos_emb (torch.Tensor): Positional embedding tensor</span>
</span><span id="RelPositionMultiHeadedAttention.forward-388"><a href="#RelPositionMultiHeadedAttention.forward-388"><span class="linenos">388</span></a><span class="sd">                (#batch, time2, size).</span>
</span><span id="RelPositionMultiHeadedAttention.forward-389"><a href="#RelPositionMultiHeadedAttention.forward-389"><span class="linenos">389</span></a><span class="sd">            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),</span>
</span><span id="RelPositionMultiHeadedAttention.forward-390"><a href="#RelPositionMultiHeadedAttention.forward-390"><span class="linenos">390</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="RelPositionMultiHeadedAttention.forward-391"><a href="#RelPositionMultiHeadedAttention.forward-391"><span class="linenos">391</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="RelPositionMultiHeadedAttention.forward-392"><a href="#RelPositionMultiHeadedAttention.forward-392"><span class="linenos">392</span></a><span class="sd">        Returns:</span>
</span><span id="RelPositionMultiHeadedAttention.forward-393"><a href="#RelPositionMultiHeadedAttention.forward-393"><span class="linenos">393</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time1, d_model).</span>
</span><span id="RelPositionMultiHeadedAttention.forward-394"><a href="#RelPositionMultiHeadedAttention.forward-394"><span class="linenos">394</span></a><span class="sd">            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-395"><a href="#RelPositionMultiHeadedAttention.forward-395"><span class="linenos">395</span></a><span class="sd">                where `cache_t == chunk_size * num_decoding_left_chunks`</span>
</span><span id="RelPositionMultiHeadedAttention.forward-396"><a href="#RelPositionMultiHeadedAttention.forward-396"><span class="linenos">396</span></a><span class="sd">                and `head * d_k == size`</span>
</span><span id="RelPositionMultiHeadedAttention.forward-397"><a href="#RelPositionMultiHeadedAttention.forward-397"><span class="linenos">397</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RelPositionMultiHeadedAttention.forward-398"><a href="#RelPositionMultiHeadedAttention.forward-398"><span class="linenos">398</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_qkv</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-399"><a href="#RelPositionMultiHeadedAttention.forward-399"><span class="linenos">399</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, time1, head, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-400"><a href="#RelPositionMultiHeadedAttention.forward-400"><span class="linenos">400</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-401"><a href="#RelPositionMultiHeadedAttention.forward-401"><span class="linenos">401</span></a>        <span class="k">if</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention.forward-402"><a href="#RelPositionMultiHeadedAttention.forward-402"><span class="linenos">402</span></a>            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-403"><a href="#RelPositionMultiHeadedAttention.forward-403"><span class="linenos">403</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-404"><a href="#RelPositionMultiHeadedAttention.forward-404"><span class="linenos">404</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-405"><a href="#RelPositionMultiHeadedAttention.forward-405"><span class="linenos">405</span></a>        <span class="c1"># NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it&#39;s</span>
</span><span id="RelPositionMultiHeadedAttention.forward-406"><a href="#RelPositionMultiHeadedAttention.forward-406"><span class="linenos">406</span></a>        <span class="c1">#   non-trivial to calculate `next_cache_start` here.</span>
</span><span id="RelPositionMultiHeadedAttention.forward-407"><a href="#RelPositionMultiHeadedAttention.forward-407"><span class="linenos">407</span></a>        <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-408"><a href="#RelPositionMultiHeadedAttention.forward-408"><span class="linenos">408</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-409"><a href="#RelPositionMultiHeadedAttention.forward-409"><span class="linenos">409</span></a>        <span class="n">n_batch_pos</span> <span class="o">=</span> <span class="n">pos_emb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-410"><a href="#RelPositionMultiHeadedAttention.forward-410"><span class="linenos">410</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-411"><a href="#RelPositionMultiHeadedAttention.forward-411"><span class="linenos">411</span></a>        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-412"><a href="#RelPositionMultiHeadedAttention.forward-412"><span class="linenos">412</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-413"><a href="#RelPositionMultiHeadedAttention.forward-413"><span class="linenos">413</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-414"><a href="#RelPositionMultiHeadedAttention.forward-414"><span class="linenos">414</span></a>        <span class="n">q_with_bias_u</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-415"><a href="#RelPositionMultiHeadedAttention.forward-415"><span class="linenos">415</span></a>        <span class="c1"># (batch, head, time1, d_k)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-416"><a href="#RelPositionMultiHeadedAttention.forward-416"><span class="linenos">416</span></a>        <span class="n">q_with_bias_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-417"><a href="#RelPositionMultiHeadedAttention.forward-417"><span class="linenos">417</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-418"><a href="#RelPositionMultiHeadedAttention.forward-418"><span class="linenos">418</span></a>        <span class="c1"># compute attention score</span>
</span><span id="RelPositionMultiHeadedAttention.forward-419"><a href="#RelPositionMultiHeadedAttention.forward-419"><span class="linenos">419</span></a>        <span class="c1"># first compute matrix a and matrix c</span>
</span><span id="RelPositionMultiHeadedAttention.forward-420"><a href="#RelPositionMultiHeadedAttention.forward-420"><span class="linenos">420</span></a>        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
</span><span id="RelPositionMultiHeadedAttention.forward-421"><a href="#RelPositionMultiHeadedAttention.forward-421"><span class="linenos">421</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-422"><a href="#RelPositionMultiHeadedAttention.forward-422"><span class="linenos">422</span></a>        <span class="n">matrix_ac</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_u</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention.forward-423"><a href="#RelPositionMultiHeadedAttention.forward-423"><span class="linenos">423</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-424"><a href="#RelPositionMultiHeadedAttention.forward-424"><span class="linenos">424</span></a>        <span class="c1"># compute matrix b and matrix d</span>
</span><span id="RelPositionMultiHeadedAttention.forward-425"><a href="#RelPositionMultiHeadedAttention.forward-425"><span class="linenos">425</span></a>        <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-426"><a href="#RelPositionMultiHeadedAttention.forward-426"><span class="linenos">426</span></a>        <span class="n">matrix_bd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_v</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="RelPositionMultiHeadedAttention.forward-427"><a href="#RelPositionMultiHeadedAttention.forward-427"><span class="linenos">427</span></a>        <span class="c1"># NOTE(Xiang Lyu): Keep rel_shift since espnet rel_pos_emb is used</span>
</span><span id="RelPositionMultiHeadedAttention.forward-428"><a href="#RelPositionMultiHeadedAttention.forward-428"><span class="linenos">428</span></a>        <span class="k">if</span> <span class="n">matrix_ac</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">matrix_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="RelPositionMultiHeadedAttention.forward-429"><a href="#RelPositionMultiHeadedAttention.forward-429"><span class="linenos">429</span></a>            <span class="n">matrix_bd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rel_shift</span><span class="p">(</span><span class="n">matrix_bd</span><span class="p">)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-430"><a href="#RelPositionMultiHeadedAttention.forward-430"><span class="linenos">430</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-431"><a href="#RelPositionMultiHeadedAttention.forward-431"><span class="linenos">431</span></a>        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix_ac</span> <span class="o">+</span> <span class="n">matrix_bd</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
</span><span id="RelPositionMultiHeadedAttention.forward-432"><a href="#RelPositionMultiHeadedAttention.forward-432"><span class="linenos">432</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
</span><span id="RelPositionMultiHeadedAttention.forward-433"><a href="#RelPositionMultiHeadedAttention.forward-433"><span class="linenos">433</span></a>        <span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</span><span id="RelPositionMultiHeadedAttention.forward-434"><a href="#RelPositionMultiHeadedAttention.forward-434"><span class="linenos">434</span></a>
</span><span id="RelPositionMultiHeadedAttention.forward-435"><a href="#RelPositionMultiHeadedAttention.forward-435"><span class="linenos">435</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_attention</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">new_cache</span>
</span></pre></div>


            <div class="docstring"><p>Compute 'Scaled Dot Product Attention' with rel. positional encoding.
Args:
    query (torch.Tensor): Query tensor (#batch, time1, size).
    key (torch.Tensor): Key tensor (#batch, time2, size).
    value (torch.Tensor): Value tensor (#batch, time2, size).
    mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
        (#batch, time1, time2), (0, 0, 0) means fake mask.
    pos_emb (torch.Tensor): Positional embedding tensor
        (#batch, time2, size).
    cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),
        where <code>cache_t == chunk_size * num_decoding_left_chunks</code>
        and <code>head * d_k == size</code>
Returns:
    torch.Tensor: Output tensor (#batch, time1, d_model).
    torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)
        where <code>cache_t == chunk_size * num_decoding_left_chunks</code>
        and <code>head * d_k == size</code></p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#MultiHeadedAttention">MultiHeadedAttention</a></dt>
                                <dd id="RelPositionMultiHeadedAttention.d_k" class="variable"><a href="#MultiHeadedAttention.d_k">d_k</a></dd>
                <dd id="RelPositionMultiHeadedAttention.h" class="variable"><a href="#MultiHeadedAttention.h">h</a></dd>
                <dd id="RelPositionMultiHeadedAttention.linear_q" class="variable"><a href="#MultiHeadedAttention.linear_q">linear_q</a></dd>
                <dd id="RelPositionMultiHeadedAttention.linear_k" class="variable"><a href="#MultiHeadedAttention.linear_k">linear_k</a></dd>
                <dd id="RelPositionMultiHeadedAttention.linear_v" class="variable"><a href="#MultiHeadedAttention.linear_v">linear_v</a></dd>
                <dd id="RelPositionMultiHeadedAttention.linear_out" class="variable"><a href="#MultiHeadedAttention.linear_out">linear_out</a></dd>
                <dd id="RelPositionMultiHeadedAttention.dropout" class="variable"><a href="#MultiHeadedAttention.dropout">dropout</a></dd>
                <dd id="RelPositionMultiHeadedAttention.forward_qkv" class="function"><a href="#MultiHeadedAttention.forward_qkv">forward_qkv</a></dd>
                <dd id="RelPositionMultiHeadedAttention.forward_attention" class="function"><a href="#MultiHeadedAttention.forward_attention">forward_attention</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="subsequent_mask">
                            <input id="subsequent_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">subsequent_mask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">size</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="subsequent_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#subsequent_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="subsequent_mask-438"><a href="#subsequent_mask-438"><span class="linenos">438</span></a><span class="k">def</span><span class="w"> </span><span class="nf">subsequent_mask</span><span class="p">(</span>
</span><span id="subsequent_mask-439"><a href="#subsequent_mask-439"><span class="linenos">439</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="subsequent_mask-440"><a href="#subsequent_mask-440"><span class="linenos">440</span></a>    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
</span><span id="subsequent_mask-441"><a href="#subsequent_mask-441"><span class="linenos">441</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="subsequent_mask-442"><a href="#subsequent_mask-442"><span class="linenos">442</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create mask for subsequent steps (size, size).</span>
</span><span id="subsequent_mask-443"><a href="#subsequent_mask-443"><span class="linenos">443</span></a>
</span><span id="subsequent_mask-444"><a href="#subsequent_mask-444"><span class="linenos">444</span></a><span class="sd">    This mask is used only in decoder which works in an auto-regressive mode.</span>
</span><span id="subsequent_mask-445"><a href="#subsequent_mask-445"><span class="linenos">445</span></a><span class="sd">    This means the current step could only do attention with its left steps.</span>
</span><span id="subsequent_mask-446"><a href="#subsequent_mask-446"><span class="linenos">446</span></a>
</span><span id="subsequent_mask-447"><a href="#subsequent_mask-447"><span class="linenos">447</span></a><span class="sd">    In encoder, fully attention is used when streaming is not necessary and</span>
</span><span id="subsequent_mask-448"><a href="#subsequent_mask-448"><span class="linenos">448</span></a><span class="sd">    the sequence is not long. In this  case, no attention mask is needed.</span>
</span><span id="subsequent_mask-449"><a href="#subsequent_mask-449"><span class="linenos">449</span></a>
</span><span id="subsequent_mask-450"><a href="#subsequent_mask-450"><span class="linenos">450</span></a><span class="sd">    When streaming is need, chunk-based attention is used in encoder. See</span>
</span><span id="subsequent_mask-451"><a href="#subsequent_mask-451"><span class="linenos">451</span></a><span class="sd">    subsequent_chunk_mask for the chunk-based attention mask.</span>
</span><span id="subsequent_mask-452"><a href="#subsequent_mask-452"><span class="linenos">452</span></a>
</span><span id="subsequent_mask-453"><a href="#subsequent_mask-453"><span class="linenos">453</span></a><span class="sd">    Args:</span>
</span><span id="subsequent_mask-454"><a href="#subsequent_mask-454"><span class="linenos">454</span></a><span class="sd">        size (int): size of mask</span>
</span><span id="subsequent_mask-455"><a href="#subsequent_mask-455"><span class="linenos">455</span></a><span class="sd">        str device (str): &quot;cpu&quot; or &quot;cuda&quot; or torch.Tensor.device</span>
</span><span id="subsequent_mask-456"><a href="#subsequent_mask-456"><span class="linenos">456</span></a><span class="sd">        dtype (torch.device): result dtype</span>
</span><span id="subsequent_mask-457"><a href="#subsequent_mask-457"><span class="linenos">457</span></a>
</span><span id="subsequent_mask-458"><a href="#subsequent_mask-458"><span class="linenos">458</span></a><span class="sd">    Returns:</span>
</span><span id="subsequent_mask-459"><a href="#subsequent_mask-459"><span class="linenos">459</span></a><span class="sd">        torch.Tensor: mask</span>
</span><span id="subsequent_mask-460"><a href="#subsequent_mask-460"><span class="linenos">460</span></a>
</span><span id="subsequent_mask-461"><a href="#subsequent_mask-461"><span class="linenos">461</span></a><span class="sd">    Examples:</span>
</span><span id="subsequent_mask-462"><a href="#subsequent_mask-462"><span class="linenos">462</span></a><span class="sd">        &gt;&gt;&gt; subsequent_mask(3)</span>
</span><span id="subsequent_mask-463"><a href="#subsequent_mask-463"><span class="linenos">463</span></a><span class="sd">        [[1, 0, 0],</span>
</span><span id="subsequent_mask-464"><a href="#subsequent_mask-464"><span class="linenos">464</span></a><span class="sd">         [1, 1, 0],</span>
</span><span id="subsequent_mask-465"><a href="#subsequent_mask-465"><span class="linenos">465</span></a><span class="sd">         [1, 1, 1]]</span>
</span><span id="subsequent_mask-466"><a href="#subsequent_mask-466"><span class="linenos">466</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="subsequent_mask-467"><a href="#subsequent_mask-467"><span class="linenos">467</span></a>    <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="subsequent_mask-468"><a href="#subsequent_mask-468"><span class="linenos">468</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="subsequent_mask-469"><a href="#subsequent_mask-469"><span class="linenos">469</span></a>    <span class="n">arange</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="subsequent_mask-470"><a href="#subsequent_mask-470"><span class="linenos">470</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">&lt;=</span> <span class="n">arange</span>
</span><span id="subsequent_mask-471"><a href="#subsequent_mask-471"><span class="linenos">471</span></a>    <span class="k">return</span> <span class="n">mask</span>
</span></pre></div>


            <div class="docstring"><p>Create mask for subsequent steps (size, size).</p>

<p>This mask is used only in decoder which works in an auto-regressive mode.
This means the current step could only do attention with its left steps.</p>

<p>In encoder, fully attention is used when streaming is not necessary and
the sequence is not long. In this  case, no attention mask is needed.</p>

<p>When streaming is need, chunk-based attention is used in encoder. See
subsequent_chunk_mask for the chunk-based attention mask.</p>

<p>Args:
    size (int): size of mask
    str device (str): "cpu" or "cuda" or torch.Tensor.device
    dtype (torch.device): result dtype</p>

<p>Returns:
    torch.Tensor: mask</p>

<p>Examples:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>subsequent_mask(3)
          [[1, 0, 0],
           [1, 1, 0],
           [1, 1, 1]]</p>
    </blockquote>
  </blockquote>
</blockquote>
</div>


                </section>
                <section id="subsequent_chunk_mask">
                            <input id="subsequent_chunk_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">subsequent_chunk_mask</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>,</span><span class="param">	<span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="subsequent_chunk_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#subsequent_chunk_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="subsequent_chunk_mask-474"><a href="#subsequent_chunk_mask-474"><span class="linenos">474</span></a><span class="k">def</span><span class="w"> </span><span class="nf">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="subsequent_chunk_mask-475"><a href="#subsequent_chunk_mask-475"><span class="linenos">475</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="subsequent_chunk_mask-476"><a href="#subsequent_chunk_mask-476"><span class="linenos">476</span></a>    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="subsequent_chunk_mask-477"><a href="#subsequent_chunk_mask-477"><span class="linenos">477</span></a>    <span class="n">num_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="subsequent_chunk_mask-478"><a href="#subsequent_chunk_mask-478"><span class="linenos">478</span></a>    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
</span><span id="subsequent_chunk_mask-479"><a href="#subsequent_chunk_mask-479"><span class="linenos">479</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="subsequent_chunk_mask-480"><a href="#subsequent_chunk_mask-480"><span class="linenos">480</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create mask for subsequent steps (size, size) with chunk size,</span>
</span><span id="subsequent_chunk_mask-481"><a href="#subsequent_chunk_mask-481"><span class="linenos">481</span></a><span class="sd">       this is for streaming encoder</span>
</span><span id="subsequent_chunk_mask-482"><a href="#subsequent_chunk_mask-482"><span class="linenos">482</span></a>
</span><span id="subsequent_chunk_mask-483"><a href="#subsequent_chunk_mask-483"><span class="linenos">483</span></a><span class="sd">    Args:</span>
</span><span id="subsequent_chunk_mask-484"><a href="#subsequent_chunk_mask-484"><span class="linenos">484</span></a><span class="sd">        size (int): size of mask</span>
</span><span id="subsequent_chunk_mask-485"><a href="#subsequent_chunk_mask-485"><span class="linenos">485</span></a><span class="sd">        chunk_size (int): size of chunk</span>
</span><span id="subsequent_chunk_mask-486"><a href="#subsequent_chunk_mask-486"><span class="linenos">486</span></a><span class="sd">        num_left_chunks (int): number of left chunks</span>
</span><span id="subsequent_chunk_mask-487"><a href="#subsequent_chunk_mask-487"><span class="linenos">487</span></a><span class="sd">            &lt;0: use full chunk</span>
</span><span id="subsequent_chunk_mask-488"><a href="#subsequent_chunk_mask-488"><span class="linenos">488</span></a><span class="sd">            &gt;=0: use num_left_chunks</span>
</span><span id="subsequent_chunk_mask-489"><a href="#subsequent_chunk_mask-489"><span class="linenos">489</span></a><span class="sd">        device (torch.device): &quot;cpu&quot; or &quot;cuda&quot; or torch.Tensor.device</span>
</span><span id="subsequent_chunk_mask-490"><a href="#subsequent_chunk_mask-490"><span class="linenos">490</span></a>
</span><span id="subsequent_chunk_mask-491"><a href="#subsequent_chunk_mask-491"><span class="linenos">491</span></a><span class="sd">    Returns:</span>
</span><span id="subsequent_chunk_mask-492"><a href="#subsequent_chunk_mask-492"><span class="linenos">492</span></a><span class="sd">        torch.Tensor: mask</span>
</span><span id="subsequent_chunk_mask-493"><a href="#subsequent_chunk_mask-493"><span class="linenos">493</span></a>
</span><span id="subsequent_chunk_mask-494"><a href="#subsequent_chunk_mask-494"><span class="linenos">494</span></a><span class="sd">    Examples:</span>
</span><span id="subsequent_chunk_mask-495"><a href="#subsequent_chunk_mask-495"><span class="linenos">495</span></a><span class="sd">        &gt;&gt;&gt; subsequent_chunk_mask(4, 2)</span>
</span><span id="subsequent_chunk_mask-496"><a href="#subsequent_chunk_mask-496"><span class="linenos">496</span></a><span class="sd">        [[1, 1, 0, 0],</span>
</span><span id="subsequent_chunk_mask-497"><a href="#subsequent_chunk_mask-497"><span class="linenos">497</span></a><span class="sd">         [1, 1, 0, 0],</span>
</span><span id="subsequent_chunk_mask-498"><a href="#subsequent_chunk_mask-498"><span class="linenos">498</span></a><span class="sd">         [1, 1, 1, 1],</span>
</span><span id="subsequent_chunk_mask-499"><a href="#subsequent_chunk_mask-499"><span class="linenos">499</span></a><span class="sd">         [1, 1, 1, 1]]</span>
</span><span id="subsequent_chunk_mask-500"><a href="#subsequent_chunk_mask-500"><span class="linenos">500</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="subsequent_chunk_mask-501"><a href="#subsequent_chunk_mask-501"><span class="linenos">501</span></a>    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span id="subsequent_chunk_mask-502"><a href="#subsequent_chunk_mask-502"><span class="linenos">502</span></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span><span id="subsequent_chunk_mask-503"><a href="#subsequent_chunk_mask-503"><span class="linenos">503</span></a>        <span class="k">if</span> <span class="n">num_left_chunks</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="subsequent_chunk_mask-504"><a href="#subsequent_chunk_mask-504"><span class="linenos">504</span></a>            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="subsequent_chunk_mask-505"><a href="#subsequent_chunk_mask-505"><span class="linenos">505</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="subsequent_chunk_mask-506"><a href="#subsequent_chunk_mask-506"><span class="linenos">506</span></a>            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="n">num_left_chunks</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="subsequent_chunk_mask-507"><a href="#subsequent_chunk_mask-507"><span class="linenos">507</span></a>        <span class="n">ending</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="n">chunk_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="subsequent_chunk_mask-508"><a href="#subsequent_chunk_mask-508"><span class="linenos">508</span></a>        <span class="n">ret</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">ending</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="subsequent_chunk_mask-509"><a href="#subsequent_chunk_mask-509"><span class="linenos">509</span></a>    <span class="k">return</span> <span class="n">ret</span>
</span></pre></div>


            <div class="docstring"><p>Create mask for subsequent steps (size, size) with chunk size,
   this is for streaming encoder</p>

<p>Args:
    size (int): size of mask
    chunk_size (int): size of chunk
    num_left_chunks (int): number of left chunks
        &lt;0: use full chunk</p>

<blockquote>
  <p>=0: use num_left_chunks
      device (torch.device): "cpu" or "cuda" or torch.Tensor.device</p>
</blockquote>

<p>Returns:
    torch.Tensor: mask</p>

<p>Examples:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>subsequent_chunk_mask(4, 2)
          [[1, 1, 0, 0],
           [1, 1, 0, 0],
           [1, 1, 1, 1],
           [1, 1, 1, 1]]</p>
    </blockquote>
  </blockquote>
</blockquote>
</div>


                </section>
                <section id="add_optional_chunk_mask">
                            <input id="add_optional_chunk_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">add_optional_chunk_mask</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">enable_full_context</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="add_optional_chunk_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#add_optional_chunk_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="add_optional_chunk_mask-512"><a href="#add_optional_chunk_mask-512"><span class="linenos">512</span></a><span class="k">def</span><span class="w"> </span><span class="nf">add_optional_chunk_mask</span><span class="p">(</span>
</span><span id="add_optional_chunk_mask-513"><a href="#add_optional_chunk_mask-513"><span class="linenos">513</span></a>    <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-514"><a href="#add_optional_chunk_mask-514"><span class="linenos">514</span></a>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-515"><a href="#add_optional_chunk_mask-515"><span class="linenos">515</span></a>    <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-516"><a href="#add_optional_chunk_mask-516"><span class="linenos">516</span></a>    <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-517"><a href="#add_optional_chunk_mask-517"><span class="linenos">517</span></a>    <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-518"><a href="#add_optional_chunk_mask-518"><span class="linenos">518</span></a>    <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-519"><a href="#add_optional_chunk_mask-519"><span class="linenos">519</span></a>    <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-520"><a href="#add_optional_chunk_mask-520"><span class="linenos">520</span></a>    <span class="n">enable_full_context</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="add_optional_chunk_mask-521"><a href="#add_optional_chunk_mask-521"><span class="linenos">521</span></a><span class="p">):</span>
</span><span id="add_optional_chunk_mask-522"><a href="#add_optional_chunk_mask-522"><span class="linenos">522</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply optional mask for encoder.</span>
</span><span id="add_optional_chunk_mask-523"><a href="#add_optional_chunk_mask-523"><span class="linenos">523</span></a>
</span><span id="add_optional_chunk_mask-524"><a href="#add_optional_chunk_mask-524"><span class="linenos">524</span></a><span class="sd">    Args:</span>
</span><span id="add_optional_chunk_mask-525"><a href="#add_optional_chunk_mask-525"><span class="linenos">525</span></a><span class="sd">        xs (torch.Tensor): padded input, (B, L, D), L for max length</span>
</span><span id="add_optional_chunk_mask-526"><a href="#add_optional_chunk_mask-526"><span class="linenos">526</span></a><span class="sd">        mask (torch.Tensor): mask for xs, (B, 1, L)</span>
</span><span id="add_optional_chunk_mask-527"><a href="#add_optional_chunk_mask-527"><span class="linenos">527</span></a><span class="sd">        use_dynamic_chunk (bool): whether to use dynamic chunk or not</span>
</span><span id="add_optional_chunk_mask-528"><a href="#add_optional_chunk_mask-528"><span class="linenos">528</span></a><span class="sd">        use_dynamic_left_chunk (bool): whether to use dynamic left chunk for</span>
</span><span id="add_optional_chunk_mask-529"><a href="#add_optional_chunk_mask-529"><span class="linenos">529</span></a><span class="sd">            training.</span>
</span><span id="add_optional_chunk_mask-530"><a href="#add_optional_chunk_mask-530"><span class="linenos">530</span></a><span class="sd">        decoding_chunk_size (int): decoding chunk size for dynamic chunk, it&#39;s</span>
</span><span id="add_optional_chunk_mask-531"><a href="#add_optional_chunk_mask-531"><span class="linenos">531</span></a><span class="sd">            0: default for training, use random dynamic chunk.</span>
</span><span id="add_optional_chunk_mask-532"><a href="#add_optional_chunk_mask-532"><span class="linenos">532</span></a><span class="sd">            &lt;0: for decoding, use full chunk.</span>
</span><span id="add_optional_chunk_mask-533"><a href="#add_optional_chunk_mask-533"><span class="linenos">533</span></a><span class="sd">            &gt;0: for decoding, use fixed chunk size as set.</span>
</span><span id="add_optional_chunk_mask-534"><a href="#add_optional_chunk_mask-534"><span class="linenos">534</span></a><span class="sd">        static_chunk_size (int): chunk size for static chunk training/decoding</span>
</span><span id="add_optional_chunk_mask-535"><a href="#add_optional_chunk_mask-535"><span class="linenos">535</span></a><span class="sd">            if it&#39;s greater than 0, if use_dynamic_chunk is true,</span>
</span><span id="add_optional_chunk_mask-536"><a href="#add_optional_chunk_mask-536"><span class="linenos">536</span></a><span class="sd">            this parameter will be ignored</span>
</span><span id="add_optional_chunk_mask-537"><a href="#add_optional_chunk_mask-537"><span class="linenos">537</span></a><span class="sd">        num_decoding_left_chunks: number of left chunks, this is for decoding,</span>
</span><span id="add_optional_chunk_mask-538"><a href="#add_optional_chunk_mask-538"><span class="linenos">538</span></a><span class="sd">            the chunk size is decoding_chunk_size.</span>
</span><span id="add_optional_chunk_mask-539"><a href="#add_optional_chunk_mask-539"><span class="linenos">539</span></a><span class="sd">            &gt;=0: use num_decoding_left_chunks</span>
</span><span id="add_optional_chunk_mask-540"><a href="#add_optional_chunk_mask-540"><span class="linenos">540</span></a><span class="sd">            &lt;0: use all left chunks</span>
</span><span id="add_optional_chunk_mask-541"><a href="#add_optional_chunk_mask-541"><span class="linenos">541</span></a><span class="sd">        enable_full_context (bool):</span>
</span><span id="add_optional_chunk_mask-542"><a href="#add_optional_chunk_mask-542"><span class="linenos">542</span></a><span class="sd">            True: chunk size is either [1, 25] or full context(max_len)</span>
</span><span id="add_optional_chunk_mask-543"><a href="#add_optional_chunk_mask-543"><span class="linenos">543</span></a><span class="sd">            False: chunk size ~ U[1, 25]</span>
</span><span id="add_optional_chunk_mask-544"><a href="#add_optional_chunk_mask-544"><span class="linenos">544</span></a>
</span><span id="add_optional_chunk_mask-545"><a href="#add_optional_chunk_mask-545"><span class="linenos">545</span></a><span class="sd">    Returns:</span>
</span><span id="add_optional_chunk_mask-546"><a href="#add_optional_chunk_mask-546"><span class="linenos">546</span></a><span class="sd">        torch.Tensor: chunk mask of the input xs.</span>
</span><span id="add_optional_chunk_mask-547"><a href="#add_optional_chunk_mask-547"><span class="linenos">547</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="add_optional_chunk_mask-548"><a href="#add_optional_chunk_mask-548"><span class="linenos">548</span></a>    <span class="c1"># Whether to use chunk mask or not</span>
</span><span id="add_optional_chunk_mask-549"><a href="#add_optional_chunk_mask-549"><span class="linenos">549</span></a>    <span class="k">if</span> <span class="n">use_dynamic_chunk</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-550"><a href="#add_optional_chunk_mask-550"><span class="linenos">550</span></a>        <span class="n">max_len</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="add_optional_chunk_mask-551"><a href="#add_optional_chunk_mask-551"><span class="linenos">551</span></a>        <span class="k">if</span> <span class="n">decoding_chunk_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-552"><a href="#add_optional_chunk_mask-552"><span class="linenos">552</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">max_len</span>
</span><span id="add_optional_chunk_mask-553"><a href="#add_optional_chunk_mask-553"><span class="linenos">553</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="add_optional_chunk_mask-554"><a href="#add_optional_chunk_mask-554"><span class="linenos">554</span></a>        <span class="k">elif</span> <span class="n">decoding_chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-555"><a href="#add_optional_chunk_mask-555"><span class="linenos">555</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">decoding_chunk_size</span>
</span><span id="add_optional_chunk_mask-556"><a href="#add_optional_chunk_mask-556"><span class="linenos">556</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">num_decoding_left_chunks</span>
</span><span id="add_optional_chunk_mask-557"><a href="#add_optional_chunk_mask-557"><span class="linenos">557</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-558"><a href="#add_optional_chunk_mask-558"><span class="linenos">558</span></a>            <span class="c1"># chunk size is either [1, 25] or full context(max_len).</span>
</span><span id="add_optional_chunk_mask-559"><a href="#add_optional_chunk_mask-559"><span class="linenos">559</span></a>            <span class="c1"># Since we use 4 times subsampling and allow up to 1s(100 frames)</span>
</span><span id="add_optional_chunk_mask-560"><a href="#add_optional_chunk_mask-560"><span class="linenos">560</span></a>            <span class="c1"># delay, the maximum frame is 100 / 4 = 25.</span>
</span><span id="add_optional_chunk_mask-561"><a href="#add_optional_chunk_mask-561"><span class="linenos">561</span></a>            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="add_optional_chunk_mask-562"><a href="#add_optional_chunk_mask-562"><span class="linenos">562</span></a>            <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="add_optional_chunk_mask-563"><a href="#add_optional_chunk_mask-563"><span class="linenos">563</span></a>            <span class="k">if</span> <span class="n">chunk_size</span> <span class="o">&gt;</span> <span class="n">max_len</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">enable_full_context</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-564"><a href="#add_optional_chunk_mask-564"><span class="linenos">564</span></a>                <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">max_len</span>
</span><span id="add_optional_chunk_mask-565"><a href="#add_optional_chunk_mask-565"><span class="linenos">565</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-566"><a href="#add_optional_chunk_mask-566"><span class="linenos">566</span></a>                <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">chunk_size</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="add_optional_chunk_mask-567"><a href="#add_optional_chunk_mask-567"><span class="linenos">567</span></a>                <span class="k">if</span> <span class="n">use_dynamic_left_chunk</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-568"><a href="#add_optional_chunk_mask-568"><span class="linenos">568</span></a>                    <span class="n">max_left_chunks</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">chunk_size</span>
</span><span id="add_optional_chunk_mask-569"><a href="#add_optional_chunk_mask-569"><span class="linenos">569</span></a>                    <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_left_chunks</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="add_optional_chunk_mask-570"><a href="#add_optional_chunk_mask-570"><span class="linenos">570</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="add_optional_chunk_mask-571"><a href="#add_optional_chunk_mask-571"><span class="linenos">571</span></a>            <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">num_left_chunks</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">device</span>
</span><span id="add_optional_chunk_mask-572"><a href="#add_optional_chunk_mask-572"><span class="linenos">572</span></a>        <span class="p">)</span>  <span class="c1"># (L, L)</span>
</span><span id="add_optional_chunk_mask-573"><a href="#add_optional_chunk_mask-573"><span class="linenos">573</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">chunk_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
</span><span id="add_optional_chunk_mask-574"><a href="#add_optional_chunk_mask-574"><span class="linenos">574</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&amp;</span> <span class="n">chunk_masks</span>  <span class="c1"># (B, L, L)</span>
</span><span id="add_optional_chunk_mask-575"><a href="#add_optional_chunk_mask-575"><span class="linenos">575</span></a>    <span class="k">elif</span> <span class="n">static_chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-576"><a href="#add_optional_chunk_mask-576"><span class="linenos">576</span></a>        <span class="n">num_left_chunks</span> <span class="o">=</span> <span class="n">num_decoding_left_chunks</span>
</span><span id="add_optional_chunk_mask-577"><a href="#add_optional_chunk_mask-577"><span class="linenos">577</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">subsequent_chunk_mask</span><span class="p">(</span>
</span><span id="add_optional_chunk_mask-578"><a href="#add_optional_chunk_mask-578"><span class="linenos">578</span></a>            <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">static_chunk_size</span><span class="p">,</span> <span class="n">num_left_chunks</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">device</span>
</span><span id="add_optional_chunk_mask-579"><a href="#add_optional_chunk_mask-579"><span class="linenos">579</span></a>        <span class="p">)</span>  <span class="c1"># (L, L)</span>
</span><span id="add_optional_chunk_mask-580"><a href="#add_optional_chunk_mask-580"><span class="linenos">580</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">chunk_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
</span><span id="add_optional_chunk_mask-581"><a href="#add_optional_chunk_mask-581"><span class="linenos">581</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&amp;</span> <span class="n">chunk_masks</span>  <span class="c1"># (B, L, L)</span>
</span><span id="add_optional_chunk_mask-582"><a href="#add_optional_chunk_mask-582"><span class="linenos">582</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="add_optional_chunk_mask-583"><a href="#add_optional_chunk_mask-583"><span class="linenos">583</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">masks</span>
</span><span id="add_optional_chunk_mask-584"><a href="#add_optional_chunk_mask-584"><span class="linenos">584</span></a>    <span class="k">return</span> <span class="n">chunk_masks</span>
</span></pre></div>


            <div class="docstring"><p>Apply optional mask for encoder.</p>

<p>Args:
    xs (torch.Tensor): padded input, (B, L, D), L for max length
    mask (torch.Tensor): mask for xs, (B, 1, L)
    use_dynamic_chunk (bool): whether to use dynamic chunk or not
    use_dynamic_left_chunk (bool): whether to use dynamic left chunk for
        training.
    decoding_chunk_size (int): decoding chunk size for dynamic chunk, it's
        0: default for training, use random dynamic chunk.
        &lt;0: for decoding, use full chunk.</p>

<blockquote>
  <p>0: for decoding, use fixed chunk size as set.
      static_chunk_size (int): chunk size for static chunk training/decoding
          if it's greater than 0, if use_dynamic_chunk is true,
          this parameter will be ignored
      num_decoding_left_chunks: number of left chunks, this is for decoding,
          the chunk size is decoding_chunk_size.
  =0: use num_decoding_left_chunks
          &lt;0: use all left chunks
      enable_full_context (bool):
          True: chunk size is either [1, 25] or full context(max_len)
          False: chunk size ~ U[1, 25]</p>
</blockquote>

<p>Returns:
    torch.Tensor: chunk mask of the input xs.</p>
</div>


                </section>
                <section id="ConformerEncoderLayer">
                            <input id="ConformerEncoderLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ConformerEncoderLayer</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="ConformerEncoderLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoderLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoderLayer-587"><a href="#ConformerEncoderLayer-587"><span class="linenos">587</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="ConformerEncoderLayer-588"><a href="#ConformerEncoderLayer-588"><span class="linenos">588</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Encoder layer module.</span>
</span><span id="ConformerEncoderLayer-589"><a href="#ConformerEncoderLayer-589"><span class="linenos">589</span></a><span class="sd">    Args:</span>
</span><span id="ConformerEncoderLayer-590"><a href="#ConformerEncoderLayer-590"><span class="linenos">590</span></a><span class="sd">        size (int): Input dimension.</span>
</span><span id="ConformerEncoderLayer-591"><a href="#ConformerEncoderLayer-591"><span class="linenos">591</span></a><span class="sd">        self_attn (torch.nn.Module): Self-attention module instance.</span>
</span><span id="ConformerEncoderLayer-592"><a href="#ConformerEncoderLayer-592"><span class="linenos">592</span></a><span class="sd">            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention`</span>
</span><span id="ConformerEncoderLayer-593"><a href="#ConformerEncoderLayer-593"><span class="linenos">593</span></a><span class="sd">            instance can be used as the argument.</span>
</span><span id="ConformerEncoderLayer-594"><a href="#ConformerEncoderLayer-594"><span class="linenos">594</span></a><span class="sd">        feed_forward (torch.nn.Module): Feed-forward module instance.</span>
</span><span id="ConformerEncoderLayer-595"><a href="#ConformerEncoderLayer-595"><span class="linenos">595</span></a><span class="sd">            `PositionwiseFeedForward` instance can be used as the argument.</span>
</span><span id="ConformerEncoderLayer-596"><a href="#ConformerEncoderLayer-596"><span class="linenos">596</span></a><span class="sd">        feed_forward_macaron (torch.nn.Module): Additional feed-forward module</span>
</span><span id="ConformerEncoderLayer-597"><a href="#ConformerEncoderLayer-597"><span class="linenos">597</span></a><span class="sd">             instance.</span>
</span><span id="ConformerEncoderLayer-598"><a href="#ConformerEncoderLayer-598"><span class="linenos">598</span></a><span class="sd">            `PositionwiseFeedForward` instance can be used as the argument.</span>
</span><span id="ConformerEncoderLayer-599"><a href="#ConformerEncoderLayer-599"><span class="linenos">599</span></a><span class="sd">        conv_module (torch.nn.Module): Convolution module instance.</span>
</span><span id="ConformerEncoderLayer-600"><a href="#ConformerEncoderLayer-600"><span class="linenos">600</span></a><span class="sd">            `ConvlutionModule` instance can be used as the argument.</span>
</span><span id="ConformerEncoderLayer-601"><a href="#ConformerEncoderLayer-601"><span class="linenos">601</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="ConformerEncoderLayer-602"><a href="#ConformerEncoderLayer-602"><span class="linenos">602</span></a><span class="sd">        normalize_before (bool):</span>
</span><span id="ConformerEncoderLayer-603"><a href="#ConformerEncoderLayer-603"><span class="linenos">603</span></a><span class="sd">            True: use layer_norm before each sub-block.</span>
</span><span id="ConformerEncoderLayer-604"><a href="#ConformerEncoderLayer-604"><span class="linenos">604</span></a><span class="sd">            False: use layer_norm after each sub-block.</span>
</span><span id="ConformerEncoderLayer-605"><a href="#ConformerEncoderLayer-605"><span class="linenos">605</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ConformerEncoderLayer-606"><a href="#ConformerEncoderLayer-606"><span class="linenos">606</span></a>
</span><span id="ConformerEncoderLayer-607"><a href="#ConformerEncoderLayer-607"><span class="linenos">607</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer-608"><a href="#ConformerEncoderLayer-608"><span class="linenos">608</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-609"><a href="#ConformerEncoderLayer-609"><span class="linenos">609</span></a>        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-610"><a href="#ConformerEncoderLayer-610"><span class="linenos">610</span></a>        <span class="n">self_attn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-611"><a href="#ConformerEncoderLayer-611"><span class="linenos">611</span></a>        <span class="n">feed_forward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-612"><a href="#ConformerEncoderLayer-612"><span class="linenos">612</span></a>        <span class="n">feed_forward_macaron</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-613"><a href="#ConformerEncoderLayer-613"><span class="linenos">613</span></a>        <span class="n">conv_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-614"><a href="#ConformerEncoderLayer-614"><span class="linenos">614</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-615"><a href="#ConformerEncoderLayer-615"><span class="linenos">615</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-616"><a href="#ConformerEncoderLayer-616"><span class="linenos">616</span></a>    <span class="p">):</span>
</span><span id="ConformerEncoderLayer-617"><a href="#ConformerEncoderLayer-617"><span class="linenos">617</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an EncoderLayer object.&quot;&quot;&quot;</span>
</span><span id="ConformerEncoderLayer-618"><a href="#ConformerEncoderLayer-618"><span class="linenos">618</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConformerEncoderLayer-619"><a href="#ConformerEncoderLayer-619"><span class="linenos">619</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
</span><span id="ConformerEncoderLayer-620"><a href="#ConformerEncoderLayer-620"><span class="linenos">620</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
</span><span id="ConformerEncoderLayer-621"><a href="#ConformerEncoderLayer-621"><span class="linenos">621</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="o">=</span> <span class="n">feed_forward_macaron</span>
</span><span id="ConformerEncoderLayer-622"><a href="#ConformerEncoderLayer-622"><span class="linenos">622</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="o">=</span> <span class="n">conv_module</span>
</span><span id="ConformerEncoderLayer-623"><a href="#ConformerEncoderLayer-623"><span class="linenos">623</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the FNN module</span>
</span><span id="ConformerEncoderLayer-624"><a href="#ConformerEncoderLayer-624"><span class="linenos">624</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the MHA module</span>
</span><span id="ConformerEncoderLayer-625"><a href="#ConformerEncoderLayer-625"><span class="linenos">625</span></a>        <span class="k">if</span> <span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-626"><a href="#ConformerEncoderLayer-626"><span class="linenos">626</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-627"><a href="#ConformerEncoderLayer-627"><span class="linenos">627</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="ConformerEncoderLayer-628"><a href="#ConformerEncoderLayer-628"><span class="linenos">628</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-629"><a href="#ConformerEncoderLayer-629"><span class="linenos">629</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span id="ConformerEncoderLayer-630"><a href="#ConformerEncoderLayer-630"><span class="linenos">630</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-631"><a href="#ConformerEncoderLayer-631"><span class="linenos">631</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the CNN module</span>
</span><span id="ConformerEncoderLayer-632"><a href="#ConformerEncoderLayer-632"><span class="linenos">632</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer-633"><a href="#ConformerEncoderLayer-633"><span class="linenos">633</span></a>                <span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="ConformerEncoderLayer-634"><a href="#ConformerEncoderLayer-634"><span class="linenos">634</span></a>            <span class="p">)</span>  <span class="c1"># for the final output of the block</span>
</span><span id="ConformerEncoderLayer-635"><a href="#ConformerEncoderLayer-635"><span class="linenos">635</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-636"><a href="#ConformerEncoderLayer-636"><span class="linenos">636</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span><span id="ConformerEncoderLayer-637"><a href="#ConformerEncoderLayer-637"><span class="linenos">637</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span><span id="ConformerEncoderLayer-638"><a href="#ConformerEncoderLayer-638"><span class="linenos">638</span></a>
</span><span id="ConformerEncoderLayer-639"><a href="#ConformerEncoderLayer-639"><span class="linenos">639</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer-640"><a href="#ConformerEncoderLayer-640"><span class="linenos">640</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-641"><a href="#ConformerEncoderLayer-641"><span class="linenos">641</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-642"><a href="#ConformerEncoderLayer-642"><span class="linenos">642</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-643"><a href="#ConformerEncoderLayer-643"><span class="linenos">643</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer-644"><a href="#ConformerEncoderLayer-644"><span class="linenos">644</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="ConformerEncoderLayer-645"><a href="#ConformerEncoderLayer-645"><span class="linenos">645</span></a>        <span class="n">att_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConformerEncoderLayer-646"><a href="#ConformerEncoderLayer-646"><span class="linenos">646</span></a>        <span class="n">cnn_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConformerEncoderLayer-647"><a href="#ConformerEncoderLayer-647"><span class="linenos">647</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConformerEncoderLayer-648"><a href="#ConformerEncoderLayer-648"><span class="linenos">648</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute encoded features.</span>
</span><span id="ConformerEncoderLayer-649"><a href="#ConformerEncoderLayer-649"><span class="linenos">649</span></a>
</span><span id="ConformerEncoderLayer-650"><a href="#ConformerEncoderLayer-650"><span class="linenos">650</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoderLayer-651"><a href="#ConformerEncoderLayer-651"><span class="linenos">651</span></a><span class="sd">            x (torch.Tensor): (#batch, time, size)</span>
</span><span id="ConformerEncoderLayer-652"><a href="#ConformerEncoderLayer-652"><span class="linenos">652</span></a><span class="sd">            mask (torch.Tensor): Mask tensor for the input (#batch, timetime),</span>
</span><span id="ConformerEncoderLayer-653"><a href="#ConformerEncoderLayer-653"><span class="linenos">653</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="ConformerEncoderLayer-654"><a href="#ConformerEncoderLayer-654"><span class="linenos">654</span></a><span class="sd">            pos_emb (torch.Tensor): positional encoding, must not be None</span>
</span><span id="ConformerEncoderLayer-655"><a href="#ConformerEncoderLayer-655"><span class="linenos">655</span></a><span class="sd">                for ConformerEncoderLayer.</span>
</span><span id="ConformerEncoderLayer-656"><a href="#ConformerEncoderLayer-656"><span class="linenos">656</span></a><span class="sd">            mask_pad (torch.Tensor): batch padding mask used for conv module.</span>
</span><span id="ConformerEncoderLayer-657"><a href="#ConformerEncoderLayer-657"><span class="linenos">657</span></a><span class="sd">                (#batch, 1time), (0, 0, 0) means fake mask.</span>
</span><span id="ConformerEncoderLayer-658"><a href="#ConformerEncoderLayer-658"><span class="linenos">658</span></a><span class="sd">            att_cache (torch.Tensor): Cache tensor of the KEY &amp; VALUE</span>
</span><span id="ConformerEncoderLayer-659"><a href="#ConformerEncoderLayer-659"><span class="linenos">659</span></a><span class="sd">                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.</span>
</span><span id="ConformerEncoderLayer-660"><a href="#ConformerEncoderLayer-660"><span class="linenos">660</span></a><span class="sd">            cnn_cache (torch.Tensor): Convolution cache in conformer layer</span>
</span><span id="ConformerEncoderLayer-661"><a href="#ConformerEncoderLayer-661"><span class="linenos">661</span></a><span class="sd">                (#batch=1, size, cache_t2)</span>
</span><span id="ConformerEncoderLayer-662"><a href="#ConformerEncoderLayer-662"><span class="linenos">662</span></a><span class="sd">        Returns:</span>
</span><span id="ConformerEncoderLayer-663"><a href="#ConformerEncoderLayer-663"><span class="linenos">663</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, size).</span>
</span><span id="ConformerEncoderLayer-664"><a href="#ConformerEncoderLayer-664"><span class="linenos">664</span></a><span class="sd">            torch.Tensor: Mask tensor (#batch, time, time).</span>
</span><span id="ConformerEncoderLayer-665"><a href="#ConformerEncoderLayer-665"><span class="linenos">665</span></a><span class="sd">            torch.Tensor: att_cache tensor,</span>
</span><span id="ConformerEncoderLayer-666"><a href="#ConformerEncoderLayer-666"><span class="linenos">666</span></a><span class="sd">                (#batch=1, head, cache_t1 + time, d_k * 2).</span>
</span><span id="ConformerEncoderLayer-667"><a href="#ConformerEncoderLayer-667"><span class="linenos">667</span></a><span class="sd">            torch.Tensor: cnn_cahce tensor (#batch, size, cache_t2).</span>
</span><span id="ConformerEncoderLayer-668"><a href="#ConformerEncoderLayer-668"><span class="linenos">668</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoderLayer-669"><a href="#ConformerEncoderLayer-669"><span class="linenos">669</span></a>
</span><span id="ConformerEncoderLayer-670"><a href="#ConformerEncoderLayer-670"><span class="linenos">670</span></a>        <span class="c1"># whether to use macaron style</span>
</span><span id="ConformerEncoderLayer-671"><a href="#ConformerEncoderLayer-671"><span class="linenos">671</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-672"><a href="#ConformerEncoderLayer-672"><span class="linenos">672</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer-673"><a href="#ConformerEncoderLayer-673"><span class="linenos">673</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-674"><a href="#ConformerEncoderLayer-674"><span class="linenos">674</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-675"><a href="#ConformerEncoderLayer-675"><span class="linenos">675</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConformerEncoderLayer-676"><a href="#ConformerEncoderLayer-676"><span class="linenos">676</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-677"><a href="#ConformerEncoderLayer-677"><span class="linenos">677</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-678"><a href="#ConformerEncoderLayer-678"><span class="linenos">678</span></a>
</span><span id="ConformerEncoderLayer-679"><a href="#ConformerEncoderLayer-679"><span class="linenos">679</span></a>        <span class="c1"># multi-headed self-attention module</span>
</span><span id="ConformerEncoderLayer-680"><a href="#ConformerEncoderLayer-680"><span class="linenos">680</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer-681"><a href="#ConformerEncoderLayer-681"><span class="linenos">681</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-682"><a href="#ConformerEncoderLayer-682"><span class="linenos">682</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-683"><a href="#ConformerEncoderLayer-683"><span class="linenos">683</span></a>        <span class="n">x_att</span><span class="p">,</span> <span class="n">new_att_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">att_cache</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-684"><a href="#ConformerEncoderLayer-684"><span class="linenos">684</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_att</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-685"><a href="#ConformerEncoderLayer-685"><span class="linenos">685</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-686"><a href="#ConformerEncoderLayer-686"><span class="linenos">686</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-687"><a href="#ConformerEncoderLayer-687"><span class="linenos">687</span></a>
</span><span id="ConformerEncoderLayer-688"><a href="#ConformerEncoderLayer-688"><span class="linenos">688</span></a>        <span class="c1"># convolution module</span>
</span><span id="ConformerEncoderLayer-689"><a href="#ConformerEncoderLayer-689"><span class="linenos">689</span></a>        <span class="c1"># Fake new cnn cache here, and then change it in conv_module</span>
</span><span id="ConformerEncoderLayer-690"><a href="#ConformerEncoderLayer-690"><span class="linenos">690</span></a>        <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-691"><a href="#ConformerEncoderLayer-691"><span class="linenos">691</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-692"><a href="#ConformerEncoderLayer-692"><span class="linenos">692</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer-693"><a href="#ConformerEncoderLayer-693"><span class="linenos">693</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-694"><a href="#ConformerEncoderLayer-694"><span class="linenos">694</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-695"><a href="#ConformerEncoderLayer-695"><span class="linenos">695</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-696"><a href="#ConformerEncoderLayer-696"><span class="linenos">696</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-697"><a href="#ConformerEncoderLayer-697"><span class="linenos">697</span></a>
</span><span id="ConformerEncoderLayer-698"><a href="#ConformerEncoderLayer-698"><span class="linenos">698</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-699"><a href="#ConformerEncoderLayer-699"><span class="linenos">699</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-700"><a href="#ConformerEncoderLayer-700"><span class="linenos">700</span></a>
</span><span id="ConformerEncoderLayer-701"><a href="#ConformerEncoderLayer-701"><span class="linenos">701</span></a>        <span class="c1"># feed forward module</span>
</span><span id="ConformerEncoderLayer-702"><a href="#ConformerEncoderLayer-702"><span class="linenos">702</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer-703"><a href="#ConformerEncoderLayer-703"><span class="linenos">703</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-704"><a href="#ConformerEncoderLayer-704"><span class="linenos">704</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-705"><a href="#ConformerEncoderLayer-705"><span class="linenos">705</span></a>
</span><span id="ConformerEncoderLayer-706"><a href="#ConformerEncoderLayer-706"><span class="linenos">706</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConformerEncoderLayer-707"><a href="#ConformerEncoderLayer-707"><span class="linenos">707</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-708"><a href="#ConformerEncoderLayer-708"><span class="linenos">708</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-709"><a href="#ConformerEncoderLayer-709"><span class="linenos">709</span></a>
</span><span id="ConformerEncoderLayer-710"><a href="#ConformerEncoderLayer-710"><span class="linenos">710</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer-711"><a href="#ConformerEncoderLayer-711"><span class="linenos">711</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer-712"><a href="#ConformerEncoderLayer-712"><span class="linenos">712</span></a>
</span><span id="ConformerEncoderLayer-713"><a href="#ConformerEncoderLayer-713"><span class="linenos">713</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">new_att_cache</span><span class="p">,</span> <span class="n">new_cnn_cache</span>
</span></pre></div>


            <div class="docstring"><p>Encoder layer module.
Args:
    size (int): Input dimension.
    self_attn (torch.nn.Module): Self-attention module instance.
        <code><a href="#MultiHeadedAttention">MultiHeadedAttention</a></code> or <code><a href="#RelPositionMultiHeadedAttention">RelPositionMultiHeadedAttention</a></code>
        instance can be used as the argument.
    feed_forward (torch.nn.Module): Feed-forward module instance.
        <code><a href="#PositionwiseFeedForward">PositionwiseFeedForward</a></code> instance can be used as the argument.
    feed_forward_macaron (torch.nn.Module): Additional feed-forward module
         instance.
        <code><a href="#PositionwiseFeedForward">PositionwiseFeedForward</a></code> instance can be used as the argument.
    conv_module (torch.nn.Module): Convolution module instance.
        <code>ConvlutionModule</code> instance can be used as the argument.
    dropout_rate (float): Dropout rate.
    normalize_before (bool):
        True: use layer_norm before each sub-block.
        False: use layer_norm after each sub-block.</p>
</div>


                            <div id="ConformerEncoderLayer.__init__" class="classattr">
                                        <input id="ConformerEncoderLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ConformerEncoderLayer</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">self_attn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>,</span><span class="param">	<span class="n">feed_forward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">feed_forward_macaron</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">conv_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>,</span><span class="param">	<span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="ConformerEncoderLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoderLayer.__init__-607"><a href="#ConformerEncoderLayer.__init__-607"><span class="linenos">607</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer.__init__-608"><a href="#ConformerEncoderLayer.__init__-608"><span class="linenos">608</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-609"><a href="#ConformerEncoderLayer.__init__-609"><span class="linenos">609</span></a>        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-610"><a href="#ConformerEncoderLayer.__init__-610"><span class="linenos">610</span></a>        <span class="n">self_attn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-611"><a href="#ConformerEncoderLayer.__init__-611"><span class="linenos">611</span></a>        <span class="n">feed_forward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-612"><a href="#ConformerEncoderLayer.__init__-612"><span class="linenos">612</span></a>        <span class="n">feed_forward_macaron</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-613"><a href="#ConformerEncoderLayer.__init__-613"><span class="linenos">613</span></a>        <span class="n">conv_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-614"><a href="#ConformerEncoderLayer.__init__-614"><span class="linenos">614</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-615"><a href="#ConformerEncoderLayer.__init__-615"><span class="linenos">615</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.__init__-616"><a href="#ConformerEncoderLayer.__init__-616"><span class="linenos">616</span></a>    <span class="p">):</span>
</span><span id="ConformerEncoderLayer.__init__-617"><a href="#ConformerEncoderLayer.__init__-617"><span class="linenos">617</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an EncoderLayer object.&quot;&quot;&quot;</span>
</span><span id="ConformerEncoderLayer.__init__-618"><a href="#ConformerEncoderLayer.__init__-618"><span class="linenos">618</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConformerEncoderLayer.__init__-619"><a href="#ConformerEncoderLayer.__init__-619"><span class="linenos">619</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
</span><span id="ConformerEncoderLayer.__init__-620"><a href="#ConformerEncoderLayer.__init__-620"><span class="linenos">620</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
</span><span id="ConformerEncoderLayer.__init__-621"><a href="#ConformerEncoderLayer.__init__-621"><span class="linenos">621</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="o">=</span> <span class="n">feed_forward_macaron</span>
</span><span id="ConformerEncoderLayer.__init__-622"><a href="#ConformerEncoderLayer.__init__-622"><span class="linenos">622</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="o">=</span> <span class="n">conv_module</span>
</span><span id="ConformerEncoderLayer.__init__-623"><a href="#ConformerEncoderLayer.__init__-623"><span class="linenos">623</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the FNN module</span>
</span><span id="ConformerEncoderLayer.__init__-624"><a href="#ConformerEncoderLayer.__init__-624"><span class="linenos">624</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the MHA module</span>
</span><span id="ConformerEncoderLayer.__init__-625"><a href="#ConformerEncoderLayer.__init__-625"><span class="linenos">625</span></a>        <span class="k">if</span> <span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.__init__-626"><a href="#ConformerEncoderLayer.__init__-626"><span class="linenos">626</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.__init__-627"><a href="#ConformerEncoderLayer.__init__-627"><span class="linenos">627</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="ConformerEncoderLayer.__init__-628"><a href="#ConformerEncoderLayer.__init__-628"><span class="linenos">628</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.__init__-629"><a href="#ConformerEncoderLayer.__init__-629"><span class="linenos">629</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span id="ConformerEncoderLayer.__init__-630"><a href="#ConformerEncoderLayer.__init__-630"><span class="linenos">630</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.__init__-631"><a href="#ConformerEncoderLayer.__init__-631"><span class="linenos">631</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># for the CNN module</span>
</span><span id="ConformerEncoderLayer.__init__-632"><a href="#ConformerEncoderLayer.__init__-632"><span class="linenos">632</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer.__init__-633"><a href="#ConformerEncoderLayer.__init__-633"><span class="linenos">633</span></a>                <span class="n">size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="ConformerEncoderLayer.__init__-634"><a href="#ConformerEncoderLayer.__init__-634"><span class="linenos">634</span></a>            <span class="p">)</span>  <span class="c1"># for the final output of the block</span>
</span><span id="ConformerEncoderLayer.__init__-635"><a href="#ConformerEncoderLayer.__init__-635"><span class="linenos">635</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.__init__-636"><a href="#ConformerEncoderLayer.__init__-636"><span class="linenos">636</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span><span id="ConformerEncoderLayer.__init__-637"><a href="#ConformerEncoderLayer.__init__-637"><span class="linenos">637</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span></pre></div>


            <div class="docstring"><p>Construct an EncoderLayer object.</p>
</div>


                            </div>
                            <div id="ConformerEncoderLayer.self_attn" class="classattr">
                                <div class="attr variable">
            <span class="name">self_attn</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.self_attn"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.feed_forward" class="classattr">
                                <div class="attr variable">
            <span class="name">feed_forward</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.feed_forward"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.feed_forward_macaron" class="classattr">
                                <div class="attr variable">
            <span class="name">feed_forward_macaron</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.feed_forward_macaron"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.conv_module" class="classattr">
                                <div class="attr variable">
            <span class="name">conv_module</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.conv_module"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.norm_ff" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_ff</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.norm_ff"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.norm_mha" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_mha</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.norm_mha"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">dropout</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.dropout"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.size" class="classattr">
                                <div class="attr variable">
            <span class="name">size</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.size"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.normalize_before" class="classattr">
                                <div class="attr variable">
            <span class="name">normalize_before</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.normalize_before"></a>
    
    

                            </div>
                            <div id="ConformerEncoderLayer.forward" class="classattr">
                                        <input id="ConformerEncoderLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>,</span><span class="param">	<span class="n">att_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>,</span><span class="param">	<span class="n">cnn_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="ConformerEncoderLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoderLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoderLayer.forward-639"><a href="#ConformerEncoderLayer.forward-639"><span class="linenos">639</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConformerEncoderLayer.forward-640"><a href="#ConformerEncoderLayer.forward-640"><span class="linenos">640</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.forward-641"><a href="#ConformerEncoderLayer.forward-641"><span class="linenos">641</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.forward-642"><a href="#ConformerEncoderLayer.forward-642"><span class="linenos">642</span></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.forward-643"><a href="#ConformerEncoderLayer.forward-643"><span class="linenos">643</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoderLayer.forward-644"><a href="#ConformerEncoderLayer.forward-644"><span class="linenos">644</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
</span><span id="ConformerEncoderLayer.forward-645"><a href="#ConformerEncoderLayer.forward-645"><span class="linenos">645</span></a>        <span class="n">att_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConformerEncoderLayer.forward-646"><a href="#ConformerEncoderLayer.forward-646"><span class="linenos">646</span></a>        <span class="n">cnn_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
</span><span id="ConformerEncoderLayer.forward-647"><a href="#ConformerEncoderLayer.forward-647"><span class="linenos">647</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConformerEncoderLayer.forward-648"><a href="#ConformerEncoderLayer.forward-648"><span class="linenos">648</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute encoded features.</span>
</span><span id="ConformerEncoderLayer.forward-649"><a href="#ConformerEncoderLayer.forward-649"><span class="linenos">649</span></a>
</span><span id="ConformerEncoderLayer.forward-650"><a href="#ConformerEncoderLayer.forward-650"><span class="linenos">650</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoderLayer.forward-651"><a href="#ConformerEncoderLayer.forward-651"><span class="linenos">651</span></a><span class="sd">            x (torch.Tensor): (#batch, time, size)</span>
</span><span id="ConformerEncoderLayer.forward-652"><a href="#ConformerEncoderLayer.forward-652"><span class="linenos">652</span></a><span class="sd">            mask (torch.Tensor): Mask tensor for the input (#batch, timetime),</span>
</span><span id="ConformerEncoderLayer.forward-653"><a href="#ConformerEncoderLayer.forward-653"><span class="linenos">653</span></a><span class="sd">                (0, 0, 0) means fake mask.</span>
</span><span id="ConformerEncoderLayer.forward-654"><a href="#ConformerEncoderLayer.forward-654"><span class="linenos">654</span></a><span class="sd">            pos_emb (torch.Tensor): positional encoding, must not be None</span>
</span><span id="ConformerEncoderLayer.forward-655"><a href="#ConformerEncoderLayer.forward-655"><span class="linenos">655</span></a><span class="sd">                for ConformerEncoderLayer.</span>
</span><span id="ConformerEncoderLayer.forward-656"><a href="#ConformerEncoderLayer.forward-656"><span class="linenos">656</span></a><span class="sd">            mask_pad (torch.Tensor): batch padding mask used for conv module.</span>
</span><span id="ConformerEncoderLayer.forward-657"><a href="#ConformerEncoderLayer.forward-657"><span class="linenos">657</span></a><span class="sd">                (#batch, 1time), (0, 0, 0) means fake mask.</span>
</span><span id="ConformerEncoderLayer.forward-658"><a href="#ConformerEncoderLayer.forward-658"><span class="linenos">658</span></a><span class="sd">            att_cache (torch.Tensor): Cache tensor of the KEY &amp; VALUE</span>
</span><span id="ConformerEncoderLayer.forward-659"><a href="#ConformerEncoderLayer.forward-659"><span class="linenos">659</span></a><span class="sd">                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.</span>
</span><span id="ConformerEncoderLayer.forward-660"><a href="#ConformerEncoderLayer.forward-660"><span class="linenos">660</span></a><span class="sd">            cnn_cache (torch.Tensor): Convolution cache in conformer layer</span>
</span><span id="ConformerEncoderLayer.forward-661"><a href="#ConformerEncoderLayer.forward-661"><span class="linenos">661</span></a><span class="sd">                (#batch=1, size, cache_t2)</span>
</span><span id="ConformerEncoderLayer.forward-662"><a href="#ConformerEncoderLayer.forward-662"><span class="linenos">662</span></a><span class="sd">        Returns:</span>
</span><span id="ConformerEncoderLayer.forward-663"><a href="#ConformerEncoderLayer.forward-663"><span class="linenos">663</span></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, size).</span>
</span><span id="ConformerEncoderLayer.forward-664"><a href="#ConformerEncoderLayer.forward-664"><span class="linenos">664</span></a><span class="sd">            torch.Tensor: Mask tensor (#batch, time, time).</span>
</span><span id="ConformerEncoderLayer.forward-665"><a href="#ConformerEncoderLayer.forward-665"><span class="linenos">665</span></a><span class="sd">            torch.Tensor: att_cache tensor,</span>
</span><span id="ConformerEncoderLayer.forward-666"><a href="#ConformerEncoderLayer.forward-666"><span class="linenos">666</span></a><span class="sd">                (#batch=1, head, cache_t1 + time, d_k * 2).</span>
</span><span id="ConformerEncoderLayer.forward-667"><a href="#ConformerEncoderLayer.forward-667"><span class="linenos">667</span></a><span class="sd">            torch.Tensor: cnn_cahce tensor (#batch, size, cache_t2).</span>
</span><span id="ConformerEncoderLayer.forward-668"><a href="#ConformerEncoderLayer.forward-668"><span class="linenos">668</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoderLayer.forward-669"><a href="#ConformerEncoderLayer.forward-669"><span class="linenos">669</span></a>
</span><span id="ConformerEncoderLayer.forward-670"><a href="#ConformerEncoderLayer.forward-670"><span class="linenos">670</span></a>        <span class="c1"># whether to use macaron style</span>
</span><span id="ConformerEncoderLayer.forward-671"><a href="#ConformerEncoderLayer.forward-671"><span class="linenos">671</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-672"><a href="#ConformerEncoderLayer.forward-672"><span class="linenos">672</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer.forward-673"><a href="#ConformerEncoderLayer.forward-673"><span class="linenos">673</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-674"><a href="#ConformerEncoderLayer.forward-674"><span class="linenos">674</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-675"><a href="#ConformerEncoderLayer.forward-675"><span class="linenos">675</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConformerEncoderLayer.forward-676"><a href="#ConformerEncoderLayer.forward-676"><span class="linenos">676</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-677"><a href="#ConformerEncoderLayer.forward-677"><span class="linenos">677</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-678"><a href="#ConformerEncoderLayer.forward-678"><span class="linenos">678</span></a>
</span><span id="ConformerEncoderLayer.forward-679"><a href="#ConformerEncoderLayer.forward-679"><span class="linenos">679</span></a>        <span class="c1"># multi-headed self-attention module</span>
</span><span id="ConformerEncoderLayer.forward-680"><a href="#ConformerEncoderLayer.forward-680"><span class="linenos">680</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer.forward-681"><a href="#ConformerEncoderLayer.forward-681"><span class="linenos">681</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-682"><a href="#ConformerEncoderLayer.forward-682"><span class="linenos">682</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-683"><a href="#ConformerEncoderLayer.forward-683"><span class="linenos">683</span></a>        <span class="n">x_att</span><span class="p">,</span> <span class="n">new_att_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">att_cache</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-684"><a href="#ConformerEncoderLayer.forward-684"><span class="linenos">684</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_att</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-685"><a href="#ConformerEncoderLayer.forward-685"><span class="linenos">685</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-686"><a href="#ConformerEncoderLayer.forward-686"><span class="linenos">686</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-687"><a href="#ConformerEncoderLayer.forward-687"><span class="linenos">687</span></a>
</span><span id="ConformerEncoderLayer.forward-688"><a href="#ConformerEncoderLayer.forward-688"><span class="linenos">688</span></a>        <span class="c1"># convolution module</span>
</span><span id="ConformerEncoderLayer.forward-689"><a href="#ConformerEncoderLayer.forward-689"><span class="linenos">689</span></a>        <span class="c1"># Fake new cnn cache here, and then change it in conv_module</span>
</span><span id="ConformerEncoderLayer.forward-690"><a href="#ConformerEncoderLayer.forward-690"><span class="linenos">690</span></a>        <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-691"><a href="#ConformerEncoderLayer.forward-691"><span class="linenos">691</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-692"><a href="#ConformerEncoderLayer.forward-692"><span class="linenos">692</span></a>            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer.forward-693"><a href="#ConformerEncoderLayer.forward-693"><span class="linenos">693</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-694"><a href="#ConformerEncoderLayer.forward-694"><span class="linenos">694</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-695"><a href="#ConformerEncoderLayer.forward-695"><span class="linenos">695</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-696"><a href="#ConformerEncoderLayer.forward-696"><span class="linenos">696</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-697"><a href="#ConformerEncoderLayer.forward-697"><span class="linenos">697</span></a>
</span><span id="ConformerEncoderLayer.forward-698"><a href="#ConformerEncoderLayer.forward-698"><span class="linenos">698</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-699"><a href="#ConformerEncoderLayer.forward-699"><span class="linenos">699</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-700"><a href="#ConformerEncoderLayer.forward-700"><span class="linenos">700</span></a>
</span><span id="ConformerEncoderLayer.forward-701"><a href="#ConformerEncoderLayer.forward-701"><span class="linenos">701</span></a>        <span class="c1"># feed forward module</span>
</span><span id="ConformerEncoderLayer.forward-702"><a href="#ConformerEncoderLayer.forward-702"><span class="linenos">702</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConformerEncoderLayer.forward-703"><a href="#ConformerEncoderLayer.forward-703"><span class="linenos">703</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-704"><a href="#ConformerEncoderLayer.forward-704"><span class="linenos">704</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-705"><a href="#ConformerEncoderLayer.forward-705"><span class="linenos">705</span></a>
</span><span id="ConformerEncoderLayer.forward-706"><a href="#ConformerEncoderLayer.forward-706"><span class="linenos">706</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="ConformerEncoderLayer.forward-707"><a href="#ConformerEncoderLayer.forward-707"><span class="linenos">707</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-708"><a href="#ConformerEncoderLayer.forward-708"><span class="linenos">708</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-709"><a href="#ConformerEncoderLayer.forward-709"><span class="linenos">709</span></a>
</span><span id="ConformerEncoderLayer.forward-710"><a href="#ConformerEncoderLayer.forward-710"><span class="linenos">710</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConformerEncoderLayer.forward-711"><a href="#ConformerEncoderLayer.forward-711"><span class="linenos">711</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConformerEncoderLayer.forward-712"><a href="#ConformerEncoderLayer.forward-712"><span class="linenos">712</span></a>
</span><span id="ConformerEncoderLayer.forward-713"><a href="#ConformerEncoderLayer.forward-713"><span class="linenos">713</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">new_att_cache</span><span class="p">,</span> <span class="n">new_cnn_cache</span>
</span></pre></div>


            <div class="docstring"><p>Compute encoded features.</p>

<p>Args:
    x (torch.Tensor): (#batch, time, size)
    mask (torch.Tensor): Mask tensor for the input (#batch, timetime),
        (0, 0, 0) means fake mask.
    pos_emb (torch.Tensor): positional encoding, must not be None
        for ConformerEncoderLayer.
    mask_pad (torch.Tensor): batch padding mask used for conv module.
        (#batch, 1time), (0, 0, 0) means fake mask.
    att_cache (torch.Tensor): Cache tensor of the KEY &amp; VALUE
        (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.
    cnn_cache (torch.Tensor): Convolution cache in conformer layer
        (#batch=1, size, cache_t2)
Returns:
    torch.Tensor: Output tensor (#batch, time, size).
    torch.Tensor: Mask tensor (#batch, time, time).
    torch.Tensor: att_cache tensor,
        (#batch=1, head, cache_t1 + time, d_k * 2).
    torch.Tensor: cnn_cahce tensor (#batch, size, cache_t2).</p>
</div>


                            </div>
                </section>
                <section id="EspnetRelPositionalEncoding">
                            <input id="EspnetRelPositionalEncoding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">EspnetRelPositionalEncoding</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="EspnetRelPositionalEncoding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EspnetRelPositionalEncoding-716"><a href="#EspnetRelPositionalEncoding-716"><span class="linenos">716</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EspnetRelPositionalEncoding</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="EspnetRelPositionalEncoding-717"><a href="#EspnetRelPositionalEncoding-717"><span class="linenos">717</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Relative positional encoding module (new implementation).</span>
</span><span id="EspnetRelPositionalEncoding-718"><a href="#EspnetRelPositionalEncoding-718"><span class="linenos">718</span></a>
</span><span id="EspnetRelPositionalEncoding-719"><a href="#EspnetRelPositionalEncoding-719"><span class="linenos">719</span></a><span class="sd">    Details can be found in https://github.com/espnet/espnet/pull/2816.</span>
</span><span id="EspnetRelPositionalEncoding-720"><a href="#EspnetRelPositionalEncoding-720"><span class="linenos">720</span></a>
</span><span id="EspnetRelPositionalEncoding-721"><a href="#EspnetRelPositionalEncoding-721"><span class="linenos">721</span></a><span class="sd">    See : Appendix B in https://arxiv.org/abs/1901.02860</span>
</span><span id="EspnetRelPositionalEncoding-722"><a href="#EspnetRelPositionalEncoding-722"><span class="linenos">722</span></a>
</span><span id="EspnetRelPositionalEncoding-723"><a href="#EspnetRelPositionalEncoding-723"><span class="linenos">723</span></a><span class="sd">    Args:</span>
</span><span id="EspnetRelPositionalEncoding-724"><a href="#EspnetRelPositionalEncoding-724"><span class="linenos">724</span></a><span class="sd">        d_model (int): Embedding dimension.</span>
</span><span id="EspnetRelPositionalEncoding-725"><a href="#EspnetRelPositionalEncoding-725"><span class="linenos">725</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="EspnetRelPositionalEncoding-726"><a href="#EspnetRelPositionalEncoding-726"><span class="linenos">726</span></a><span class="sd">        max_len (int): Maximum input length.</span>
</span><span id="EspnetRelPositionalEncoding-727"><a href="#EspnetRelPositionalEncoding-727"><span class="linenos">727</span></a>
</span><span id="EspnetRelPositionalEncoding-728"><a href="#EspnetRelPositionalEncoding-728"><span class="linenos">728</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding-729"><a href="#EspnetRelPositionalEncoding-729"><span class="linenos">729</span></a>
</span><span id="EspnetRelPositionalEncoding-730"><a href="#EspnetRelPositionalEncoding-730"><span class="linenos">730</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
</span><span id="EspnetRelPositionalEncoding-731"><a href="#EspnetRelPositionalEncoding-731"><span class="linenos">731</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an PositionalEncoding object.&quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding-732"><a href="#EspnetRelPositionalEncoding-732"><span class="linenos">732</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">EspnetRelPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="EspnetRelPositionalEncoding-733"><a href="#EspnetRelPositionalEncoding-733"><span class="linenos">733</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="EspnetRelPositionalEncoding-734"><a href="#EspnetRelPositionalEncoding-734"><span class="linenos">734</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-735"><a href="#EspnetRelPositionalEncoding-735"><span class="linenos">735</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-736"><a href="#EspnetRelPositionalEncoding-736"><span class="linenos">736</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="EspnetRelPositionalEncoding-737"><a href="#EspnetRelPositionalEncoding-737"><span class="linenos">737</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">))</span>
</span><span id="EspnetRelPositionalEncoding-738"><a href="#EspnetRelPositionalEncoding-738"><span class="linenos">738</span></a>
</span><span id="EspnetRelPositionalEncoding-739"><a href="#EspnetRelPositionalEncoding-739"><span class="linenos">739</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">extend_pe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="EspnetRelPositionalEncoding-740"><a href="#EspnetRelPositionalEncoding-740"><span class="linenos">740</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the positional encodings.&quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding-741"><a href="#EspnetRelPositionalEncoding-741"><span class="linenos">741</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding-742"><a href="#EspnetRelPositionalEncoding-742"><span class="linenos">742</span></a>            <span class="c1"># self.pe contains both positive and negative parts</span>
</span><span id="EspnetRelPositionalEncoding-743"><a href="#EspnetRelPositionalEncoding-743"><span class="linenos">743</span></a>            <span class="c1"># the length of self.pe is 2 * input_len - 1</span>
</span><span id="EspnetRelPositionalEncoding-744"><a href="#EspnetRelPositionalEncoding-744"><span class="linenos">744</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding-745"><a href="#EspnetRelPositionalEncoding-745"><span class="linenos">745</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding-746"><a href="#EspnetRelPositionalEncoding-746"><span class="linenos">746</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-747"><a href="#EspnetRelPositionalEncoding-747"><span class="linenos">747</span></a>                <span class="k">return</span>
</span><span id="EspnetRelPositionalEncoding-748"><a href="#EspnetRelPositionalEncoding-748"><span class="linenos">748</span></a>        <span class="c1"># Suppose `i` means to the position of query vecotr and `j` means the</span>
</span><span id="EspnetRelPositionalEncoding-749"><a href="#EspnetRelPositionalEncoding-749"><span class="linenos">749</span></a>        <span class="c1"># position of key vector. We use position relative positions when keys</span>
</span><span id="EspnetRelPositionalEncoding-750"><a href="#EspnetRelPositionalEncoding-750"><span class="linenos">750</span></a>        <span class="c1"># are to the left (i&gt;j) and negative relative positions otherwise (i&lt;j).</span>
</span><span id="EspnetRelPositionalEncoding-751"><a href="#EspnetRelPositionalEncoding-751"><span class="linenos">751</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-752"><a href="#EspnetRelPositionalEncoding-752"><span class="linenos">752</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-753"><a href="#EspnetRelPositionalEncoding-753"><span class="linenos">753</span></a>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-754"><a href="#EspnetRelPositionalEncoding-754"><span class="linenos">754</span></a>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding-755"><a href="#EspnetRelPositionalEncoding-755"><span class="linenos">755</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-756"><a href="#EspnetRelPositionalEncoding-756"><span class="linenos">756</span></a>            <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-757"><a href="#EspnetRelPositionalEncoding-757"><span class="linenos">757</span></a>        <span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-758"><a href="#EspnetRelPositionalEncoding-758"><span class="linenos">758</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-759"><a href="#EspnetRelPositionalEncoding-759"><span class="linenos">759</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-760"><a href="#EspnetRelPositionalEncoding-760"><span class="linenos">760</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-761"><a href="#EspnetRelPositionalEncoding-761"><span class="linenos">761</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-762"><a href="#EspnetRelPositionalEncoding-762"><span class="linenos">762</span></a>
</span><span id="EspnetRelPositionalEncoding-763"><a href="#EspnetRelPositionalEncoding-763"><span class="linenos">763</span></a>        <span class="c1"># Reserve the order of positive indices and concat both positive and</span>
</span><span id="EspnetRelPositionalEncoding-764"><a href="#EspnetRelPositionalEncoding-764"><span class="linenos">764</span></a>        <span class="c1"># negative indices. This is used to support the shifting trick</span>
</span><span id="EspnetRelPositionalEncoding-765"><a href="#EspnetRelPositionalEncoding-765"><span class="linenos">765</span></a>        <span class="c1"># as in https://arxiv.org/abs/1901.02860</span>
</span><span id="EspnetRelPositionalEncoding-766"><a href="#EspnetRelPositionalEncoding-766"><span class="linenos">766</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">pe_positive</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-767"><a href="#EspnetRelPositionalEncoding-767"><span class="linenos">767</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">pe_negative</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-768"><a href="#EspnetRelPositionalEncoding-768"><span class="linenos">768</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pe_positive</span><span class="p">,</span> <span class="n">pe_negative</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-769"><a href="#EspnetRelPositionalEncoding-769"><span class="linenos">769</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-770"><a href="#EspnetRelPositionalEncoding-770"><span class="linenos">770</span></a>
</span><span id="EspnetRelPositionalEncoding-771"><a href="#EspnetRelPositionalEncoding-771"><span class="linenos">771</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding-772"><a href="#EspnetRelPositionalEncoding-772"><span class="linenos">772</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="EspnetRelPositionalEncoding-773"><a href="#EspnetRelPositionalEncoding-773"><span class="linenos">773</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="EspnetRelPositionalEncoding-774"><a href="#EspnetRelPositionalEncoding-774"><span class="linenos">774</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Add positional encoding.</span>
</span><span id="EspnetRelPositionalEncoding-775"><a href="#EspnetRelPositionalEncoding-775"><span class="linenos">775</span></a>
</span><span id="EspnetRelPositionalEncoding-776"><a href="#EspnetRelPositionalEncoding-776"><span class="linenos">776</span></a><span class="sd">        Args:</span>
</span><span id="EspnetRelPositionalEncoding-777"><a href="#EspnetRelPositionalEncoding-777"><span class="linenos">777</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, time, `*`).</span>
</span><span id="EspnetRelPositionalEncoding-778"><a href="#EspnetRelPositionalEncoding-778"><span class="linenos">778</span></a>
</span><span id="EspnetRelPositionalEncoding-779"><a href="#EspnetRelPositionalEncoding-779"><span class="linenos">779</span></a><span class="sd">        Returns:</span>
</span><span id="EspnetRelPositionalEncoding-780"><a href="#EspnetRelPositionalEncoding-780"><span class="linenos">780</span></a><span class="sd">            torch.Tensor: Encoded tensor (batch, time, `*`).</span>
</span><span id="EspnetRelPositionalEncoding-781"><a href="#EspnetRelPositionalEncoding-781"><span class="linenos">781</span></a>
</span><span id="EspnetRelPositionalEncoding-782"><a href="#EspnetRelPositionalEncoding-782"><span class="linenos">782</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding-783"><a href="#EspnetRelPositionalEncoding-783"><span class="linenos">783</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-784"><a href="#EspnetRelPositionalEncoding-784"><span class="linenos">784</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span>
</span><span id="EspnetRelPositionalEncoding-785"><a href="#EspnetRelPositionalEncoding-785"><span class="linenos">785</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-786"><a href="#EspnetRelPositionalEncoding-786"><span class="linenos">786</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding-787"><a href="#EspnetRelPositionalEncoding-787"><span class="linenos">787</span></a>
</span><span id="EspnetRelPositionalEncoding-788"><a href="#EspnetRelPositionalEncoding-788"><span class="linenos">788</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding-789"><a href="#EspnetRelPositionalEncoding-789"><span class="linenos">789</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="EspnetRelPositionalEncoding-790"><a href="#EspnetRelPositionalEncoding-790"><span class="linenos">790</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding-791"><a href="#EspnetRelPositionalEncoding-791"><span class="linenos">791</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;For getting encoding in a streaming fashion</span>
</span><span id="EspnetRelPositionalEncoding-792"><a href="#EspnetRelPositionalEncoding-792"><span class="linenos">792</span></a>
</span><span id="EspnetRelPositionalEncoding-793"><a href="#EspnetRelPositionalEncoding-793"><span class="linenos">793</span></a><span class="sd">        Attention!!!!!</span>
</span><span id="EspnetRelPositionalEncoding-794"><a href="#EspnetRelPositionalEncoding-794"><span class="linenos">794</span></a><span class="sd">        we apply dropout only once at the whole utterance level in a none</span>
</span><span id="EspnetRelPositionalEncoding-795"><a href="#EspnetRelPositionalEncoding-795"><span class="linenos">795</span></a><span class="sd">        streaming way, but will call this function several times with</span>
</span><span id="EspnetRelPositionalEncoding-796"><a href="#EspnetRelPositionalEncoding-796"><span class="linenos">796</span></a><span class="sd">        increasing input size in a streaming scenario, so the dropout will</span>
</span><span id="EspnetRelPositionalEncoding-797"><a href="#EspnetRelPositionalEncoding-797"><span class="linenos">797</span></a><span class="sd">        be applied several times.</span>
</span><span id="EspnetRelPositionalEncoding-798"><a href="#EspnetRelPositionalEncoding-798"><span class="linenos">798</span></a>
</span><span id="EspnetRelPositionalEncoding-799"><a href="#EspnetRelPositionalEncoding-799"><span class="linenos">799</span></a><span class="sd">        Args:</span>
</span><span id="EspnetRelPositionalEncoding-800"><a href="#EspnetRelPositionalEncoding-800"><span class="linenos">800</span></a><span class="sd">            offset (int or torch.tensor): start offset</span>
</span><span id="EspnetRelPositionalEncoding-801"><a href="#EspnetRelPositionalEncoding-801"><span class="linenos">801</span></a><span class="sd">            size (int): required size of position encoding</span>
</span><span id="EspnetRelPositionalEncoding-802"><a href="#EspnetRelPositionalEncoding-802"><span class="linenos">802</span></a>
</span><span id="EspnetRelPositionalEncoding-803"><a href="#EspnetRelPositionalEncoding-803"><span class="linenos">803</span></a><span class="sd">        Returns:</span>
</span><span id="EspnetRelPositionalEncoding-804"><a href="#EspnetRelPositionalEncoding-804"><span class="linenos">804</span></a><span class="sd">            torch.Tensor: Corresponding encoding</span>
</span><span id="EspnetRelPositionalEncoding-805"><a href="#EspnetRelPositionalEncoding-805"><span class="linenos">805</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding-806"><a href="#EspnetRelPositionalEncoding-806"><span class="linenos">806</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[</span>
</span><span id="EspnetRelPositionalEncoding-807"><a href="#EspnetRelPositionalEncoding-807"><span class="linenos">807</span></a>            <span class="p">:,</span>
</span><span id="EspnetRelPositionalEncoding-808"><a href="#EspnetRelPositionalEncoding-808"><span class="linenos">808</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">size</span><span class="p">,</span>
</span><span id="EspnetRelPositionalEncoding-809"><a href="#EspnetRelPositionalEncoding-809"><span class="linenos">809</span></a>        <span class="p">]</span>
</span><span id="EspnetRelPositionalEncoding-810"><a href="#EspnetRelPositionalEncoding-810"><span class="linenos">810</span></a>        <span class="k">return</span> <span class="n">pos_emb</span>
</span></pre></div>


            <div class="docstring"><p>Relative positional encoding module (new implementation).</p>

<p>Details can be found in <a href="https://github.com/espnet/espnet/pull/2816">https://github.com/espnet/espnet/pull/2816</a>.</p>

<p>See : Appendix B in <a href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a></p>

<p>Args:
    d_model (int): Embedding dimension.
    dropout_rate (float): Dropout rate.
    max_len (int): Maximum input length.</p>
</div>


                            <div id="EspnetRelPositionalEncoding.__init__" class="classattr">
                                        <input id="EspnetRelPositionalEncoding.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">EspnetRelPositionalEncoding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>, </span><span class="param"><span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span></span>)</span>

                <label class="view-source-button" for="EspnetRelPositionalEncoding.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EspnetRelPositionalEncoding.__init__-730"><a href="#EspnetRelPositionalEncoding.__init__-730"><span class="linenos">730</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
</span><span id="EspnetRelPositionalEncoding.__init__-731"><a href="#EspnetRelPositionalEncoding.__init__-731"><span class="linenos">731</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an PositionalEncoding object.&quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding.__init__-732"><a href="#EspnetRelPositionalEncoding.__init__-732"><span class="linenos">732</span></a>        <span class="nb">super</span><span class="p">(</span><span class="n">EspnetRelPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="EspnetRelPositionalEncoding.__init__-733"><a href="#EspnetRelPositionalEncoding.__init__-733"><span class="linenos">733</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="EspnetRelPositionalEncoding.__init__-734"><a href="#EspnetRelPositionalEncoding.__init__-734"><span class="linenos">734</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.__init__-735"><a href="#EspnetRelPositionalEncoding.__init__-735"><span class="linenos">735</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.__init__-736"><a href="#EspnetRelPositionalEncoding.__init__-736"><span class="linenos">736</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="EspnetRelPositionalEncoding.__init__-737"><a href="#EspnetRelPositionalEncoding.__init__-737"><span class="linenos">737</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Construct an PositionalEncoding object.</p>
</div>


                            </div>
                            <div id="EspnetRelPositionalEncoding.d_model" class="classattr">
                                <div class="attr variable">
            <span class="name">d_model</span>

        
    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.d_model"></a>
    
    

                            </div>
                            <div id="EspnetRelPositionalEncoding.xscale" class="classattr">
                                <div class="attr variable">
            <span class="name">xscale</span>

        
    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.xscale"></a>
    
    

                            </div>
                            <div id="EspnetRelPositionalEncoding.dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">dropout</span>

        
    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.dropout"></a>
    
    

                            </div>
                            <div id="EspnetRelPositionalEncoding.pe" class="classattr">
                                <div class="attr variable">
            <span class="name">pe</span>

        
    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.pe"></a>
    
    

                            </div>
                            <div id="EspnetRelPositionalEncoding.extend_pe" class="classattr">
                                        <input id="EspnetRelPositionalEncoding.extend_pe-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">extend_pe</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="EspnetRelPositionalEncoding.extend_pe-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.extend_pe"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EspnetRelPositionalEncoding.extend_pe-739"><a href="#EspnetRelPositionalEncoding.extend_pe-739"><span class="linenos">739</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">extend_pe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-740"><a href="#EspnetRelPositionalEncoding.extend_pe-740"><span class="linenos">740</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the positional encodings.&quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-741"><a href="#EspnetRelPositionalEncoding.extend_pe-741"><span class="linenos">741</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-742"><a href="#EspnetRelPositionalEncoding.extend_pe-742"><span class="linenos">742</span></a>            <span class="c1"># self.pe contains both positive and negative parts</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-743"><a href="#EspnetRelPositionalEncoding.extend_pe-743"><span class="linenos">743</span></a>            <span class="c1"># the length of self.pe is 2 * input_len - 1</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-744"><a href="#EspnetRelPositionalEncoding.extend_pe-744"><span class="linenos">744</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-745"><a href="#EspnetRelPositionalEncoding.extend_pe-745"><span class="linenos">745</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-746"><a href="#EspnetRelPositionalEncoding.extend_pe-746"><span class="linenos">746</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-747"><a href="#EspnetRelPositionalEncoding.extend_pe-747"><span class="linenos">747</span></a>                <span class="k">return</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-748"><a href="#EspnetRelPositionalEncoding.extend_pe-748"><span class="linenos">748</span></a>        <span class="c1"># Suppose `i` means to the position of query vecotr and `j` means the</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-749"><a href="#EspnetRelPositionalEncoding.extend_pe-749"><span class="linenos">749</span></a>        <span class="c1"># position of key vector. We use position relative positions when keys</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-750"><a href="#EspnetRelPositionalEncoding.extend_pe-750"><span class="linenos">750</span></a>        <span class="c1"># are to the left (i&gt;j) and negative relative positions otherwise (i&lt;j).</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-751"><a href="#EspnetRelPositionalEncoding.extend_pe-751"><span class="linenos">751</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-752"><a href="#EspnetRelPositionalEncoding.extend_pe-752"><span class="linenos">752</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-753"><a href="#EspnetRelPositionalEncoding.extend_pe-753"><span class="linenos">753</span></a>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-754"><a href="#EspnetRelPositionalEncoding.extend_pe-754"><span class="linenos">754</span></a>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-755"><a href="#EspnetRelPositionalEncoding.extend_pe-755"><span class="linenos">755</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-756"><a href="#EspnetRelPositionalEncoding.extend_pe-756"><span class="linenos">756</span></a>            <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-757"><a href="#EspnetRelPositionalEncoding.extend_pe-757"><span class="linenos">757</span></a>        <span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-758"><a href="#EspnetRelPositionalEncoding.extend_pe-758"><span class="linenos">758</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-759"><a href="#EspnetRelPositionalEncoding.extend_pe-759"><span class="linenos">759</span></a>        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-760"><a href="#EspnetRelPositionalEncoding.extend_pe-760"><span class="linenos">760</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-761"><a href="#EspnetRelPositionalEncoding.extend_pe-761"><span class="linenos">761</span></a>        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-762"><a href="#EspnetRelPositionalEncoding.extend_pe-762"><span class="linenos">762</span></a>
</span><span id="EspnetRelPositionalEncoding.extend_pe-763"><a href="#EspnetRelPositionalEncoding.extend_pe-763"><span class="linenos">763</span></a>        <span class="c1"># Reserve the order of positive indices and concat both positive and</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-764"><a href="#EspnetRelPositionalEncoding.extend_pe-764"><span class="linenos">764</span></a>        <span class="c1"># negative indices. This is used to support the shifting trick</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-765"><a href="#EspnetRelPositionalEncoding.extend_pe-765"><span class="linenos">765</span></a>        <span class="c1"># as in https://arxiv.org/abs/1901.02860</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-766"><a href="#EspnetRelPositionalEncoding.extend_pe-766"><span class="linenos">766</span></a>        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">pe_positive</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-767"><a href="#EspnetRelPositionalEncoding.extend_pe-767"><span class="linenos">767</span></a>        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">pe_negative</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-768"><a href="#EspnetRelPositionalEncoding.extend_pe-768"><span class="linenos">768</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pe_positive</span><span class="p">,</span> <span class="n">pe_negative</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.extend_pe-769"><a href="#EspnetRelPositionalEncoding.extend_pe-769"><span class="linenos">769</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Reset the positional encodings.</p>
</div>


                            </div>
                            <div id="EspnetRelPositionalEncoding.forward" class="classattr">
                                        <input id="EspnetRelPositionalEncoding.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="EspnetRelPositionalEncoding.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EspnetRelPositionalEncoding.forward-771"><a href="#EspnetRelPositionalEncoding.forward-771"><span class="linenos">771</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding.forward-772"><a href="#EspnetRelPositionalEncoding.forward-772"><span class="linenos">772</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="EspnetRelPositionalEncoding.forward-773"><a href="#EspnetRelPositionalEncoding.forward-773"><span class="linenos">773</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="EspnetRelPositionalEncoding.forward-774"><a href="#EspnetRelPositionalEncoding.forward-774"><span class="linenos">774</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Add positional encoding.</span>
</span><span id="EspnetRelPositionalEncoding.forward-775"><a href="#EspnetRelPositionalEncoding.forward-775"><span class="linenos">775</span></a>
</span><span id="EspnetRelPositionalEncoding.forward-776"><a href="#EspnetRelPositionalEncoding.forward-776"><span class="linenos">776</span></a><span class="sd">        Args:</span>
</span><span id="EspnetRelPositionalEncoding.forward-777"><a href="#EspnetRelPositionalEncoding.forward-777"><span class="linenos">777</span></a><span class="sd">            x (torch.Tensor): Input tensor (batch, time, `*`).</span>
</span><span id="EspnetRelPositionalEncoding.forward-778"><a href="#EspnetRelPositionalEncoding.forward-778"><span class="linenos">778</span></a>
</span><span id="EspnetRelPositionalEncoding.forward-779"><a href="#EspnetRelPositionalEncoding.forward-779"><span class="linenos">779</span></a><span class="sd">        Returns:</span>
</span><span id="EspnetRelPositionalEncoding.forward-780"><a href="#EspnetRelPositionalEncoding.forward-780"><span class="linenos">780</span></a><span class="sd">            torch.Tensor: Encoded tensor (batch, time, `*`).</span>
</span><span id="EspnetRelPositionalEncoding.forward-781"><a href="#EspnetRelPositionalEncoding.forward-781"><span class="linenos">781</span></a>
</span><span id="EspnetRelPositionalEncoding.forward-782"><a href="#EspnetRelPositionalEncoding.forward-782"><span class="linenos">782</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding.forward-783"><a href="#EspnetRelPositionalEncoding.forward-783"><span class="linenos">783</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.forward-784"><a href="#EspnetRelPositionalEncoding.forward-784"><span class="linenos">784</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">xscale</span>
</span><span id="EspnetRelPositionalEncoding.forward-785"><a href="#EspnetRelPositionalEncoding.forward-785"><span class="linenos">785</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">)</span>
</span><span id="EspnetRelPositionalEncoding.forward-786"><a href="#EspnetRelPositionalEncoding.forward-786"><span class="linenos">786</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Add positional encoding.</p>

<p>Args:
    x (torch.Tensor): Input tensor (batch, time, <code>*</code>).</p>

<p>Returns:
    torch.Tensor: Encoded tensor (batch, time, <code>*</code>).</p>
</div>


                            </div>
                            <div id="EspnetRelPositionalEncoding.position_encoding" class="classattr">
                                        <input id="EspnetRelPositionalEncoding.position_encoding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">position_encoding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>, </span><span class="param"><span class="n">size</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="EspnetRelPositionalEncoding.position_encoding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EspnetRelPositionalEncoding.position_encoding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EspnetRelPositionalEncoding.position_encoding-788"><a href="#EspnetRelPositionalEncoding.position_encoding-788"><span class="linenos">788</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-789"><a href="#EspnetRelPositionalEncoding.position_encoding-789"><span class="linenos">789</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-790"><a href="#EspnetRelPositionalEncoding.position_encoding-790"><span class="linenos">790</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-791"><a href="#EspnetRelPositionalEncoding.position_encoding-791"><span class="linenos">791</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;For getting encoding in a streaming fashion</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-792"><a href="#EspnetRelPositionalEncoding.position_encoding-792"><span class="linenos">792</span></a>
</span><span id="EspnetRelPositionalEncoding.position_encoding-793"><a href="#EspnetRelPositionalEncoding.position_encoding-793"><span class="linenos">793</span></a><span class="sd">        Attention!!!!!</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-794"><a href="#EspnetRelPositionalEncoding.position_encoding-794"><span class="linenos">794</span></a><span class="sd">        we apply dropout only once at the whole utterance level in a none</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-795"><a href="#EspnetRelPositionalEncoding.position_encoding-795"><span class="linenos">795</span></a><span class="sd">        streaming way, but will call this function several times with</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-796"><a href="#EspnetRelPositionalEncoding.position_encoding-796"><span class="linenos">796</span></a><span class="sd">        increasing input size in a streaming scenario, so the dropout will</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-797"><a href="#EspnetRelPositionalEncoding.position_encoding-797"><span class="linenos">797</span></a><span class="sd">        be applied several times.</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-798"><a href="#EspnetRelPositionalEncoding.position_encoding-798"><span class="linenos">798</span></a>
</span><span id="EspnetRelPositionalEncoding.position_encoding-799"><a href="#EspnetRelPositionalEncoding.position_encoding-799"><span class="linenos">799</span></a><span class="sd">        Args:</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-800"><a href="#EspnetRelPositionalEncoding.position_encoding-800"><span class="linenos">800</span></a><span class="sd">            offset (int or torch.tensor): start offset</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-801"><a href="#EspnetRelPositionalEncoding.position_encoding-801"><span class="linenos">801</span></a><span class="sd">            size (int): required size of position encoding</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-802"><a href="#EspnetRelPositionalEncoding.position_encoding-802"><span class="linenos">802</span></a>
</span><span id="EspnetRelPositionalEncoding.position_encoding-803"><a href="#EspnetRelPositionalEncoding.position_encoding-803"><span class="linenos">803</span></a><span class="sd">        Returns:</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-804"><a href="#EspnetRelPositionalEncoding.position_encoding-804"><span class="linenos">804</span></a><span class="sd">            torch.Tensor: Corresponding encoding</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-805"><a href="#EspnetRelPositionalEncoding.position_encoding-805"><span class="linenos">805</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-806"><a href="#EspnetRelPositionalEncoding.position_encoding-806"><span class="linenos">806</span></a>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-807"><a href="#EspnetRelPositionalEncoding.position_encoding-807"><span class="linenos">807</span></a>            <span class="p">:,</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-808"><a href="#EspnetRelPositionalEncoding.position_encoding-808"><span class="linenos">808</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">size</span><span class="p">,</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-809"><a href="#EspnetRelPositionalEncoding.position_encoding-809"><span class="linenos">809</span></a>        <span class="p">]</span>
</span><span id="EspnetRelPositionalEncoding.position_encoding-810"><a href="#EspnetRelPositionalEncoding.position_encoding-810"><span class="linenos">810</span></a>        <span class="k">return</span> <span class="n">pos_emb</span>
</span></pre></div>


            <div class="docstring"><p>For getting encoding in a streaming fashion</p>

<p>Attention!!!!!
we apply dropout only once at the whole utterance level in a none
streaming way, but will call this function several times with
increasing input size in a streaming scenario, so the dropout will
be applied several times.</p>

<p>Args:
    offset (int or torch.tensor): start offset
    size (int): required size of position encoding</p>

<p>Returns:
    torch.Tensor: Corresponding encoding</p>
</div>


                            </div>
                </section>
                <section id="LinearEmbed">
                            <input id="LinearEmbed-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">LinearEmbed</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="LinearEmbed-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbed"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbed-813"><a href="#LinearEmbed-813"><span class="linenos">813</span></a><span class="k">class</span><span class="w"> </span><span class="nc">LinearEmbed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="LinearEmbed-814"><a href="#LinearEmbed-814"><span class="linenos">814</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear transform the input without subsampling</span>
</span><span id="LinearEmbed-815"><a href="#LinearEmbed-815"><span class="linenos">815</span></a>
</span><span id="LinearEmbed-816"><a href="#LinearEmbed-816"><span class="linenos">816</span></a><span class="sd">    Args:</span>
</span><span id="LinearEmbed-817"><a href="#LinearEmbed-817"><span class="linenos">817</span></a><span class="sd">        idim (int): Input dimension.</span>
</span><span id="LinearEmbed-818"><a href="#LinearEmbed-818"><span class="linenos">818</span></a><span class="sd">        odim (int): Output dimension.</span>
</span><span id="LinearEmbed-819"><a href="#LinearEmbed-819"><span class="linenos">819</span></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
</span><span id="LinearEmbed-820"><a href="#LinearEmbed-820"><span class="linenos">820</span></a>
</span><span id="LinearEmbed-821"><a href="#LinearEmbed-821"><span class="linenos">821</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="LinearEmbed-822"><a href="#LinearEmbed-822"><span class="linenos">822</span></a>
</span><span id="LinearEmbed-823"><a href="#LinearEmbed-823"><span class="linenos">823</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearEmbed-824"><a href="#LinearEmbed-824"><span class="linenos">824</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">odim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">pos_enc_class</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
</span><span id="LinearEmbed-825"><a href="#LinearEmbed-825"><span class="linenos">825</span></a>    <span class="p">):</span>
</span><span id="LinearEmbed-826"><a href="#LinearEmbed-826"><span class="linenos">826</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an linear object.&quot;&quot;&quot;</span>
</span><span id="LinearEmbed-827"><a href="#LinearEmbed-827"><span class="linenos">827</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LinearEmbed-828"><a href="#LinearEmbed-828"><span class="linenos">828</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="LinearEmbed-829"><a href="#LinearEmbed-829"><span class="linenos">829</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">),</span>
</span><span id="LinearEmbed-830"><a href="#LinearEmbed-830"><span class="linenos">830</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
</span><span id="LinearEmbed-831"><a href="#LinearEmbed-831"><span class="linenos">831</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
</span><span id="LinearEmbed-832"><a href="#LinearEmbed-832"><span class="linenos">832</span></a>        <span class="p">)</span>
</span><span id="LinearEmbed-833"><a href="#LinearEmbed-833"><span class="linenos">833</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">pos_enc_class</span>  <span class="c1"># rel_pos_espnet</span>
</span><span id="LinearEmbed-834"><a href="#LinearEmbed-834"><span class="linenos">834</span></a>
</span><span id="LinearEmbed-835"><a href="#LinearEmbed-835"><span class="linenos">835</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="LinearEmbed-836"><a href="#LinearEmbed-836"><span class="linenos">836</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="LinearEmbed-837"><a href="#LinearEmbed-837"><span class="linenos">837</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="LinearEmbed-838"><a href="#LinearEmbed-838"><span class="linenos">838</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="LinearEmbed-839"><a href="#LinearEmbed-839"><span class="linenos">839</span></a>
</span><span id="LinearEmbed-840"><a href="#LinearEmbed-840"><span class="linenos">840</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="LinearEmbed-841"><a href="#LinearEmbed-841"><span class="linenos">841</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="LinearEmbed-842"><a href="#LinearEmbed-842"><span class="linenos">842</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="LinearEmbed-843"><a href="#LinearEmbed-843"><span class="linenos">843</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Input x.</span>
</span><span id="LinearEmbed-844"><a href="#LinearEmbed-844"><span class="linenos">844</span></a>
</span><span id="LinearEmbed-845"><a href="#LinearEmbed-845"><span class="linenos">845</span></a><span class="sd">        Args:</span>
</span><span id="LinearEmbed-846"><a href="#LinearEmbed-846"><span class="linenos">846</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, idim).</span>
</span><span id="LinearEmbed-847"><a href="#LinearEmbed-847"><span class="linenos">847</span></a><span class="sd">            x_mask (torch.Tensor): Input mask (#batch, 1, time).</span>
</span><span id="LinearEmbed-848"><a href="#LinearEmbed-848"><span class="linenos">848</span></a>
</span><span id="LinearEmbed-849"><a href="#LinearEmbed-849"><span class="linenos">849</span></a><span class="sd">        Returns:</span>
</span><span id="LinearEmbed-850"><a href="#LinearEmbed-850"><span class="linenos">850</span></a><span class="sd">            torch.Tensor: linear input tensor (#batch, time&#39;, odim),</span>
</span><span id="LinearEmbed-851"><a href="#LinearEmbed-851"><span class="linenos">851</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="LinearEmbed-852"><a href="#LinearEmbed-852"><span class="linenos">852</span></a><span class="sd">            torch.Tensor: linear input mask (#batch, 1, time&#39;),</span>
</span><span id="LinearEmbed-853"><a href="#LinearEmbed-853"><span class="linenos">853</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="LinearEmbed-854"><a href="#LinearEmbed-854"><span class="linenos">854</span></a>
</span><span id="LinearEmbed-855"><a href="#LinearEmbed-855"><span class="linenos">855</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearEmbed-856"><a href="#LinearEmbed-856"><span class="linenos">856</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="LinearEmbed-857"><a href="#LinearEmbed-857"><span class="linenos">857</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
</span><span id="LinearEmbed-858"><a href="#LinearEmbed-858"><span class="linenos">858</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span>
</span></pre></div>


            <div class="docstring"><p>Linear transform the input without subsampling</p>

<p>Args:
    idim (int): Input dimension.
    odim (int): Output dimension.
    dropout_rate (float): Dropout rate.</p>
</div>


                            <div id="LinearEmbed.__init__" class="classattr">
                                        <input id="LinearEmbed.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">LinearEmbed</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">idim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">odim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">pos_enc_class</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span></span>)</span>

                <label class="view-source-button" for="LinearEmbed.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbed.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbed.__init__-823"><a href="#LinearEmbed.__init__-823"><span class="linenos">823</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearEmbed.__init__-824"><a href="#LinearEmbed.__init__-824"><span class="linenos">824</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">odim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">pos_enc_class</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
</span><span id="LinearEmbed.__init__-825"><a href="#LinearEmbed.__init__-825"><span class="linenos">825</span></a>    <span class="p">):</span>
</span><span id="LinearEmbed.__init__-826"><a href="#LinearEmbed.__init__-826"><span class="linenos">826</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct an linear object.&quot;&quot;&quot;</span>
</span><span id="LinearEmbed.__init__-827"><a href="#LinearEmbed.__init__-827"><span class="linenos">827</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LinearEmbed.__init__-828"><a href="#LinearEmbed.__init__-828"><span class="linenos">828</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="LinearEmbed.__init__-829"><a href="#LinearEmbed.__init__-829"><span class="linenos">829</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">),</span>
</span><span id="LinearEmbed.__init__-830"><a href="#LinearEmbed.__init__-830"><span class="linenos">830</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
</span><span id="LinearEmbed.__init__-831"><a href="#LinearEmbed.__init__-831"><span class="linenos">831</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
</span><span id="LinearEmbed.__init__-832"><a href="#LinearEmbed.__init__-832"><span class="linenos">832</span></a>        <span class="p">)</span>
</span><span id="LinearEmbed.__init__-833"><a href="#LinearEmbed.__init__-833"><span class="linenos">833</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">pos_enc_class</span>  <span class="c1"># rel_pos_espnet</span>
</span></pre></div>


            <div class="docstring"><p>Construct an linear object.</p>
</div>


                            </div>
                            <div id="LinearEmbed.out" class="classattr">
                                <div class="attr variable">
            <span class="name">out</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbed.out"></a>
    
    

                            </div>
                            <div id="LinearEmbed.pos_enc" class="classattr">
                                <div class="attr variable">
            <span class="name">pos_enc</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbed.pos_enc"></a>
    
    

                            </div>
                            <div id="LinearEmbed.position_encoding" class="classattr">
                                        <input id="LinearEmbed.position_encoding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">position_encoding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>, </span><span class="param"><span class="n">size</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="LinearEmbed.position_encoding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbed.position_encoding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbed.position_encoding-835"><a href="#LinearEmbed.position_encoding-835"><span class="linenos">835</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">position_encoding</span><span class="p">(</span>
</span><span id="LinearEmbed.position_encoding-836"><a href="#LinearEmbed.position_encoding-836"><span class="linenos">836</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="LinearEmbed.position_encoding-837"><a href="#LinearEmbed.position_encoding-837"><span class="linenos">837</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="LinearEmbed.position_encoding-838"><a href="#LinearEmbed.position_encoding-838"><span class="linenos">838</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="LinearEmbed.forward" class="classattr">
                                        <input id="LinearEmbed.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="LinearEmbed.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbed.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbed.forward-840"><a href="#LinearEmbed.forward-840"><span class="linenos">840</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="LinearEmbed.forward-841"><a href="#LinearEmbed.forward-841"><span class="linenos">841</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="LinearEmbed.forward-842"><a href="#LinearEmbed.forward-842"><span class="linenos">842</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="LinearEmbed.forward-843"><a href="#LinearEmbed.forward-843"><span class="linenos">843</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Input x.</span>
</span><span id="LinearEmbed.forward-844"><a href="#LinearEmbed.forward-844"><span class="linenos">844</span></a>
</span><span id="LinearEmbed.forward-845"><a href="#LinearEmbed.forward-845"><span class="linenos">845</span></a><span class="sd">        Args:</span>
</span><span id="LinearEmbed.forward-846"><a href="#LinearEmbed.forward-846"><span class="linenos">846</span></a><span class="sd">            x (torch.Tensor): Input tensor (#batch, time, idim).</span>
</span><span id="LinearEmbed.forward-847"><a href="#LinearEmbed.forward-847"><span class="linenos">847</span></a><span class="sd">            x_mask (torch.Tensor): Input mask (#batch, 1, time).</span>
</span><span id="LinearEmbed.forward-848"><a href="#LinearEmbed.forward-848"><span class="linenos">848</span></a>
</span><span id="LinearEmbed.forward-849"><a href="#LinearEmbed.forward-849"><span class="linenos">849</span></a><span class="sd">        Returns:</span>
</span><span id="LinearEmbed.forward-850"><a href="#LinearEmbed.forward-850"><span class="linenos">850</span></a><span class="sd">            torch.Tensor: linear input tensor (#batch, time&#39;, odim),</span>
</span><span id="LinearEmbed.forward-851"><a href="#LinearEmbed.forward-851"><span class="linenos">851</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="LinearEmbed.forward-852"><a href="#LinearEmbed.forward-852"><span class="linenos">852</span></a><span class="sd">            torch.Tensor: linear input mask (#batch, 1, time&#39;),</span>
</span><span id="LinearEmbed.forward-853"><a href="#LinearEmbed.forward-853"><span class="linenos">853</span></a><span class="sd">                where time&#39; = time .</span>
</span><span id="LinearEmbed.forward-854"><a href="#LinearEmbed.forward-854"><span class="linenos">854</span></a>
</span><span id="LinearEmbed.forward-855"><a href="#LinearEmbed.forward-855"><span class="linenos">855</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearEmbed.forward-856"><a href="#LinearEmbed.forward-856"><span class="linenos">856</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="LinearEmbed.forward-857"><a href="#LinearEmbed.forward-857"><span class="linenos">857</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
</span><span id="LinearEmbed.forward-858"><a href="#LinearEmbed.forward-858"><span class="linenos">858</span></a>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span>
</span></pre></div>


            <div class="docstring"><p>Input x.</p>

<p>Args:
    x (torch.Tensor): Input tensor (#batch, time, idim).
    x_mask (torch.Tensor): Input mask (#batch, 1, time).</p>

<p>Returns:
    torch.Tensor: linear input tensor (#batch, time', odim),
        where time' = time .
    torch.Tensor: linear input mask (#batch, 1, time'),
        where time' = time .</p>
</div>


                            </div>
                </section>
                <section id="ATTENTION_CLASSES">
                    <div class="attr variable">
            <span class="name">ATTENTION_CLASSES</span>        =
<input id="ATTENTION_CLASSES-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="ATTENTION_CLASSES-view-value"></label><span class="default_value">{&#39;selfattn&#39;: &lt;class &#39;<a href="#MultiHeadedAttention">MultiHeadedAttention</a>&#39;&gt;, &#39;rel_selfattn&#39;: &lt;class &#39;<a href="#RelPositionMultiHeadedAttention">RelPositionMultiHeadedAttention</a>&#39;&gt;}</span>

        
    </div>
    <a class="headerlink" href="#ATTENTION_CLASSES"></a>
    
    

                </section>
                <section id="ACTIVATION_CLASSES">
                    <div class="attr variable">
            <span class="name">ACTIVATION_CLASSES</span>        =
<input id="ACTIVATION_CLASSES-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="ACTIVATION_CLASSES-view-value"></label><span class="default_value">{&#39;hardtanh&#39;: &lt;class &#39;torch.nn.modules.activation.Hardtanh&#39;&gt;, &#39;tanh&#39;: &lt;class &#39;torch.nn.modules.activation.Tanh&#39;&gt;, &#39;relu&#39;: &lt;class &#39;torch.nn.modules.activation.ReLU&#39;&gt;, &#39;selu&#39;: &lt;class &#39;torch.nn.modules.activation.SELU&#39;&gt;, &#39;swish&#39;: &lt;class &#39;torch.nn.modules.activation.SiLU&#39;&gt;, &#39;gelu&#39;: &lt;class &#39;torch.nn.modules.activation.GELU&#39;&gt;}</span>

        
    </div>
    <a class="headerlink" href="#ACTIVATION_CLASSES"></a>
    
    

                </section>
                <section id="make_pad_mask">
                            <input id="make_pad_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">make_pad_mask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>, </span><span class="param"><span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="make_pad_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#make_pad_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="make_pad_mask-876"><a href="#make_pad_mask-876"><span class="linenos">876</span></a><span class="k">def</span><span class="w"> </span><span class="nf">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="make_pad_mask-877"><a href="#make_pad_mask-877"><span class="linenos">877</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Make mask tensor containing indices of padded part.</span>
</span><span id="make_pad_mask-878"><a href="#make_pad_mask-878"><span class="linenos">878</span></a>
</span><span id="make_pad_mask-879"><a href="#make_pad_mask-879"><span class="linenos">879</span></a><span class="sd">    See description of make_non_pad_mask.</span>
</span><span id="make_pad_mask-880"><a href="#make_pad_mask-880"><span class="linenos">880</span></a>
</span><span id="make_pad_mask-881"><a href="#make_pad_mask-881"><span class="linenos">881</span></a><span class="sd">    Args:</span>
</span><span id="make_pad_mask-882"><a href="#make_pad_mask-882"><span class="linenos">882</span></a><span class="sd">        lengths (torch.Tensor): Batch of lengths (B,).</span>
</span><span id="make_pad_mask-883"><a href="#make_pad_mask-883"><span class="linenos">883</span></a><span class="sd">    Returns:</span>
</span><span id="make_pad_mask-884"><a href="#make_pad_mask-884"><span class="linenos">884</span></a><span class="sd">        torch.Tensor: Mask tensor containing indices of padded part.</span>
</span><span id="make_pad_mask-885"><a href="#make_pad_mask-885"><span class="linenos">885</span></a>
</span><span id="make_pad_mask-886"><a href="#make_pad_mask-886"><span class="linenos">886</span></a><span class="sd">    Examples:</span>
</span><span id="make_pad_mask-887"><a href="#make_pad_mask-887"><span class="linenos">887</span></a><span class="sd">        &gt;&gt;&gt; lengths = [5, 3, 2]</span>
</span><span id="make_pad_mask-888"><a href="#make_pad_mask-888"><span class="linenos">888</span></a><span class="sd">        &gt;&gt;&gt; make_pad_mask(lengths)</span>
</span><span id="make_pad_mask-889"><a href="#make_pad_mask-889"><span class="linenos">889</span></a><span class="sd">        masks = [[0, 0, 0, 0 ,0],</span>
</span><span id="make_pad_mask-890"><a href="#make_pad_mask-890"><span class="linenos">890</span></a><span class="sd">                 [0, 0, 0, 1, 1],</span>
</span><span id="make_pad_mask-891"><a href="#make_pad_mask-891"><span class="linenos">891</span></a><span class="sd">                 [0, 0, 1, 1, 1]]</span>
</span><span id="make_pad_mask-892"><a href="#make_pad_mask-892"><span class="linenos">892</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="make_pad_mask-893"><a href="#make_pad_mask-893"><span class="linenos">893</span></a>    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="make_pad_mask-894"><a href="#make_pad_mask-894"><span class="linenos">894</span></a>    <span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span> <span class="k">if</span> <span class="n">max_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="make_pad_mask-895"><a href="#make_pad_mask-895"><span class="linenos">895</span></a>    <span class="n">seq_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="make_pad_mask-896"><a href="#make_pad_mask-896"><span class="linenos">896</span></a>    <span class="n">seq_range_expand</span> <span class="o">=</span> <span class="n">seq_range</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
</span><span id="make_pad_mask-897"><a href="#make_pad_mask-897"><span class="linenos">897</span></a>    <span class="n">seq_length_expand</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="make_pad_mask-898"><a href="#make_pad_mask-898"><span class="linenos">898</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">seq_range_expand</span> <span class="o">&gt;=</span> <span class="n">seq_length_expand</span>
</span><span id="make_pad_mask-899"><a href="#make_pad_mask-899"><span class="linenos">899</span></a>    <span class="k">return</span> <span class="n">mask</span>
</span></pre></div>


            <div class="docstring"><p>Make mask tensor containing indices of padded part.</p>

<p>See description of make_non_pad_mask.</p>

<p>Args:
    lengths (torch.Tensor): Batch of lengths (B,).
Returns:
    torch.Tensor: Mask tensor containing indices of padded part.</p>

<p>Examples:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>lengths = [5, 3, 2]
      make_pad_mask(lengths)
          masks = [[0, 0, 0, 0 ,0],
                   [0, 0, 0, 1, 1],
                   [0, 0, 1, 1, 1]]</p>
    </blockquote>
  </blockquote>
</blockquote>
</div>


                </section>
                <section id="ConformerEncoder">
                            <input id="ConformerEncoder-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ConformerEncoder</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="ConformerEncoder-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoder"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoder-903"><a href="#ConformerEncoder-903"><span class="linenos"> 903</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ConformerEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="ConformerEncoder-904"><a href="#ConformerEncoder-904"><span class="linenos"> 904</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Conformer encoder module.&quot;&quot;&quot;</span>
</span><span id="ConformerEncoder-905"><a href="#ConformerEncoder-905"><span class="linenos"> 905</span></a>
</span><span id="ConformerEncoder-906"><a href="#ConformerEncoder-906"><span class="linenos"> 906</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConformerEncoder-907"><a href="#ConformerEncoder-907"><span class="linenos"> 907</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder-908"><a href="#ConformerEncoder-908"><span class="linenos"> 908</span></a>        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConformerEncoder-909"><a href="#ConformerEncoder-909"><span class="linenos"> 909</span></a>        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
</span><span id="ConformerEncoder-910"><a href="#ConformerEncoder-910"><span class="linenos"> 910</span></a>        <span class="n">attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="ConformerEncoder-911"><a href="#ConformerEncoder-911"><span class="linenos"> 911</span></a>        <span class="n">linear_units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span id="ConformerEncoder-912"><a href="#ConformerEncoder-912"><span class="linenos"> 912</span></a>        <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
</span><span id="ConformerEncoder-913"><a href="#ConformerEncoder-913"><span class="linenos"> 913</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoder-914"><a href="#ConformerEncoder-914"><span class="linenos"> 914</span></a>        <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoder-915"><a href="#ConformerEncoder-915"><span class="linenos"> 915</span></a>        <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="ConformerEncoder-916"><a href="#ConformerEncoder-916"><span class="linenos"> 916</span></a>        <span class="n">input_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder-917"><a href="#ConformerEncoder-917"><span class="linenos"> 917</span></a>        <span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_pos_espnet&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder-918"><a href="#ConformerEncoder-918"><span class="linenos"> 918</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoder-919"><a href="#ConformerEncoder-919"><span class="linenos"> 919</span></a>        <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># 1: causal_mask; 0: full_mask</span>
</span><span id="ConformerEncoder-920"><a href="#ConformerEncoder-920"><span class="linenos"> 920</span></a>        <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-921"><a href="#ConformerEncoder-921"><span class="linenos"> 921</span></a>        <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-922"><a href="#ConformerEncoder-922"><span class="linenos"> 922</span></a>        <span class="n">positionwise_conv_kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="ConformerEncoder-923"><a href="#ConformerEncoder-923"><span class="linenos"> 923</span></a>        <span class="n">macaron_style</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-924"><a href="#ConformerEncoder-924"><span class="linenos"> 924</span></a>        <span class="n">selfattention_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_selfattn&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder-925"><a href="#ConformerEncoder-925"><span class="linenos"> 925</span></a>        <span class="n">activation_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;swish&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder-926"><a href="#ConformerEncoder-926"><span class="linenos"> 926</span></a>        <span class="n">use_cnn_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-927"><a href="#ConformerEncoder-927"><span class="linenos"> 927</span></a>        <span class="n">cnn_module_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="ConformerEncoder-928"><a href="#ConformerEncoder-928"><span class="linenos"> 928</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-929"><a href="#ConformerEncoder-929"><span class="linenos"> 929</span></a>        <span class="n">cnn_module_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder-930"><a href="#ConformerEncoder-930"><span class="linenos"> 930</span></a>        <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoder-931"><a href="#ConformerEncoder-931"><span class="linenos"> 931</span></a>        <span class="n">gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder-932"><a href="#ConformerEncoder-932"><span class="linenos"> 932</span></a>    <span class="p">):</span>
</span><span id="ConformerEncoder-933"><a href="#ConformerEncoder-933"><span class="linenos"> 933</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct ConformerEncoder</span>
</span><span id="ConformerEncoder-934"><a href="#ConformerEncoder-934"><span class="linenos"> 934</span></a>
</span><span id="ConformerEncoder-935"><a href="#ConformerEncoder-935"><span class="linenos"> 935</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoder-936"><a href="#ConformerEncoder-936"><span class="linenos"> 936</span></a><span class="sd">            input_size to use_dynamic_chunk, see in BaseEncoder</span>
</span><span id="ConformerEncoder-937"><a href="#ConformerEncoder-937"><span class="linenos"> 937</span></a><span class="sd">            positionwise_conv_kernel_size (int): Kernel size of positionwise</span>
</span><span id="ConformerEncoder-938"><a href="#ConformerEncoder-938"><span class="linenos"> 938</span></a><span class="sd">                conv1d layer.</span>
</span><span id="ConformerEncoder-939"><a href="#ConformerEncoder-939"><span class="linenos"> 939</span></a><span class="sd">            macaron_style (bool): Whether to use macaron style for</span>
</span><span id="ConformerEncoder-940"><a href="#ConformerEncoder-940"><span class="linenos"> 940</span></a><span class="sd">                positionwise layer.</span>
</span><span id="ConformerEncoder-941"><a href="#ConformerEncoder-941"><span class="linenos"> 941</span></a><span class="sd">            selfattention_layer_type (str): Encoder attention layer type,</span>
</span><span id="ConformerEncoder-942"><a href="#ConformerEncoder-942"><span class="linenos"> 942</span></a><span class="sd">                the parameter has no effect now, it&#39;s just for configure</span>
</span><span id="ConformerEncoder-943"><a href="#ConformerEncoder-943"><span class="linenos"> 943</span></a><span class="sd">                compatibility. #&#39;rel_selfattn&#39;</span>
</span><span id="ConformerEncoder-944"><a href="#ConformerEncoder-944"><span class="linenos"> 944</span></a><span class="sd">            activation_type (str): Encoder activation function type.</span>
</span><span id="ConformerEncoder-945"><a href="#ConformerEncoder-945"><span class="linenos"> 945</span></a><span class="sd">            use_cnn_module (bool): Whether to use convolution module.</span>
</span><span id="ConformerEncoder-946"><a href="#ConformerEncoder-946"><span class="linenos"> 946</span></a><span class="sd">            cnn_module_kernel (int): Kernel size of convolution module.</span>
</span><span id="ConformerEncoder-947"><a href="#ConformerEncoder-947"><span class="linenos"> 947</span></a><span class="sd">            causal (bool): whether to use causal convolution or not.</span>
</span><span id="ConformerEncoder-948"><a href="#ConformerEncoder-948"><span class="linenos"> 948</span></a><span class="sd">            key_bias: whether use bias in attention.linear_k, False for whisper models.</span>
</span><span id="ConformerEncoder-949"><a href="#ConformerEncoder-949"><span class="linenos"> 949</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoder-950"><a href="#ConformerEncoder-950"><span class="linenos"> 950</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConformerEncoder-951"><a href="#ConformerEncoder-951"><span class="linenos"> 951</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
</span><span id="ConformerEncoder-952"><a href="#ConformerEncoder-952"><span class="linenos"> 952</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">LinearEmbed</span><span class="p">(</span>
</span><span id="ConformerEncoder-953"><a href="#ConformerEncoder-953"><span class="linenos"> 953</span></a>            <span class="n">input_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-954"><a href="#ConformerEncoder-954"><span class="linenos"> 954</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-955"><a href="#ConformerEncoder-955"><span class="linenos"> 955</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder-956"><a href="#ConformerEncoder-956"><span class="linenos"> 956</span></a>            <span class="n">EspnetRelPositionalEncoding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">positional_dropout_rate</span><span class="p">),</span>
</span><span id="ConformerEncoder-957"><a href="#ConformerEncoder-957"><span class="linenos"> 957</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-958"><a href="#ConformerEncoder-958"><span class="linenos"> 958</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span><span id="ConformerEncoder-959"><a href="#ConformerEncoder-959"><span class="linenos"> 959</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="ConformerEncoder-960"><a href="#ConformerEncoder-960"><span class="linenos"> 960</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">gradient_checkpointing</span>
</span><span id="ConformerEncoder-961"><a href="#ConformerEncoder-961"><span class="linenos"> 961</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="ConformerEncoder-962"><a href="#ConformerEncoder-962"><span class="linenos"> 962</span></a>
</span><span id="ConformerEncoder-963"><a href="#ConformerEncoder-963"><span class="linenos"> 963</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span> <span class="o">=</span> <span class="n">static_chunk_size</span>
</span><span id="ConformerEncoder-964"><a href="#ConformerEncoder-964"><span class="linenos"> 964</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="ConformerEncoder-965"><a href="#ConformerEncoder-965"><span class="linenos"> 965</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_left_chunk</span>
</span><span id="ConformerEncoder-966"><a href="#ConformerEncoder-966"><span class="linenos"> 966</span></a>        <span class="n">activation</span> <span class="o">=</span> <span class="n">ACTIVATION_CLASSES</span><span class="p">[</span><span class="n">activation_type</span><span class="p">]()</span>
</span><span id="ConformerEncoder-967"><a href="#ConformerEncoder-967"><span class="linenos"> 967</span></a>
</span><span id="ConformerEncoder-968"><a href="#ConformerEncoder-968"><span class="linenos"> 968</span></a>        <span class="c1"># self-attention module definition</span>
</span><span id="ConformerEncoder-969"><a href="#ConformerEncoder-969"><span class="linenos"> 969</span></a>        <span class="n">encoder_selfattn_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder-970"><a href="#ConformerEncoder-970"><span class="linenos"> 970</span></a>            <span class="n">attention_heads</span><span class="p">,</span>
</span><span id="ConformerEncoder-971"><a href="#ConformerEncoder-971"><span class="linenos"> 971</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-972"><a href="#ConformerEncoder-972"><span class="linenos"> 972</span></a>            <span class="n">attention_dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder-973"><a href="#ConformerEncoder-973"><span class="linenos"> 973</span></a>            <span class="n">key_bias</span><span class="p">,</span>
</span><span id="ConformerEncoder-974"><a href="#ConformerEncoder-974"><span class="linenos"> 974</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-975"><a href="#ConformerEncoder-975"><span class="linenos"> 975</span></a>        <span class="c1"># feed-forward module definition</span>
</span><span id="ConformerEncoder-976"><a href="#ConformerEncoder-976"><span class="linenos"> 976</span></a>        <span class="n">positionwise_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder-977"><a href="#ConformerEncoder-977"><span class="linenos"> 977</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-978"><a href="#ConformerEncoder-978"><span class="linenos"> 978</span></a>            <span class="n">linear_units</span><span class="p">,</span>
</span><span id="ConformerEncoder-979"><a href="#ConformerEncoder-979"><span class="linenos"> 979</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder-980"><a href="#ConformerEncoder-980"><span class="linenos"> 980</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="ConformerEncoder-981"><a href="#ConformerEncoder-981"><span class="linenos"> 981</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-982"><a href="#ConformerEncoder-982"><span class="linenos"> 982</span></a>        <span class="c1"># convolution module definition</span>
</span><span id="ConformerEncoder-983"><a href="#ConformerEncoder-983"><span class="linenos"> 983</span></a>        <span class="n">convolution_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder-984"><a href="#ConformerEncoder-984"><span class="linenos"> 984</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-985"><a href="#ConformerEncoder-985"><span class="linenos"> 985</span></a>            <span class="n">cnn_module_kernel</span><span class="p">,</span>
</span><span id="ConformerEncoder-986"><a href="#ConformerEncoder-986"><span class="linenos"> 986</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="ConformerEncoder-987"><a href="#ConformerEncoder-987"><span class="linenos"> 987</span></a>            <span class="n">cnn_module_norm</span><span class="p">,</span>
</span><span id="ConformerEncoder-988"><a href="#ConformerEncoder-988"><span class="linenos"> 988</span></a>            <span class="n">causal</span><span class="p">,</span>
</span><span id="ConformerEncoder-989"><a href="#ConformerEncoder-989"><span class="linenos"> 989</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-990"><a href="#ConformerEncoder-990"><span class="linenos"> 990</span></a>
</span><span id="ConformerEncoder-991"><a href="#ConformerEncoder-991"><span class="linenos"> 991</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="ConformerEncoder-992"><a href="#ConformerEncoder-992"><span class="linenos"> 992</span></a>            <span class="p">[</span>
</span><span id="ConformerEncoder-993"><a href="#ConformerEncoder-993"><span class="linenos"> 993</span></a>                <span class="n">ConformerEncoderLayer</span><span class="p">(</span>
</span><span id="ConformerEncoder-994"><a href="#ConformerEncoder-994"><span class="linenos"> 994</span></a>                    <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-995"><a href="#ConformerEncoder-995"><span class="linenos"> 995</span></a>                    <span class="n">RelPositionMultiHeadedAttention</span><span class="p">(</span><span class="o">*</span><span class="n">encoder_selfattn_layer_args</span><span class="p">),</span>
</span><span id="ConformerEncoder-996"><a href="#ConformerEncoder-996"><span class="linenos"> 996</span></a>                    <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">),</span>
</span><span id="ConformerEncoder-997"><a href="#ConformerEncoder-997"><span class="linenos"> 997</span></a>                    <span class="p">(</span>
</span><span id="ConformerEncoder-998"><a href="#ConformerEncoder-998"><span class="linenos"> 998</span></a>                        <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">)</span>
</span><span id="ConformerEncoder-999"><a href="#ConformerEncoder-999"><span class="linenos"> 999</span></a>                        <span class="k">if</span> <span class="n">macaron_style</span>
</span><span id="ConformerEncoder-1000"><a href="#ConformerEncoder-1000"><span class="linenos">1000</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConformerEncoder-1001"><a href="#ConformerEncoder-1001"><span class="linenos">1001</span></a>                    <span class="p">),</span>
</span><span id="ConformerEncoder-1002"><a href="#ConformerEncoder-1002"><span class="linenos">1002</span></a>                    <span class="p">(</span>
</span><span id="ConformerEncoder-1003"><a href="#ConformerEncoder-1003"><span class="linenos">1003</span></a>                        <span class="n">ConvolutionModule</span><span class="p">(</span><span class="o">*</span><span class="n">convolution_layer_args</span><span class="p">)</span>
</span><span id="ConformerEncoder-1004"><a href="#ConformerEncoder-1004"><span class="linenos">1004</span></a>                        <span class="k">if</span> <span class="n">use_cnn_module</span>
</span><span id="ConformerEncoder-1005"><a href="#ConformerEncoder-1005"><span class="linenos">1005</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConformerEncoder-1006"><a href="#ConformerEncoder-1006"><span class="linenos">1006</span></a>                    <span class="p">),</span>
</span><span id="ConformerEncoder-1007"><a href="#ConformerEncoder-1007"><span class="linenos">1007</span></a>                    <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder-1008"><a href="#ConformerEncoder-1008"><span class="linenos">1008</span></a>                    <span class="n">normalize_before</span><span class="p">,</span>
</span><span id="ConformerEncoder-1009"><a href="#ConformerEncoder-1009"><span class="linenos">1009</span></a>                <span class="p">)</span>
</span><span id="ConformerEncoder-1010"><a href="#ConformerEncoder-1010"><span class="linenos">1010</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
</span><span id="ConformerEncoder-1011"><a href="#ConformerEncoder-1011"><span class="linenos">1011</span></a>            <span class="p">]</span>
</span><span id="ConformerEncoder-1012"><a href="#ConformerEncoder-1012"><span class="linenos">1012</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-1013"><a href="#ConformerEncoder-1013"><span class="linenos">1013</span></a>
</span><span id="ConformerEncoder-1014"><a href="#ConformerEncoder-1014"><span class="linenos">1014</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers</span><span class="p">(</span>
</span><span id="ConformerEncoder-1015"><a href="#ConformerEncoder-1015"><span class="linenos">1015</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder-1016"><a href="#ConformerEncoder-1016"><span class="linenos">1016</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1017"><a href="#ConformerEncoder-1017"><span class="linenos">1017</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1018"><a href="#ConformerEncoder-1018"><span class="linenos">1018</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1019"><a href="#ConformerEncoder-1019"><span class="linenos">1019</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1020"><a href="#ConformerEncoder-1020"><span class="linenos">1020</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="ConformerEncoder-1021"><a href="#ConformerEncoder-1021"><span class="linenos">1021</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="ConformerEncoder-1022"><a href="#ConformerEncoder-1022"><span class="linenos">1022</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder-1023"><a href="#ConformerEncoder-1023"><span class="linenos">1023</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span><span id="ConformerEncoder-1024"><a href="#ConformerEncoder-1024"><span class="linenos">1024</span></a>
</span><span id="ConformerEncoder-1025"><a href="#ConformerEncoder-1025"><span class="linenos">1025</span></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
</span><span id="ConformerEncoder-1026"><a href="#ConformerEncoder-1026"><span class="linenos">1026</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers_checkpointed</span><span class="p">(</span>
</span><span id="ConformerEncoder-1027"><a href="#ConformerEncoder-1027"><span class="linenos">1027</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder-1028"><a href="#ConformerEncoder-1028"><span class="linenos">1028</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1029"><a href="#ConformerEncoder-1029"><span class="linenos">1029</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1030"><a href="#ConformerEncoder-1030"><span class="linenos">1030</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1031"><a href="#ConformerEncoder-1031"><span class="linenos">1031</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1032"><a href="#ConformerEncoder-1032"><span class="linenos">1032</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="ConformerEncoder-1033"><a href="#ConformerEncoder-1033"><span class="linenos">1033</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="ConformerEncoder-1034"><a href="#ConformerEncoder-1034"><span class="linenos">1034</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
</span><span id="ConformerEncoder-1035"><a href="#ConformerEncoder-1035"><span class="linenos">1035</span></a>                <span class="n">layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">use_reentrant</span><span class="o">=</span><span class="kc">False</span>
</span><span id="ConformerEncoder-1036"><a href="#ConformerEncoder-1036"><span class="linenos">1036</span></a>            <span class="p">)</span>
</span><span id="ConformerEncoder-1037"><a href="#ConformerEncoder-1037"><span class="linenos">1037</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span><span id="ConformerEncoder-1038"><a href="#ConformerEncoder-1038"><span class="linenos">1038</span></a>
</span><span id="ConformerEncoder-1039"><a href="#ConformerEncoder-1039"><span class="linenos">1039</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConformerEncoder-1040"><a href="#ConformerEncoder-1040"><span class="linenos">1040</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder-1041"><a href="#ConformerEncoder-1041"><span class="linenos">1041</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1042"><a href="#ConformerEncoder-1042"><span class="linenos">1042</span></a>        <span class="n">pad_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder-1043"><a href="#ConformerEncoder-1043"><span class="linenos">1043</span></a>        <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="ConformerEncoder-1044"><a href="#ConformerEncoder-1044"><span class="linenos">1044</span></a>        <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConformerEncoder-1045"><a href="#ConformerEncoder-1045"><span class="linenos">1045</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConformerEncoder-1046"><a href="#ConformerEncoder-1046"><span class="linenos">1046</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed positions in tensor.</span>
</span><span id="ConformerEncoder-1047"><a href="#ConformerEncoder-1047"><span class="linenos">1047</span></a>
</span><span id="ConformerEncoder-1048"><a href="#ConformerEncoder-1048"><span class="linenos">1048</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoder-1049"><a href="#ConformerEncoder-1049"><span class="linenos">1049</span></a><span class="sd">            xs: padded input tensor (B, T, D)</span>
</span><span id="ConformerEncoder-1050"><a href="#ConformerEncoder-1050"><span class="linenos">1050</span></a><span class="sd">            xs_lens: input length (B)</span>
</span><span id="ConformerEncoder-1051"><a href="#ConformerEncoder-1051"><span class="linenos">1051</span></a><span class="sd">            decoding_chunk_size: decoding chunk size for dynamic chunk</span>
</span><span id="ConformerEncoder-1052"><a href="#ConformerEncoder-1052"><span class="linenos">1052</span></a><span class="sd">                0: default for training, use random dynamic chunk.</span>
</span><span id="ConformerEncoder-1053"><a href="#ConformerEncoder-1053"><span class="linenos">1053</span></a><span class="sd">                &lt;0: for decoding, use full chunk.</span>
</span><span id="ConformerEncoder-1054"><a href="#ConformerEncoder-1054"><span class="linenos">1054</span></a><span class="sd">                &gt;0: for decoding, use fixed chunk size as set.</span>
</span><span id="ConformerEncoder-1055"><a href="#ConformerEncoder-1055"><span class="linenos">1055</span></a><span class="sd">            num_decoding_left_chunks: number of left chunks, this is for decoding,</span>
</span><span id="ConformerEncoder-1056"><a href="#ConformerEncoder-1056"><span class="linenos">1056</span></a><span class="sd">            the chunk size is decoding_chunk_size.</span>
</span><span id="ConformerEncoder-1057"><a href="#ConformerEncoder-1057"><span class="linenos">1057</span></a><span class="sd">                &gt;=0: use num_decoding_left_chunks</span>
</span><span id="ConformerEncoder-1058"><a href="#ConformerEncoder-1058"><span class="linenos">1058</span></a><span class="sd">                &lt;0: use all left chunks</span>
</span><span id="ConformerEncoder-1059"><a href="#ConformerEncoder-1059"><span class="linenos">1059</span></a><span class="sd">        Returns:</span>
</span><span id="ConformerEncoder-1060"><a href="#ConformerEncoder-1060"><span class="linenos">1060</span></a><span class="sd">            encoder output tensor xs, and subsampled masks</span>
</span><span id="ConformerEncoder-1061"><a href="#ConformerEncoder-1061"><span class="linenos">1061</span></a><span class="sd">            xs: padded output tensor (B, T&#39; ~= T/subsample_rate, D)</span>
</span><span id="ConformerEncoder-1062"><a href="#ConformerEncoder-1062"><span class="linenos">1062</span></a><span class="sd">            masks: torch.Tensor batch padding mask after subsample</span>
</span><span id="ConformerEncoder-1063"><a href="#ConformerEncoder-1063"><span class="linenos">1063</span></a><span class="sd">                (B, 1, T&#39; ~= T/subsample_rate)</span>
</span><span id="ConformerEncoder-1064"><a href="#ConformerEncoder-1064"><span class="linenos">1064</span></a><span class="sd">        NOTE(xcsong):</span>
</span><span id="ConformerEncoder-1065"><a href="#ConformerEncoder-1065"><span class="linenos">1065</span></a><span class="sd">            We pass the `__call__` method of the modules instead of `forward` to the</span>
</span><span id="ConformerEncoder-1066"><a href="#ConformerEncoder-1066"><span class="linenos">1066</span></a><span class="sd">            checkpointing API because `__call__` attaches all the hooks of the module.</span>
</span><span id="ConformerEncoder-1067"><a href="#ConformerEncoder-1067"><span class="linenos">1067</span></a><span class="sd">            https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</span>
</span><span id="ConformerEncoder-1068"><a href="#ConformerEncoder-1068"><span class="linenos">1068</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoder-1069"><a href="#ConformerEncoder-1069"><span class="linenos">1069</span></a>        <span class="n">T</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="ConformerEncoder-1070"><a href="#ConformerEncoder-1070"><span class="linenos">1070</span></a>        <span class="n">masks</span> <span class="o">=</span> <span class="n">pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, T)</span>
</span><span id="ConformerEncoder-1071"><a href="#ConformerEncoder-1071"><span class="linenos">1071</span></a>        <span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="ConformerEncoder-1072"><a href="#ConformerEncoder-1072"><span class="linenos">1072</span></a>        <span class="n">mask_pad</span> <span class="o">=</span> <span class="n">masks</span>  <span class="c1"># (B, 1, T/subsample_rate)</span>
</span><span id="ConformerEncoder-1073"><a href="#ConformerEncoder-1073"><span class="linenos">1073</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">add_optional_chunk_mask</span><span class="p">(</span>
</span><span id="ConformerEncoder-1074"><a href="#ConformerEncoder-1074"><span class="linenos">1074</span></a>            <span class="n">xs</span><span class="p">,</span>
</span><span id="ConformerEncoder-1075"><a href="#ConformerEncoder-1075"><span class="linenos">1075</span></a>            <span class="n">masks</span><span class="p">,</span>
</span><span id="ConformerEncoder-1076"><a href="#ConformerEncoder-1076"><span class="linenos">1076</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span><span class="p">,</span>
</span><span id="ConformerEncoder-1077"><a href="#ConformerEncoder-1077"><span class="linenos">1077</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span><span class="p">,</span>
</span><span id="ConformerEncoder-1078"><a href="#ConformerEncoder-1078"><span class="linenos">1078</span></a>            <span class="n">decoding_chunk_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-1079"><a href="#ConformerEncoder-1079"><span class="linenos">1079</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span><span class="p">,</span>
</span><span id="ConformerEncoder-1080"><a href="#ConformerEncoder-1080"><span class="linenos">1080</span></a>            <span class="n">num_decoding_left_chunks</span><span class="p">,</span>
</span><span id="ConformerEncoder-1081"><a href="#ConformerEncoder-1081"><span class="linenos">1081</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder-1082"><a href="#ConformerEncoder-1082"><span class="linenos">1082</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span><span id="ConformerEncoder-1083"><a href="#ConformerEncoder-1083"><span class="linenos">1083</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers_checkpointed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder-1084"><a href="#ConformerEncoder-1084"><span class="linenos">1084</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConformerEncoder-1085"><a href="#ConformerEncoder-1085"><span class="linenos">1085</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder-1086"><a href="#ConformerEncoder-1086"><span class="linenos">1086</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoder-1087"><a href="#ConformerEncoder-1087"><span class="linenos">1087</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="ConformerEncoder-1088"><a href="#ConformerEncoder-1088"><span class="linenos">1088</span></a>        <span class="c1"># Here we assume the mask is not changed in encoder layers, so just</span>
</span><span id="ConformerEncoder-1089"><a href="#ConformerEncoder-1089"><span class="linenos">1089</span></a>        <span class="c1"># return the masks before encoder layers, and the masks will be used</span>
</span><span id="ConformerEncoder-1090"><a href="#ConformerEncoder-1090"><span class="linenos">1090</span></a>        <span class="c1"># for cross attention with decoder later</span>
</span><span id="ConformerEncoder-1091"><a href="#ConformerEncoder-1091"><span class="linenos">1091</span></a>        <span class="k">return</span> <span class="n">xs</span><span class="p">,</span> <span class="n">masks</span>
</span></pre></div>


            <div class="docstring"><p>Conformer encoder module.</p>
</div>


                            <div id="ConformerEncoder.__init__" class="classattr">
                                        <input id="ConformerEncoder.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ConformerEncoder</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>,</span><span class="param">	<span class="n">attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">linear_units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span>,</span><span class="param">	<span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span>,</span><span class="param">	<span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>,</span><span class="param">	<span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>,</span><span class="param">	<span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>,</span><span class="param">	<span class="n">input_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span>,</span><span class="param">	<span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;rel_pos_espnet&#39;</span>,</span><span class="param">	<span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">positionwise_conv_kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">macaron_style</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">selfattention_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;rel_selfattn&#39;</span>,</span><span class="param">	<span class="n">activation_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;swish&#39;</span>,</span><span class="param">	<span class="n">use_cnn_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">cnn_module_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">cnn_module_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;batch_norm&#39;</span>,</span><span class="param">	<span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ConformerEncoder.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoder.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoder.__init__-906"><a href="#ConformerEncoder.__init__-906"><span class="linenos"> 906</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ConformerEncoder.__init__-907"><a href="#ConformerEncoder.__init__-907"><span class="linenos"> 907</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-908"><a href="#ConformerEncoder.__init__-908"><span class="linenos"> 908</span></a>        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-909"><a href="#ConformerEncoder.__init__-909"><span class="linenos"> 909</span></a>        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-910"><a href="#ConformerEncoder.__init__-910"><span class="linenos"> 910</span></a>        <span class="n">attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-911"><a href="#ConformerEncoder.__init__-911"><span class="linenos"> 911</span></a>        <span class="n">linear_units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-912"><a href="#ConformerEncoder.__init__-912"><span class="linenos"> 912</span></a>        <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-913"><a href="#ConformerEncoder.__init__-913"><span class="linenos"> 913</span></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-914"><a href="#ConformerEncoder.__init__-914"><span class="linenos"> 914</span></a>        <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-915"><a href="#ConformerEncoder.__init__-915"><span class="linenos"> 915</span></a>        <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-916"><a href="#ConformerEncoder.__init__-916"><span class="linenos"> 916</span></a>        <span class="n">input_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-917"><a href="#ConformerEncoder.__init__-917"><span class="linenos"> 917</span></a>        <span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_pos_espnet&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-918"><a href="#ConformerEncoder.__init__-918"><span class="linenos"> 918</span></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-919"><a href="#ConformerEncoder.__init__-919"><span class="linenos"> 919</span></a>        <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># 1: causal_mask; 0: full_mask</span>
</span><span id="ConformerEncoder.__init__-920"><a href="#ConformerEncoder.__init__-920"><span class="linenos"> 920</span></a>        <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-921"><a href="#ConformerEncoder.__init__-921"><span class="linenos"> 921</span></a>        <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-922"><a href="#ConformerEncoder.__init__-922"><span class="linenos"> 922</span></a>        <span class="n">positionwise_conv_kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-923"><a href="#ConformerEncoder.__init__-923"><span class="linenos"> 923</span></a>        <span class="n">macaron_style</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-924"><a href="#ConformerEncoder.__init__-924"><span class="linenos"> 924</span></a>        <span class="n">selfattention_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rel_selfattn&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-925"><a href="#ConformerEncoder.__init__-925"><span class="linenos"> 925</span></a>        <span class="n">activation_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;swish&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-926"><a href="#ConformerEncoder.__init__-926"><span class="linenos"> 926</span></a>        <span class="n">use_cnn_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-927"><a href="#ConformerEncoder.__init__-927"><span class="linenos"> 927</span></a>        <span class="n">cnn_module_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-928"><a href="#ConformerEncoder.__init__-928"><span class="linenos"> 928</span></a>        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-929"><a href="#ConformerEncoder.__init__-929"><span class="linenos"> 929</span></a>        <span class="n">cnn_module_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_norm&quot;</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-930"><a href="#ConformerEncoder.__init__-930"><span class="linenos"> 930</span></a>        <span class="n">key_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-931"><a href="#ConformerEncoder.__init__-931"><span class="linenos"> 931</span></a>        <span class="n">gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-932"><a href="#ConformerEncoder.__init__-932"><span class="linenos"> 932</span></a>    <span class="p">):</span>
</span><span id="ConformerEncoder.__init__-933"><a href="#ConformerEncoder.__init__-933"><span class="linenos"> 933</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct ConformerEncoder</span>
</span><span id="ConformerEncoder.__init__-934"><a href="#ConformerEncoder.__init__-934"><span class="linenos"> 934</span></a>
</span><span id="ConformerEncoder.__init__-935"><a href="#ConformerEncoder.__init__-935"><span class="linenos"> 935</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoder.__init__-936"><a href="#ConformerEncoder.__init__-936"><span class="linenos"> 936</span></a><span class="sd">            input_size to use_dynamic_chunk, see in BaseEncoder</span>
</span><span id="ConformerEncoder.__init__-937"><a href="#ConformerEncoder.__init__-937"><span class="linenos"> 937</span></a><span class="sd">            positionwise_conv_kernel_size (int): Kernel size of positionwise</span>
</span><span id="ConformerEncoder.__init__-938"><a href="#ConformerEncoder.__init__-938"><span class="linenos"> 938</span></a><span class="sd">                conv1d layer.</span>
</span><span id="ConformerEncoder.__init__-939"><a href="#ConformerEncoder.__init__-939"><span class="linenos"> 939</span></a><span class="sd">            macaron_style (bool): Whether to use macaron style for</span>
</span><span id="ConformerEncoder.__init__-940"><a href="#ConformerEncoder.__init__-940"><span class="linenos"> 940</span></a><span class="sd">                positionwise layer.</span>
</span><span id="ConformerEncoder.__init__-941"><a href="#ConformerEncoder.__init__-941"><span class="linenos"> 941</span></a><span class="sd">            selfattention_layer_type (str): Encoder attention layer type,</span>
</span><span id="ConformerEncoder.__init__-942"><a href="#ConformerEncoder.__init__-942"><span class="linenos"> 942</span></a><span class="sd">                the parameter has no effect now, it&#39;s just for configure</span>
</span><span id="ConformerEncoder.__init__-943"><a href="#ConformerEncoder.__init__-943"><span class="linenos"> 943</span></a><span class="sd">                compatibility. #&#39;rel_selfattn&#39;</span>
</span><span id="ConformerEncoder.__init__-944"><a href="#ConformerEncoder.__init__-944"><span class="linenos"> 944</span></a><span class="sd">            activation_type (str): Encoder activation function type.</span>
</span><span id="ConformerEncoder.__init__-945"><a href="#ConformerEncoder.__init__-945"><span class="linenos"> 945</span></a><span class="sd">            use_cnn_module (bool): Whether to use convolution module.</span>
</span><span id="ConformerEncoder.__init__-946"><a href="#ConformerEncoder.__init__-946"><span class="linenos"> 946</span></a><span class="sd">            cnn_module_kernel (int): Kernel size of convolution module.</span>
</span><span id="ConformerEncoder.__init__-947"><a href="#ConformerEncoder.__init__-947"><span class="linenos"> 947</span></a><span class="sd">            causal (bool): whether to use causal convolution or not.</span>
</span><span id="ConformerEncoder.__init__-948"><a href="#ConformerEncoder.__init__-948"><span class="linenos"> 948</span></a><span class="sd">            key_bias: whether use bias in attention.linear_k, False for whisper models.</span>
</span><span id="ConformerEncoder.__init__-949"><a href="#ConformerEncoder.__init__-949"><span class="linenos"> 949</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoder.__init__-950"><a href="#ConformerEncoder.__init__-950"><span class="linenos"> 950</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ConformerEncoder.__init__-951"><a href="#ConformerEncoder.__init__-951"><span class="linenos"> 951</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
</span><span id="ConformerEncoder.__init__-952"><a href="#ConformerEncoder.__init__-952"><span class="linenos"> 952</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">LinearEmbed</span><span class="p">(</span>
</span><span id="ConformerEncoder.__init__-953"><a href="#ConformerEncoder.__init__-953"><span class="linenos"> 953</span></a>            <span class="n">input_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-954"><a href="#ConformerEncoder.__init__-954"><span class="linenos"> 954</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-955"><a href="#ConformerEncoder.__init__-955"><span class="linenos"> 955</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-956"><a href="#ConformerEncoder.__init__-956"><span class="linenos"> 956</span></a>            <span class="n">EspnetRelPositionalEncoding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">positional_dropout_rate</span><span class="p">),</span>
</span><span id="ConformerEncoder.__init__-957"><a href="#ConformerEncoder.__init__-957"><span class="linenos"> 957</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder.__init__-958"><a href="#ConformerEncoder.__init__-958"><span class="linenos"> 958</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">normalize_before</span>
</span><span id="ConformerEncoder.__init__-959"><a href="#ConformerEncoder.__init__-959"><span class="linenos"> 959</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="ConformerEncoder.__init__-960"><a href="#ConformerEncoder.__init__-960"><span class="linenos"> 960</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">gradient_checkpointing</span>
</span><span id="ConformerEncoder.__init__-961"><a href="#ConformerEncoder.__init__-961"><span class="linenos"> 961</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="ConformerEncoder.__init__-962"><a href="#ConformerEncoder.__init__-962"><span class="linenos"> 962</span></a>
</span><span id="ConformerEncoder.__init__-963"><a href="#ConformerEncoder.__init__-963"><span class="linenos"> 963</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span> <span class="o">=</span> <span class="n">static_chunk_size</span>
</span><span id="ConformerEncoder.__init__-964"><a href="#ConformerEncoder.__init__-964"><span class="linenos"> 964</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_chunk</span>
</span><span id="ConformerEncoder.__init__-965"><a href="#ConformerEncoder.__init__-965"><span class="linenos"> 965</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span> <span class="o">=</span> <span class="n">use_dynamic_left_chunk</span>
</span><span id="ConformerEncoder.__init__-966"><a href="#ConformerEncoder.__init__-966"><span class="linenos"> 966</span></a>        <span class="n">activation</span> <span class="o">=</span> <span class="n">ACTIVATION_CLASSES</span><span class="p">[</span><span class="n">activation_type</span><span class="p">]()</span>
</span><span id="ConformerEncoder.__init__-967"><a href="#ConformerEncoder.__init__-967"><span class="linenos"> 967</span></a>
</span><span id="ConformerEncoder.__init__-968"><a href="#ConformerEncoder.__init__-968"><span class="linenos"> 968</span></a>        <span class="c1"># self-attention module definition</span>
</span><span id="ConformerEncoder.__init__-969"><a href="#ConformerEncoder.__init__-969"><span class="linenos"> 969</span></a>        <span class="n">encoder_selfattn_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder.__init__-970"><a href="#ConformerEncoder.__init__-970"><span class="linenos"> 970</span></a>            <span class="n">attention_heads</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-971"><a href="#ConformerEncoder.__init__-971"><span class="linenos"> 971</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-972"><a href="#ConformerEncoder.__init__-972"><span class="linenos"> 972</span></a>            <span class="n">attention_dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-973"><a href="#ConformerEncoder.__init__-973"><span class="linenos"> 973</span></a>            <span class="n">key_bias</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-974"><a href="#ConformerEncoder.__init__-974"><span class="linenos"> 974</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder.__init__-975"><a href="#ConformerEncoder.__init__-975"><span class="linenos"> 975</span></a>        <span class="c1"># feed-forward module definition</span>
</span><span id="ConformerEncoder.__init__-976"><a href="#ConformerEncoder.__init__-976"><span class="linenos"> 976</span></a>        <span class="n">positionwise_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder.__init__-977"><a href="#ConformerEncoder.__init__-977"><span class="linenos"> 977</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-978"><a href="#ConformerEncoder.__init__-978"><span class="linenos"> 978</span></a>            <span class="n">linear_units</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-979"><a href="#ConformerEncoder.__init__-979"><span class="linenos"> 979</span></a>            <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-980"><a href="#ConformerEncoder.__init__-980"><span class="linenos"> 980</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-981"><a href="#ConformerEncoder.__init__-981"><span class="linenos"> 981</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder.__init__-982"><a href="#ConformerEncoder.__init__-982"><span class="linenos"> 982</span></a>        <span class="c1"># convolution module definition</span>
</span><span id="ConformerEncoder.__init__-983"><a href="#ConformerEncoder.__init__-983"><span class="linenos"> 983</span></a>        <span class="n">convolution_layer_args</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ConformerEncoder.__init__-984"><a href="#ConformerEncoder.__init__-984"><span class="linenos"> 984</span></a>            <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-985"><a href="#ConformerEncoder.__init__-985"><span class="linenos"> 985</span></a>            <span class="n">cnn_module_kernel</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-986"><a href="#ConformerEncoder.__init__-986"><span class="linenos"> 986</span></a>            <span class="n">activation</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-987"><a href="#ConformerEncoder.__init__-987"><span class="linenos"> 987</span></a>            <span class="n">cnn_module_norm</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-988"><a href="#ConformerEncoder.__init__-988"><span class="linenos"> 988</span></a>            <span class="n">causal</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-989"><a href="#ConformerEncoder.__init__-989"><span class="linenos"> 989</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder.__init__-990"><a href="#ConformerEncoder.__init__-990"><span class="linenos"> 990</span></a>
</span><span id="ConformerEncoder.__init__-991"><a href="#ConformerEncoder.__init__-991"><span class="linenos"> 991</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="ConformerEncoder.__init__-992"><a href="#ConformerEncoder.__init__-992"><span class="linenos"> 992</span></a>            <span class="p">[</span>
</span><span id="ConformerEncoder.__init__-993"><a href="#ConformerEncoder.__init__-993"><span class="linenos"> 993</span></a>                <span class="n">ConformerEncoderLayer</span><span class="p">(</span>
</span><span id="ConformerEncoder.__init__-994"><a href="#ConformerEncoder.__init__-994"><span class="linenos"> 994</span></a>                    <span class="n">output_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-995"><a href="#ConformerEncoder.__init__-995"><span class="linenos"> 995</span></a>                    <span class="n">RelPositionMultiHeadedAttention</span><span class="p">(</span><span class="o">*</span><span class="n">encoder_selfattn_layer_args</span><span class="p">),</span>
</span><span id="ConformerEncoder.__init__-996"><a href="#ConformerEncoder.__init__-996"><span class="linenos"> 996</span></a>                    <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">),</span>
</span><span id="ConformerEncoder.__init__-997"><a href="#ConformerEncoder.__init__-997"><span class="linenos"> 997</span></a>                    <span class="p">(</span>
</span><span id="ConformerEncoder.__init__-998"><a href="#ConformerEncoder.__init__-998"><span class="linenos"> 998</span></a>                        <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">)</span>
</span><span id="ConformerEncoder.__init__-999"><a href="#ConformerEncoder.__init__-999"><span class="linenos"> 999</span></a>                        <span class="k">if</span> <span class="n">macaron_style</span>
</span><span id="ConformerEncoder.__init__-1000"><a href="#ConformerEncoder.__init__-1000"><span class="linenos">1000</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConformerEncoder.__init__-1001"><a href="#ConformerEncoder.__init__-1001"><span class="linenos">1001</span></a>                    <span class="p">),</span>
</span><span id="ConformerEncoder.__init__-1002"><a href="#ConformerEncoder.__init__-1002"><span class="linenos">1002</span></a>                    <span class="p">(</span>
</span><span id="ConformerEncoder.__init__-1003"><a href="#ConformerEncoder.__init__-1003"><span class="linenos">1003</span></a>                        <span class="n">ConvolutionModule</span><span class="p">(</span><span class="o">*</span><span class="n">convolution_layer_args</span><span class="p">)</span>
</span><span id="ConformerEncoder.__init__-1004"><a href="#ConformerEncoder.__init__-1004"><span class="linenos">1004</span></a>                        <span class="k">if</span> <span class="n">use_cnn_module</span>
</span><span id="ConformerEncoder.__init__-1005"><a href="#ConformerEncoder.__init__-1005"><span class="linenos">1005</span></a>                        <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConformerEncoder.__init__-1006"><a href="#ConformerEncoder.__init__-1006"><span class="linenos">1006</span></a>                    <span class="p">),</span>
</span><span id="ConformerEncoder.__init__-1007"><a href="#ConformerEncoder.__init__-1007"><span class="linenos">1007</span></a>                    <span class="n">dropout_rate</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-1008"><a href="#ConformerEncoder.__init__-1008"><span class="linenos">1008</span></a>                    <span class="n">normalize_before</span><span class="p">,</span>
</span><span id="ConformerEncoder.__init__-1009"><a href="#ConformerEncoder.__init__-1009"><span class="linenos">1009</span></a>                <span class="p">)</span>
</span><span id="ConformerEncoder.__init__-1010"><a href="#ConformerEncoder.__init__-1010"><span class="linenos">1010</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
</span><span id="ConformerEncoder.__init__-1011"><a href="#ConformerEncoder.__init__-1011"><span class="linenos">1011</span></a>            <span class="p">]</span>
</span><span id="ConformerEncoder.__init__-1012"><a href="#ConformerEncoder.__init__-1012"><span class="linenos">1012</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct ConformerEncoder</p>

<p>Args:
    input_size to use_dynamic_chunk, see in BaseEncoder
    positionwise_conv_kernel_size (int): Kernel size of positionwise
        conv1d layer.
    macaron_style (bool): Whether to use macaron style for
        positionwise layer.
    selfattention_layer_type (str): Encoder attention layer type,
        the parameter has no effect now, it's just for configure
        compatibility. #'rel_selfattn'
    activation_type (str): Encoder activation function type.
    use_cnn_module (bool): Whether to use convolution module.
    cnn_module_kernel (int): Kernel size of convolution module.
    causal (bool): whether to use causal convolution or not.
    key_bias: whether use bias in attention.linear_k, False for whisper models.</p>
</div>


                            </div>
                            <div id="ConformerEncoder.output_size" class="classattr">
                                <div class="attr variable">
            <span class="name">output_size</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.output_size"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.embed" class="classattr">
                                <div class="attr variable">
            <span class="name">embed</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.embed"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.normalize_before" class="classattr">
                                <div class="attr variable">
            <span class="name">normalize_before</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.normalize_before"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.after_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">after_norm</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.after_norm"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.gradient_checkpointing" class="classattr">
                                <div class="attr variable">
            <span class="name">gradient_checkpointing</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.gradient_checkpointing"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.use_dynamic_chunk" class="classattr">
                                <div class="attr variable">
            <span class="name">use_dynamic_chunk</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.use_dynamic_chunk"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.static_chunk_size" class="classattr">
                                <div class="attr variable">
            <span class="name">static_chunk_size</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.static_chunk_size"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.use_dynamic_left_chunk" class="classattr">
                                <div class="attr variable">
            <span class="name">use_dynamic_left_chunk</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.use_dynamic_left_chunk"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.encoders" class="classattr">
                                <div class="attr variable">
            <span class="name">encoders</span>

        
    </div>
    <a class="headerlink" href="#ConformerEncoder.encoders"></a>
    
    

                            </div>
                            <div id="ConformerEncoder.forward_layers" class="classattr">
                                        <input id="ConformerEncoder.forward_layers-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward_layers</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="ConformerEncoder.forward_layers-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoder.forward_layers"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoder.forward_layers-1014"><a href="#ConformerEncoder.forward_layers-1014"><span class="linenos">1014</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers</span><span class="p">(</span>
</span><span id="ConformerEncoder.forward_layers-1015"><a href="#ConformerEncoder.forward_layers-1015"><span class="linenos">1015</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers-1016"><a href="#ConformerEncoder.forward_layers-1016"><span class="linenos">1016</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers-1017"><a href="#ConformerEncoder.forward_layers-1017"><span class="linenos">1017</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers-1018"><a href="#ConformerEncoder.forward_layers-1018"><span class="linenos">1018</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers-1019"><a href="#ConformerEncoder.forward_layers-1019"><span class="linenos">1019</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers-1020"><a href="#ConformerEncoder.forward_layers-1020"><span class="linenos">1020</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward_layers-1021"><a href="#ConformerEncoder.forward_layers-1021"><span class="linenos">1021</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward_layers-1022"><a href="#ConformerEncoder.forward_layers-1022"><span class="linenos">1022</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward_layers-1023"><a href="#ConformerEncoder.forward_layers-1023"><span class="linenos">1023</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span></pre></div>


    

                            </div>
                            <div id="ConformerEncoder.forward_layers_checkpointed" class="classattr">
                                        <input id="ConformerEncoder.forward_layers_checkpointed-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-torch.jit.unused">@torch.jit.unused</div>

        <span class="def">def</span>
        <span class="name">forward_layers_checkpointed</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="ConformerEncoder.forward_layers_checkpointed-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoder.forward_layers_checkpointed"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoder.forward_layers_checkpointed-1025"><a href="#ConformerEncoder.forward_layers_checkpointed-1025"><span class="linenos">1025</span></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1026"><a href="#ConformerEncoder.forward_layers_checkpointed-1026"><span class="linenos">1026</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward_layers_checkpointed</span><span class="p">(</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1027"><a href="#ConformerEncoder.forward_layers_checkpointed-1027"><span class="linenos">1027</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1028"><a href="#ConformerEncoder.forward_layers_checkpointed-1028"><span class="linenos">1028</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1029"><a href="#ConformerEncoder.forward_layers_checkpointed-1029"><span class="linenos">1029</span></a>        <span class="n">chunk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1030"><a href="#ConformerEncoder.forward_layers_checkpointed-1030"><span class="linenos">1030</span></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1031"><a href="#ConformerEncoder.forward_layers_checkpointed-1031"><span class="linenos">1031</span></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1032"><a href="#ConformerEncoder.forward_layers_checkpointed-1032"><span class="linenos">1032</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1033"><a href="#ConformerEncoder.forward_layers_checkpointed-1033"><span class="linenos">1033</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1034"><a href="#ConformerEncoder.forward_layers_checkpointed-1034"><span class="linenos">1034</span></a>            <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1035"><a href="#ConformerEncoder.forward_layers_checkpointed-1035"><span class="linenos">1035</span></a>                <span class="n">layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">use_reentrant</span><span class="o">=</span><span class="kc">False</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1036"><a href="#ConformerEncoder.forward_layers_checkpointed-1036"><span class="linenos">1036</span></a>            <span class="p">)</span>
</span><span id="ConformerEncoder.forward_layers_checkpointed-1037"><a href="#ConformerEncoder.forward_layers_checkpointed-1037"><span class="linenos">1037</span></a>        <span class="k">return</span> <span class="n">xs</span>
</span></pre></div>


    

                            </div>
                            <div id="ConformerEncoder.forward" class="classattr">
                                        <input id="ConformerEncoder.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pad_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="ConformerEncoder.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ConformerEncoder.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConformerEncoder.forward-1039"><a href="#ConformerEncoder.forward-1039"><span class="linenos">1039</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ConformerEncoder.forward-1040"><a href="#ConformerEncoder.forward-1040"><span class="linenos">1040</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1041"><a href="#ConformerEncoder.forward-1041"><span class="linenos">1041</span></a>        <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1042"><a href="#ConformerEncoder.forward-1042"><span class="linenos">1042</span></a>        <span class="n">pad_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1043"><a href="#ConformerEncoder.forward-1043"><span class="linenos">1043</span></a>        <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1044"><a href="#ConformerEncoder.forward-1044"><span class="linenos">1044</span></a>        <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1045"><a href="#ConformerEncoder.forward-1045"><span class="linenos">1045</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="ConformerEncoder.forward-1046"><a href="#ConformerEncoder.forward-1046"><span class="linenos">1046</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed positions in tensor.</span>
</span><span id="ConformerEncoder.forward-1047"><a href="#ConformerEncoder.forward-1047"><span class="linenos">1047</span></a>
</span><span id="ConformerEncoder.forward-1048"><a href="#ConformerEncoder.forward-1048"><span class="linenos">1048</span></a><span class="sd">        Args:</span>
</span><span id="ConformerEncoder.forward-1049"><a href="#ConformerEncoder.forward-1049"><span class="linenos">1049</span></a><span class="sd">            xs: padded input tensor (B, T, D)</span>
</span><span id="ConformerEncoder.forward-1050"><a href="#ConformerEncoder.forward-1050"><span class="linenos">1050</span></a><span class="sd">            xs_lens: input length (B)</span>
</span><span id="ConformerEncoder.forward-1051"><a href="#ConformerEncoder.forward-1051"><span class="linenos">1051</span></a><span class="sd">            decoding_chunk_size: decoding chunk size for dynamic chunk</span>
</span><span id="ConformerEncoder.forward-1052"><a href="#ConformerEncoder.forward-1052"><span class="linenos">1052</span></a><span class="sd">                0: default for training, use random dynamic chunk.</span>
</span><span id="ConformerEncoder.forward-1053"><a href="#ConformerEncoder.forward-1053"><span class="linenos">1053</span></a><span class="sd">                &lt;0: for decoding, use full chunk.</span>
</span><span id="ConformerEncoder.forward-1054"><a href="#ConformerEncoder.forward-1054"><span class="linenos">1054</span></a><span class="sd">                &gt;0: for decoding, use fixed chunk size as set.</span>
</span><span id="ConformerEncoder.forward-1055"><a href="#ConformerEncoder.forward-1055"><span class="linenos">1055</span></a><span class="sd">            num_decoding_left_chunks: number of left chunks, this is for decoding,</span>
</span><span id="ConformerEncoder.forward-1056"><a href="#ConformerEncoder.forward-1056"><span class="linenos">1056</span></a><span class="sd">            the chunk size is decoding_chunk_size.</span>
</span><span id="ConformerEncoder.forward-1057"><a href="#ConformerEncoder.forward-1057"><span class="linenos">1057</span></a><span class="sd">                &gt;=0: use num_decoding_left_chunks</span>
</span><span id="ConformerEncoder.forward-1058"><a href="#ConformerEncoder.forward-1058"><span class="linenos">1058</span></a><span class="sd">                &lt;0: use all left chunks</span>
</span><span id="ConformerEncoder.forward-1059"><a href="#ConformerEncoder.forward-1059"><span class="linenos">1059</span></a><span class="sd">        Returns:</span>
</span><span id="ConformerEncoder.forward-1060"><a href="#ConformerEncoder.forward-1060"><span class="linenos">1060</span></a><span class="sd">            encoder output tensor xs, and subsampled masks</span>
</span><span id="ConformerEncoder.forward-1061"><a href="#ConformerEncoder.forward-1061"><span class="linenos">1061</span></a><span class="sd">            xs: padded output tensor (B, T&#39; ~= T/subsample_rate, D)</span>
</span><span id="ConformerEncoder.forward-1062"><a href="#ConformerEncoder.forward-1062"><span class="linenos">1062</span></a><span class="sd">            masks: torch.Tensor batch padding mask after subsample</span>
</span><span id="ConformerEncoder.forward-1063"><a href="#ConformerEncoder.forward-1063"><span class="linenos">1063</span></a><span class="sd">                (B, 1, T&#39; ~= T/subsample_rate)</span>
</span><span id="ConformerEncoder.forward-1064"><a href="#ConformerEncoder.forward-1064"><span class="linenos">1064</span></a><span class="sd">        NOTE(xcsong):</span>
</span><span id="ConformerEncoder.forward-1065"><a href="#ConformerEncoder.forward-1065"><span class="linenos">1065</span></a><span class="sd">            We pass the `__call__` method of the modules instead of `forward` to the</span>
</span><span id="ConformerEncoder.forward-1066"><a href="#ConformerEncoder.forward-1066"><span class="linenos">1066</span></a><span class="sd">            checkpointing API because `__call__` attaches all the hooks of the module.</span>
</span><span id="ConformerEncoder.forward-1067"><a href="#ConformerEncoder.forward-1067"><span class="linenos">1067</span></a><span class="sd">            https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</span>
</span><span id="ConformerEncoder.forward-1068"><a href="#ConformerEncoder.forward-1068"><span class="linenos">1068</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConformerEncoder.forward-1069"><a href="#ConformerEncoder.forward-1069"><span class="linenos">1069</span></a>        <span class="n">T</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward-1070"><a href="#ConformerEncoder.forward-1070"><span class="linenos">1070</span></a>        <span class="n">masks</span> <span class="o">=</span> <span class="n">pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, T)</span>
</span><span id="ConformerEncoder.forward-1071"><a href="#ConformerEncoder.forward-1071"><span class="linenos">1071</span></a>        <span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward-1072"><a href="#ConformerEncoder.forward-1072"><span class="linenos">1072</span></a>        <span class="n">mask_pad</span> <span class="o">=</span> <span class="n">masks</span>  <span class="c1"># (B, 1, T/subsample_rate)</span>
</span><span id="ConformerEncoder.forward-1073"><a href="#ConformerEncoder.forward-1073"><span class="linenos">1073</span></a>        <span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">add_optional_chunk_mask</span><span class="p">(</span>
</span><span id="ConformerEncoder.forward-1074"><a href="#ConformerEncoder.forward-1074"><span class="linenos">1074</span></a>            <span class="n">xs</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1075"><a href="#ConformerEncoder.forward-1075"><span class="linenos">1075</span></a>            <span class="n">masks</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1076"><a href="#ConformerEncoder.forward-1076"><span class="linenos">1076</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1077"><a href="#ConformerEncoder.forward-1077"><span class="linenos">1077</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1078"><a href="#ConformerEncoder.forward-1078"><span class="linenos">1078</span></a>            <span class="n">decoding_chunk_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1079"><a href="#ConformerEncoder.forward-1079"><span class="linenos">1079</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1080"><a href="#ConformerEncoder.forward-1080"><span class="linenos">1080</span></a>            <span class="n">num_decoding_left_chunks</span><span class="p">,</span>
</span><span id="ConformerEncoder.forward-1081"><a href="#ConformerEncoder.forward-1081"><span class="linenos">1081</span></a>        <span class="p">)</span>
</span><span id="ConformerEncoder.forward-1082"><a href="#ConformerEncoder.forward-1082"><span class="linenos">1082</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward-1083"><a href="#ConformerEncoder.forward-1083"><span class="linenos">1083</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers_checkpointed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward-1084"><a href="#ConformerEncoder.forward-1084"><span class="linenos">1084</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward-1085"><a href="#ConformerEncoder.forward-1085"><span class="linenos">1085</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_layers</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward-1086"><a href="#ConformerEncoder.forward-1086"><span class="linenos">1086</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
</span><span id="ConformerEncoder.forward-1087"><a href="#ConformerEncoder.forward-1087"><span class="linenos">1087</span></a>            <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</span><span id="ConformerEncoder.forward-1088"><a href="#ConformerEncoder.forward-1088"><span class="linenos">1088</span></a>        <span class="c1"># Here we assume the mask is not changed in encoder layers, so just</span>
</span><span id="ConformerEncoder.forward-1089"><a href="#ConformerEncoder.forward-1089"><span class="linenos">1089</span></a>        <span class="c1"># return the masks before encoder layers, and the masks will be used</span>
</span><span id="ConformerEncoder.forward-1090"><a href="#ConformerEncoder.forward-1090"><span class="linenos">1090</span></a>        <span class="c1"># for cross attention with decoder later</span>
</span><span id="ConformerEncoder.forward-1091"><a href="#ConformerEncoder.forward-1091"><span class="linenos">1091</span></a>        <span class="k">return</span> <span class="n">xs</span><span class="p">,</span> <span class="n">masks</span>
</span></pre></div>


            <div class="docstring"><p>Embed positions in tensor.</p>

<p>Args:
    xs: padded input tensor (B, T, D)
    xs_lens: input length (B)
    decoding_chunk_size: decoding chunk size for dynamic chunk
        0: default for training, use random dynamic chunk.
        &lt;0: for decoding, use full chunk.</p>

<blockquote>
  <p>0: for decoding, use fixed chunk size as set.
      num_decoding_left_chunks: number of left chunks, this is for decoding,
      the chunk size is decoding_chunk_size.
  =0: use num_decoding_left_chunks
          &lt;0: use all left chunks
  Returns:
      encoder output tensor xs, and subsampled masks
      xs: padded output tensor (B, T' ~= T/subsample_rate, D)
      masks: torch.Tensor batch padding mask after subsample
          (B, 1, T' ~= T/subsample_rate)
  NOTE(xcsong):
      We pass the <code>__call__</code> method of the modules instead of <code><a href="#ConformerEncoder.forward">forward</a></code> to the
      checkpointing API because <code>__call__</code> attaches all the hooks of the module.
      <a href="https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2">https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</a></p>
</blockquote>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>